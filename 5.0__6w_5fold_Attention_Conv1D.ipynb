{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "189c37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import kaleido ##pip install -U kaleido ##to save a plotly fig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bbce8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('Stable_Data_CSV.csv')\n",
    "df2 = pd.read_csv('6wUsingBL.csv')\n",
    "df_stable = df2[df2.set_index(['participant_id']).index.isin(df1.set_index(['participant_id']).index)]\n",
    "df_stable_US = df_stable.loc[(df_stable['arm'] == 1)]\n",
    "df_stable_WN = df_stable.loc[(df_stable['arm'] == 2)]\n",
    "print(len(df_stable_US))\n",
    "print(len(df_stable_WN))\n",
    "df_stable_US=(df_stable_US[['c_3','sl_1','a_2','q_1','tfi_total.1','tfi_total.2','tfi_total.3']])\n",
    "df_stable_WN=(df_stable_WN[['sl_2','r_1','e_1','tfi_total.1','tfi_total.2','tfi_total.3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0973c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df):\n",
    "    sc_X = StandardScaler()\n",
    "    sc_y = StandardScaler()\n",
    "    \n",
    "    X=df.drop(['tfi_total.2','tfi_total.3'],axis=1)\n",
    "    y=df[['tfi_total.2']]\n",
    "    \n",
    "    #scale x\n",
    "    x_scaler=sc_X.fit(X)\n",
    "    X=x_scaler.transform(X)\n",
    "    #scale y\n",
    "    y_scaler=sc_y.fit(y)\n",
    "    y=y_scaler.transform(y)\n",
    "    \n",
    "    \"\"\"\n",
    "    Reshape rule:\n",
    "    tensor of shape (batch size, sequence length, features), \n",
    "    where sequence length is the number of time steps and features is each input timeseries.\n",
    "    \"\"\"\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    print(X.shape,y.shape)\n",
    "    \n",
    "    return X,y,x_scaler,y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8389c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Include residual connections, layer normalization, and dropout.\n",
    "The resulting layer can be stacked multiple times.\n",
    "The projection layers are implemented through `keras.layers.Conv1D`.\n",
    "\"\"\"\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    K.clear_session()\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    \"\"\"    \n",
    "    can stack multiple of the transformer_encoder blocks and \n",
    "    can also proceed to add the final Multi-Layer Perceptron regression head.\n",
    "    \"\"\"\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    \"\"\"\n",
    "    a pooling layer is used to to reduce the output tensor of the TransformerEncoder \n",
    "    part of our model down to a vector of features for each data point in the current batch.\n",
    "    \"\"\"\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x) \n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e3704137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and evaluate\n",
    "\n",
    "\n",
    "def prediction(x_train,y_train,y_scaler):\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    model = build_model(\n",
    "        input_shape,\n",
    "        head_size=5, # Embedding size for each token #key_dim\n",
    "        num_heads=4, # Number of attention heads\n",
    "        ff_dim=5, # Hidden layer size in feed forward network inside transformer\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "\n",
    "    def error_in_tfi(y_true,y_pred): \n",
    "        y=y_true.numpy()\n",
    "        yhat=y_pred.numpy()\n",
    "        y=np.reshape(y, (1,-1))\n",
    "        yhat=np.reshape(yhat, (1,-1))\n",
    "        y=y_scaler.inverse_transform(y)\n",
    "        yhat=y_scaler.inverse_transform(yhat)\n",
    "        y=tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        yhat=tf.convert_to_tensor(yhat, dtype=tf.float32)\n",
    "        return K.mean(abs(y - yhat), axis=-1)  #K.mean(square(y_true - y_pred), axis=-1)\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        #metrics=[keras.metrics.MeanAbsoluteError()],\n",
    "        run_eagerly=True,\n",
    "        metrics=[error_in_tfi],\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "    history=model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=200,\n",
    "        batch_size=4,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    return model,history\n",
    "    #model.evaluate(x_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39060e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 5, 1) (31, 1)\n",
      "TRAIN: [ 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30] TEST: [0 1 2 3 4 5 6]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 5, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 5, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 5, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 5, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 5, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 5, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 5, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 5, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 5, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 5, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 5, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 5, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 5, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 5, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 5, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.__operators__.add_6 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 5, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 5, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 5, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 5)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          768         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,641\n",
      "Trainable params: 29,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 2s 198ms/step - loss: 1.0805 - error_in_tfi: 18.4605 - val_loss: 0.4429 - val_error_in_tfi: 7.3310\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.1138 - error_in_tfi: 17.8442 - val_loss: 0.4424 - val_error_in_tfi: 7.3241\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9862 - error_in_tfi: 16.3898 - val_loss: 0.4420 - val_error_in_tfi: 7.3252\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9831 - error_in_tfi: 16.5198 - val_loss: 0.4419 - val_error_in_tfi: 7.3468\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1027 - error_in_tfi: 17.5355 - val_loss: 0.4418 - val_error_in_tfi: 7.3674\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8958 - error_in_tfi: 15.3780 - val_loss: 0.4413 - val_error_in_tfi: 7.3790\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9505 - error_in_tfi: 16.3284 - val_loss: 0.4407 - val_error_in_tfi: 7.3790\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.0461 - error_in_tfi: 17.3511 - val_loss: 0.4402 - val_error_in_tfi: 7.3854\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 1.0571 - error_in_tfi: 16.9346 - val_loss: 0.4398 - val_error_in_tfi: 7.3994\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.0510 - error_in_tfi: 17.2654 - val_loss: 0.4395 - val_error_in_tfi: 7.4223\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.8627 - error_in_tfi: 15.2883 - val_loss: 0.4391 - val_error_in_tfi: 7.4348\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9470 - error_in_tfi: 16.0341 - val_loss: 0.4387 - val_error_in_tfi: 7.4541\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.0009 - error_in_tfi: 16.7721 - val_loss: 0.4383 - val_error_in_tfi: 7.4636\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.0525 - error_in_tfi: 17.5053 - val_loss: 0.4379 - val_error_in_tfi: 7.4788\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9732 - error_in_tfi: 15.9844 - val_loss: 0.4375 - val_error_in_tfi: 7.4891\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.9941 - error_in_tfi: 16.9331 - val_loss: 0.4372 - val_error_in_tfi: 7.5021\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9679 - error_in_tfi: 16.4265 - val_loss: 0.4368 - val_error_in_tfi: 7.5092\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9719 - error_in_tfi: 16.1454 - val_loss: 0.4364 - val_error_in_tfi: 7.5283\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9245 - error_in_tfi: 15.6917 - val_loss: 0.4361 - val_error_in_tfi: 7.5534\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1375 - error_in_tfi: 18.0786 - val_loss: 0.4359 - val_error_in_tfi: 7.5753\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9706 - error_in_tfi: 16.0939 - val_loss: 0.4357 - val_error_in_tfi: 7.6021\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8891 - error_in_tfi: 15.4165 - val_loss: 0.4353 - val_error_in_tfi: 7.6194\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8856 - error_in_tfi: 15.6518 - val_loss: 0.4347 - val_error_in_tfi: 7.6198\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8350 - error_in_tfi: 15.2183 - val_loss: 0.4345 - val_error_in_tfi: 7.6349\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9449 - error_in_tfi: 16.0443 - val_loss: 0.4344 - val_error_in_tfi: 7.6612\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.0922 - error_in_tfi: 17.9218 - val_loss: 0.4343 - val_error_in_tfi: 7.6916\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9412 - error_in_tfi: 15.1316 - val_loss: 0.4340 - val_error_in_tfi: 7.7091\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9120 - error_in_tfi: 15.5826 - val_loss: 0.4336 - val_error_in_tfi: 7.7244\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8766 - error_in_tfi: 15.3644 - val_loss: 0.4331 - val_error_in_tfi: 7.7316\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.0503 - error_in_tfi: 17.0082 - val_loss: 0.4327 - val_error_in_tfi: 7.7426\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.2077 - error_in_tfi: 18.6441 - val_loss: 0.4321 - val_error_in_tfi: 7.7416\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8487 - error_in_tfi: 15.9691 - val_loss: 0.4315 - val_error_in_tfi: 7.7485\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.0127 - error_in_tfi: 16.3543 - val_loss: 0.4311 - val_error_in_tfi: 7.7633\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.9748 - error_in_tfi: 16.3262 - val_loss: 0.4305 - val_error_in_tfi: 7.7589\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9703 - error_in_tfi: 15.8402 - val_loss: 0.4304 - val_error_in_tfi: 7.7794\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8190 - error_in_tfi: 15.6934 - val_loss: 0.4301 - val_error_in_tfi: 7.7939\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9230 - error_in_tfi: 16.2260 - val_loss: 0.4300 - val_error_in_tfi: 7.8158\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.9691 - error_in_tfi: 17.0249 - val_loss: 0.4298 - val_error_in_tfi: 7.8406\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 1.0082 - error_in_tfi: 16.5597 - val_loss: 0.4295 - val_error_in_tfi: 7.8608\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.8376 - error_in_tfi: 15.5718 - val_loss: 0.4293 - val_error_in_tfi: 7.8887\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.9610 - error_in_tfi: 15.4993 - val_loss: 0.4290 - val_error_in_tfi: 7.8956\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.0034 - error_in_tfi: 16.6372 - val_loss: 0.4287 - val_error_in_tfi: 7.9042\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.7652 - error_in_tfi: 15.2327 - val_loss: 0.4285 - val_error_in_tfi: 7.9158\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.9904 - error_in_tfi: 17.0982 - val_loss: 0.4282 - val_error_in_tfi: 7.9296\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9568 - error_in_tfi: 16.2135 - val_loss: 0.4283 - val_error_in_tfi: 7.9594\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 1.0464 - error_in_tfi: 16.7768 - val_loss: 0.4281 - val_error_in_tfi: 7.9889\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8231 - error_in_tfi: 14.9525 - val_loss: 0.4280 - val_error_in_tfi: 8.0100\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.8239 - error_in_tfi: 15.0438 - val_loss: 0.4278 - val_error_in_tfi: 8.0282\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9821 - error_in_tfi: 16.7276 - val_loss: 0.4276 - val_error_in_tfi: 8.0444\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.9650 - error_in_tfi: 16.6466 - val_loss: 0.4273 - val_error_in_tfi: 8.0565\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9029 - error_in_tfi: 15.5582 - val_loss: 0.4269 - val_error_in_tfi: 8.0653\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.8995 - error_in_tfi: 15.7217 - val_loss: 0.4259 - val_error_in_tfi: 8.0588\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.7623 - error_in_tfi: 14.6091 - val_loss: 0.4256 - val_error_in_tfi: 8.0806\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.9475 - error_in_tfi: 16.0301 - val_loss: 0.4251 - val_error_in_tfi: 8.0753\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.8773 - error_in_tfi: 14.8708 - val_loss: 0.4248 - val_error_in_tfi: 8.0868\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.7732 - error_in_tfi: 13.5454 - val_loss: 0.4246 - val_error_in_tfi: 8.1061\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8701 - error_in_tfi: 15.5572 - val_loss: 0.4240 - val_error_in_tfi: 8.1032\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8707 - error_in_tfi: 14.7725 - val_loss: 0.4236 - val_error_in_tfi: 8.0979\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8274 - error_in_tfi: 13.9442 - val_loss: 0.4234 - val_error_in_tfi: 8.1215\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.8655 - error_in_tfi: 15.6516 - val_loss: 0.4232 - val_error_in_tfi: 8.1334\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9754 - error_in_tfi: 16.2278 - val_loss: 0.4227 - val_error_in_tfi: 8.1320\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.0232 - error_in_tfi: 17.0695 - val_loss: 0.4221 - val_error_in_tfi: 8.1313\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 1.0172 - error_in_tfi: 16.1475 - val_loss: 0.4213 - val_error_in_tfi: 8.1301\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.9659 - error_in_tfi: 17.2916 - val_loss: 0.4209 - val_error_in_tfi: 8.1422\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.7959 - error_in_tfi: 15.1607 - val_loss: 0.4208 - val_error_in_tfi: 8.1680\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.0386 - error_in_tfi: 16.0982 - val_loss: 0.4201 - val_error_in_tfi: 8.1601\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8477 - error_in_tfi: 15.6088 - val_loss: 0.4199 - val_error_in_tfi: 8.1827\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9969 - error_in_tfi: 16.7881 - val_loss: 0.4194 - val_error_in_tfi: 8.1882\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7996 - error_in_tfi: 14.5907 - val_loss: 0.4190 - val_error_in_tfi: 8.2032\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.8988 - error_in_tfi: 15.3649 - val_loss: 0.4191 - val_error_in_tfi: 8.2360\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7806 - error_in_tfi: 14.1819 - val_loss: 0.4189 - val_error_in_tfi: 8.2510\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9119 - error_in_tfi: 15.5492 - val_loss: 0.4185 - val_error_in_tfi: 8.2518\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7811 - error_in_tfi: 15.3048 - val_loss: 0.4180 - val_error_in_tfi: 8.2619\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.7422 - error_in_tfi: 14.0929 - val_loss: 0.4174 - val_error_in_tfi: 8.2510\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.8633 - error_in_tfi: 15.7460 - val_loss: 0.4170 - val_error_in_tfi: 8.2497\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.0610 - error_in_tfi: 17.2213 - val_loss: 0.4168 - val_error_in_tfi: 8.2542\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.8579 - error_in_tfi: 15.3371 - val_loss: 0.4163 - val_error_in_tfi: 8.2439\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.8708 - error_in_tfi: 15.8149 - val_loss: 0.4158 - val_error_in_tfi: 8.2325\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.9340 - error_in_tfi: 15.0531 - val_loss: 0.4157 - val_error_in_tfi: 8.2472\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.7668 - error_in_tfi: 14.9835 - val_loss: 0.4155 - val_error_in_tfi: 8.2618\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.7683 - error_in_tfi: 13.7064 - val_loss: 0.4148 - val_error_in_tfi: 8.2551\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.7163 - error_in_tfi: 13.8574 - val_loss: 0.4145 - val_error_in_tfi: 8.2567\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.8738 - error_in_tfi: 14.0152 - val_loss: 0.4141 - val_error_in_tfi: 8.2651\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.8925 - error_in_tfi: 15.8326 - val_loss: 0.4136 - val_error_in_tfi: 8.2615\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7420 - error_in_tfi: 13.9493 - val_loss: 0.4130 - val_error_in_tfi: 8.2478\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8009 - error_in_tfi: 14.6728 - val_loss: 0.4127 - val_error_in_tfi: 8.2554\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7456 - error_in_tfi: 14.0627 - val_loss: 0.4121 - val_error_in_tfi: 8.2518\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8725 - error_in_tfi: 15.6106 - val_loss: 0.4114 - val_error_in_tfi: 8.2369\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7514 - error_in_tfi: 13.9342 - val_loss: 0.4110 - val_error_in_tfi: 8.2434\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7589 - error_in_tfi: 14.3969 - val_loss: 0.4107 - val_error_in_tfi: 8.2500\n",
      "Epoch 91/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9428 - error_in_tfi: 15.5899 - val_loss: 0.4102 - val_error_in_tfi: 8.2494\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.8558 - error_in_tfi: 15.7231 - val_loss: 0.4096 - val_error_in_tfi: 8.2474\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7737 - error_in_tfi: 14.3202 - val_loss: 0.4092 - val_error_in_tfi: 8.2500\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8592 - error_in_tfi: 15.0258 - val_loss: 0.4091 - val_error_in_tfi: 8.2742\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8468 - error_in_tfi: 14.9791 - val_loss: 0.4086 - val_error_in_tfi: 8.2640\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7435 - error_in_tfi: 14.3309 - val_loss: 0.4085 - val_error_in_tfi: 8.2781\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7195 - error_in_tfi: 13.4868 - val_loss: 0.4082 - val_error_in_tfi: 8.2741\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7854 - error_in_tfi: 14.3876 - val_loss: 0.4077 - val_error_in_tfi: 8.2754\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7930 - error_in_tfi: 15.0312 - val_loss: 0.4068 - val_error_in_tfi: 8.2414\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7843 - error_in_tfi: 14.6113 - val_loss: 0.4066 - val_error_in_tfi: 8.2427\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7969 - error_in_tfi: 14.2830 - val_loss: 0.4064 - val_error_in_tfi: 8.2555\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.7653 - error_in_tfi: 13.7383 - val_loss: 0.4058 - val_error_in_tfi: 8.2541\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8011 - error_in_tfi: 14.5338 - val_loss: 0.4054 - val_error_in_tfi: 8.2619\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7224 - error_in_tfi: 13.6991 - val_loss: 0.4052 - val_error_in_tfi: 8.2808\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8105 - error_in_tfi: 14.2958 - val_loss: 0.4047 - val_error_in_tfi: 8.2798\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8632 - error_in_tfi: 15.0381 - val_loss: 0.4043 - val_error_in_tfi: 8.2857\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8794 - error_in_tfi: 15.5689 - val_loss: 0.4039 - val_error_in_tfi: 8.2896\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6861 - error_in_tfi: 13.1591 - val_loss: 0.4035 - val_error_in_tfi: 8.3011\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9064 - error_in_tfi: 16.3161 - val_loss: 0.4028 - val_error_in_tfi: 8.2967\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8279 - error_in_tfi: 14.6660 - val_loss: 0.4025 - val_error_in_tfi: 8.3090\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7680 - error_in_tfi: 14.0361 - val_loss: 0.4024 - val_error_in_tfi: 8.3302\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7687 - error_in_tfi: 13.8992 - val_loss: 0.4022 - val_error_in_tfi: 8.3538\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8469 - error_in_tfi: 14.7892 - val_loss: 0.4020 - val_error_in_tfi: 8.3683\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7616 - error_in_tfi: 14.2394 - val_loss: 0.4019 - val_error_in_tfi: 8.3848\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7701 - error_in_tfi: 13.2105 - val_loss: 0.4017 - val_error_in_tfi: 8.3953\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7963 - error_in_tfi: 14.7165 - val_loss: 0.4015 - val_error_in_tfi: 8.4084\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.8160 - error_in_tfi: 14.8924 - val_loss: 0.4015 - val_error_in_tfi: 8.4347\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8311 - error_in_tfi: 14.3748 - val_loss: 0.4014 - val_error_in_tfi: 8.4491\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8346 - error_in_tfi: 14.9849 - val_loss: 0.4009 - val_error_in_tfi: 8.4432\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7515 - error_in_tfi: 14.1636 - val_loss: 0.4006 - val_error_in_tfi: 8.4486\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6722 - error_in_tfi: 13.0994 - val_loss: 0.4004 - val_error_in_tfi: 8.4502\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7711 - error_in_tfi: 14.2555 - val_loss: 0.3999 - val_error_in_tfi: 8.4419\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8484 - error_in_tfi: 14.7730 - val_loss: 0.3993 - val_error_in_tfi: 8.4275\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7892 - error_in_tfi: 14.2558 - val_loss: 0.3988 - val_error_in_tfi: 8.4288\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7743 - error_in_tfi: 14.3078 - val_loss: 0.3981 - val_error_in_tfi: 8.4234\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8632 - error_in_tfi: 14.8267 - val_loss: 0.3980 - val_error_in_tfi: 8.4406\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6643 - error_in_tfi: 12.9555 - val_loss: 0.3975 - val_error_in_tfi: 8.4428\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9437 - error_in_tfi: 16.4960 - val_loss: 0.3970 - val_error_in_tfi: 8.4275\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7973 - error_in_tfi: 14.2649 - val_loss: 0.3967 - val_error_in_tfi: 8.4425\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8234 - error_in_tfi: 14.6035 - val_loss: 0.3961 - val_error_in_tfi: 8.4400\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7773 - error_in_tfi: 13.6236 - val_loss: 0.3950 - val_error_in_tfi: 8.4089\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7875 - error_in_tfi: 13.9702 - val_loss: 0.3944 - val_error_in_tfi: 8.3948\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.7026 - error_in_tfi: 12.6294 - val_loss: 0.3937 - val_error_in_tfi: 8.3815\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8795 - error_in_tfi: 15.2699 - val_loss: 0.3933 - val_error_in_tfi: 8.3852\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8584 - error_in_tfi: 14.6445 - val_loss: 0.3930 - val_error_in_tfi: 8.3810\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7051 - error_in_tfi: 13.3247 - val_loss: 0.3925 - val_error_in_tfi: 8.3876\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.7283 - error_in_tfi: 13.9105 - val_loss: 0.3921 - val_error_in_tfi: 8.4015\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.6261 - error_in_tfi: 13.1857 - val_loss: 0.3918 - val_error_in_tfi: 8.4138\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.6664 - error_in_tfi: 13.5396 - val_loss: 0.3917 - val_error_in_tfi: 8.4416\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.7736 - error_in_tfi: 13.2471 - val_loss: 0.3912 - val_error_in_tfi: 8.4404\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.6847 - error_in_tfi: 13.1321 - val_loss: 0.3909 - val_error_in_tfi: 8.4441\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.8728 - error_in_tfi: 15.6301 - val_loss: 0.3907 - val_error_in_tfi: 8.4482\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.6766 - error_in_tfi: 12.5388 - val_loss: 0.3909 - val_error_in_tfi: 8.4743\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.7824 - error_in_tfi: 13.4869 - val_loss: 0.3908 - val_error_in_tfi: 8.4798\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6933 - error_in_tfi: 13.1459 - val_loss: 0.3906 - val_error_in_tfi: 8.4910\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7523 - error_in_tfi: 14.4102 - val_loss: 0.3904 - val_error_in_tfi: 8.5045\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6890 - error_in_tfi: 12.9019 - val_loss: 0.3902 - val_error_in_tfi: 8.5058\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7219 - error_in_tfi: 13.8036 - val_loss: 0.3901 - val_error_in_tfi: 8.5145\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8036 - error_in_tfi: 14.3846 - val_loss: 0.3896 - val_error_in_tfi: 8.5017\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7491 - error_in_tfi: 14.2823 - val_loss: 0.3892 - val_error_in_tfi: 8.5014\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7872 - error_in_tfi: 14.4030 - val_loss: 0.3886 - val_error_in_tfi: 8.4936\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8997 - error_in_tfi: 14.8880 - val_loss: 0.3886 - val_error_in_tfi: 8.5026\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.6806 - error_in_tfi: 12.6010 - val_loss: 0.3884 - val_error_in_tfi: 8.5093\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7616 - error_in_tfi: 14.0993 - val_loss: 0.3881 - val_error_in_tfi: 8.5042\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8848 - error_in_tfi: 15.7452 - val_loss: 0.3875 - val_error_in_tfi: 8.4846\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.7467 - error_in_tfi: 14.5472 - val_loss: 0.3872 - val_error_in_tfi: 8.4918\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8165 - error_in_tfi: 14.0562 - val_loss: 0.3870 - val_error_in_tfi: 8.4961\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6380 - error_in_tfi: 12.9103 - val_loss: 0.3869 - val_error_in_tfi: 8.5022\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7262 - error_in_tfi: 13.3724 - val_loss: 0.3864 - val_error_in_tfi: 8.4856\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7670 - error_in_tfi: 14.2874 - val_loss: 0.3857 - val_error_in_tfi: 8.4634\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7424 - error_in_tfi: 13.9223 - val_loss: 0.3856 - val_error_in_tfi: 8.4648\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8285 - error_in_tfi: 15.0533 - val_loss: 0.3849 - val_error_in_tfi: 8.4578\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6734 - error_in_tfi: 13.1576 - val_loss: 0.3845 - val_error_in_tfi: 8.4705\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7072 - error_in_tfi: 13.5120 - val_loss: 0.3836 - val_error_in_tfi: 8.4634\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7060 - error_in_tfi: 13.5354 - val_loss: 0.3827 - val_error_in_tfi: 8.4513\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7283 - error_in_tfi: 13.7559 - val_loss: 0.3822 - val_error_in_tfi: 8.4487\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6894 - error_in_tfi: 13.1522 - val_loss: 0.3817 - val_error_in_tfi: 8.4548\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9079 - error_in_tfi: 15.2756 - val_loss: 0.3808 - val_error_in_tfi: 8.4217\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7614 - error_in_tfi: 13.6529 - val_loss: 0.3804 - val_error_in_tfi: 8.4246\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7978 - error_in_tfi: 14.8041 - val_loss: 0.3801 - val_error_in_tfi: 8.4182\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6409 - error_in_tfi: 12.6394 - val_loss: 0.3795 - val_error_in_tfi: 8.4117\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7402 - error_in_tfi: 14.0344 - val_loss: 0.3787 - val_error_in_tfi: 8.3857\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6898 - error_in_tfi: 13.8942 - val_loss: 0.3786 - val_error_in_tfi: 8.3918\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6786 - error_in_tfi: 12.9546 - val_loss: 0.3780 - val_error_in_tfi: 8.3712\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7019 - error_in_tfi: 13.4064 - val_loss: 0.3773 - val_error_in_tfi: 8.3429\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7396 - error_in_tfi: 14.0880 - val_loss: 0.3769 - val_error_in_tfi: 8.3202\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7853 - error_in_tfi: 14.5501 - val_loss: 0.3766 - val_error_in_tfi: 8.3189\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7124 - error_in_tfi: 13.4839 - val_loss: 0.3761 - val_error_in_tfi: 8.3070\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7525 - error_in_tfi: 13.1259 - val_loss: 0.3754 - val_error_in_tfi: 8.2925\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8145 - error_in_tfi: 14.4420 - val_loss: 0.3750 - val_error_in_tfi: 8.2842\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6550 - error_in_tfi: 12.7824 - val_loss: 0.3742 - val_error_in_tfi: 8.2750\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.7472 - error_in_tfi: 14.4789 - val_loss: 0.3740 - val_error_in_tfi: 8.2768\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.7247 - error_in_tfi: 13.6254 - val_loss: 0.3736 - val_error_in_tfi: 8.2767\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6583 - error_in_tfi: 12.5301 - val_loss: 0.3729 - val_error_in_tfi: 8.2462\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6335 - error_in_tfi: 12.9525 - val_loss: 0.3726 - val_error_in_tfi: 8.2431\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6480 - error_in_tfi: 12.2623 - val_loss: 0.3722 - val_error_in_tfi: 8.2289\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7418 - error_in_tfi: 14.4506 - val_loss: 0.3719 - val_error_in_tfi: 8.2220\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.6610 - error_in_tfi: 12.7414 - val_loss: 0.3721 - val_error_in_tfi: 8.2292\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7307 - error_in_tfi: 12.9982 - val_loss: 0.3718 - val_error_in_tfi: 8.2145\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7830 - error_in_tfi: 14.0091 - val_loss: 0.3718 - val_error_in_tfi: 8.2279\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6720 - error_in_tfi: 13.4199 - val_loss: 0.3712 - val_error_in_tfi: 8.2121\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6413 - error_in_tfi: 13.2161 - val_loss: 0.3705 - val_error_in_tfi: 8.2042\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8400 - error_in_tfi: 14.3565 - val_loss: 0.3697 - val_error_in_tfi: 8.1831\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7236 - error_in_tfi: 13.0073 - val_loss: 0.3692 - val_error_in_tfi: 8.1668\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7401 - error_in_tfi: 13.9173 - val_loss: 0.3690 - val_error_in_tfi: 8.1693\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8703 - error_in_tfi: 15.2932 - val_loss: 0.3683 - val_error_in_tfi: 8.1527\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6674 - error_in_tfi: 12.9539 - val_loss: 0.3680 - val_error_in_tfi: 8.1489\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7036 - error_in_tfi: 12.9314 - val_loss: 0.3677 - val_error_in_tfi: 8.1549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6643 - error_in_tfi: 12.9373 - val_loss: 0.3675 - val_error_in_tfi: 8.1592\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6130 - error_in_tfi: 12.5077 - val_loss: 0.3673 - val_error_in_tfi: 8.1701\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.0816 - error_in_tfi: 16.6480\n",
      "TRAIN: [ 0  1  2  3  4  5  6 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n",
      " 30] TEST: [ 7  8  9 10 11 12]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 5, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 5, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 5, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 5, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 5, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 5, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 5, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 5, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 5, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 5, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 5, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 5, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 5, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 5, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_3[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 5, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 5, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 5, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 5, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 5)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          768         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,641\n",
      "Trainable params: 29,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 1.3295 - error_in_tfi: 18.7887 - val_loss: 0.4944 - val_error_in_tfi: 11.6204\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.3830 - error_in_tfi: 19.6764 - val_loss: 0.4951 - val_error_in_tfi: 11.6335\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.4037 - error_in_tfi: 18.3405 - val_loss: 0.4954 - val_error_in_tfi: 11.6439\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.2175 - error_in_tfi: 16.9899 - val_loss: 0.4951 - val_error_in_tfi: 11.6518\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.1933 - error_in_tfi: 17.4844 - val_loss: 0.4948 - val_error_in_tfi: 11.6570\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.2944 - error_in_tfi: 18.8867 - val_loss: 0.4950 - val_error_in_tfi: 11.6835\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.3605 - error_in_tfi: 19.3194 - val_loss: 0.4950 - val_error_in_tfi: 11.7055\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.1012 - error_in_tfi: 16.8627 - val_loss: 0.4942 - val_error_in_tfi: 11.7011\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 1.0237 - error_in_tfi: 16.7663 - val_loss: 0.4946 - val_error_in_tfi: 11.7290\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1329 - error_in_tfi: 17.0170 - val_loss: 0.4948 - val_error_in_tfi: 11.7476\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.1982 - error_in_tfi: 17.6381 - val_loss: 0.4953 - val_error_in_tfi: 11.7748\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 1.0573 - error_in_tfi: 16.9294 - val_loss: 0.4951 - val_error_in_tfi: 11.7822\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.0457 - error_in_tfi: 16.5202 - val_loss: 0.4954 - val_error_in_tfi: 11.8128\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 1.1179 - error_in_tfi: 17.2783 - val_loss: 0.4952 - val_error_in_tfi: 11.8232\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.1227 - error_in_tfi: 17.5963 - val_loss: 0.4952 - val_error_in_tfi: 11.8365\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.1397 - error_in_tfi: 17.4893 - val_loss: 0.4958 - val_error_in_tfi: 11.8705\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.0238 - error_in_tfi: 16.6431 - val_loss: 0.4968 - val_error_in_tfi: 11.9065\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.1559 - error_in_tfi: 17.2971 - val_loss: 0.4976 - val_error_in_tfi: 11.9458\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.8251 - error_in_tfi: 16.0490\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 19 20 21 22 23 24 25 26 27 28 29\n",
      " 30] TEST: [13 14 15 16 17 18]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 5, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 5, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 5, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 5, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 5, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 5, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layer_normalization_1 (LayerNo  (None, 5, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 5, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 5, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 5, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 5, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 5, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 5, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 5, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 5, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 5, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 5, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 5, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 5)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          768         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,641\n",
      "Trainable params: 29,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1.1746 - error_in_tfi: 17.4716 - val_loss: 0.4854 - val_error_in_tfi: 6.7773\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0798 - error_in_tfi: 16.3943 - val_loss: 0.4853 - val_error_in_tfi: 6.8034\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.9980 - error_in_tfi: 15.7481 - val_loss: 0.4854 - val_error_in_tfi: 6.8406\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 1.1328 - error_in_tfi: 17.5099 - val_loss: 0.4859 - val_error_in_tfi: 6.8707\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.0626 - error_in_tfi: 16.6300 - val_loss: 0.4862 - val_error_in_tfi: 6.9088\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.9660 - error_in_tfi: 15.4432 - val_loss: 0.4873 - val_error_in_tfi: 6.9402\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.0632 - error_in_tfi: 16.0827 - val_loss: 0.4885 - val_error_in_tfi: 6.9789\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.9845 - error_in_tfi: 15.5587 - val_loss: 0.4892 - val_error_in_tfi: 7.0136\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.1505 - error_in_tfi: 17.8975 - val_loss: 0.4897 - val_error_in_tfi: 7.0465\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 1.0436 - error_in_tfi: 16.2503 - val_loss: 0.4915 - val_error_in_tfi: 7.0742\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9675 - error_in_tfi: 15.9305 - val_loss: 0.4927 - val_error_in_tfi: 7.1077\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.9854 - error_in_tfi: 15.7621 - val_loss: 0.4936 - val_error_in_tfi: 7.1530\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.6942 - error_in_tfi: 23.4437\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 25 26 27 28 29\n",
      " 30] TEST: [19 20 21 22 23 24]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 5, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 5, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 5, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 5, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 5, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 5, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 5, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 5, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 5, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 5, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 5, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 5, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 5, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 5, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 5, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 5, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 5, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 5, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 5)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          768         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,641\n",
      "Trainable params: 29,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.6860 - error_in_tfi: 21.6979 - val_loss: 0.6907 - val_error_in_tfi: 7.0470\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.6690 - error_in_tfi: 21.0003 - val_loss: 0.6882 - val_error_in_tfi: 6.9528\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.6355 - error_in_tfi: 21.6712 - val_loss: 0.6848 - val_error_in_tfi: 6.8515\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.5768 - error_in_tfi: 21.3504 - val_loss: 0.6816 - val_error_in_tfi: 6.7494\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 1.7855 - error_in_tfi: 21.8158 - val_loss: 0.6782 - val_error_in_tfi: 6.7884\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.6370 - error_in_tfi: 21.1202 - val_loss: 0.6754 - val_error_in_tfi: 6.8598\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.6969 - error_in_tfi: 21.2024 - val_loss: 0.6728 - val_error_in_tfi: 6.9375\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 1.5490 - error_in_tfi: 20.1967 - val_loss: 0.6704 - val_error_in_tfi: 7.0105\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 1.6828 - error_in_tfi: 21.2444 - val_loss: 0.6673 - val_error_in_tfi: 7.0694\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 1.7154 - error_in_tfi: 22.2809 - val_loss: 0.6641 - val_error_in_tfi: 7.1391\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.2799 - error_in_tfi: 18.3844 - val_loss: 0.6613 - val_error_in_tfi: 7.2037\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 1.4655 - error_in_tfi: 19.3401 - val_loss: 0.6586 - val_error_in_tfi: 7.2763\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.4666 - error_in_tfi: 19.7900 - val_loss: 0.6553 - val_error_in_tfi: 7.3405\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 1.6476 - error_in_tfi: 20.7299 - val_loss: 0.6526 - val_error_in_tfi: 7.4085\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.4607 - error_in_tfi: 19.7370 - val_loss: 0.6505 - val_error_in_tfi: 7.4905\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.4130 - error_in_tfi: 19.0615 - val_loss: 0.6476 - val_error_in_tfi: 7.5539\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.5527 - error_in_tfi: 20.7405 - val_loss: 0.6459 - val_error_in_tfi: 7.6348\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.4897 - error_in_tfi: 19.7290 - val_loss: 0.6446 - val_error_in_tfi: 7.7118\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.5081 - error_in_tfi: 20.3531 - val_loss: 0.6429 - val_error_in_tfi: 7.7874\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.4669 - error_in_tfi: 20.1961 - val_loss: 0.6413 - val_error_in_tfi: 7.8670\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 151ms/step - loss: 1.5683 - error_in_tfi: 20.4724 - val_loss: 0.6389 - val_error_in_tfi: 7.9353\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.4955 - error_in_tfi: 20.9037 - val_loss: 0.6371 - val_error_in_tfi: 8.0061\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.4370 - error_in_tfi: 19.9183 - val_loss: 0.6355 - val_error_in_tfi: 8.0719\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.3633 - error_in_tfi: 18.7882 - val_loss: 0.6335 - val_error_in_tfi: 8.1341\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.4136 - error_in_tfi: 19.0890 - val_loss: 0.6320 - val_error_in_tfi: 8.2001\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.3586 - error_in_tfi: 19.3348 - val_loss: 0.6304 - val_error_in_tfi: 8.2565\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.5130 - error_in_tfi: 19.8459 - val_loss: 0.6286 - val_error_in_tfi: 8.3267\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.4325 - error_in_tfi: 19.6832 - val_loss: 0.6272 - val_error_in_tfi: 8.3968\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.4792 - error_in_tfi: 20.6806 - val_loss: 0.6260 - val_error_in_tfi: 8.4627\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.4794 - error_in_tfi: 20.7783 - val_loss: 0.6253 - val_error_in_tfi: 8.5379\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.4091 - error_in_tfi: 19.6872 - val_loss: 0.6242 - val_error_in_tfi: 8.6069\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 1.3247 - error_in_tfi: 19.4889 - val_loss: 0.6234 - val_error_in_tfi: 8.6784\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.3107 - error_in_tfi: 19.4915 - val_loss: 0.6219 - val_error_in_tfi: 8.7400\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 1.4855 - error_in_tfi: 20.6679 - val_loss: 0.6208 - val_error_in_tfi: 8.8067\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 1.2677 - error_in_tfi: 19.3948 - val_loss: 0.6192 - val_error_in_tfi: 8.8620\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.3870 - error_in_tfi: 19.9979 - val_loss: 0.6179 - val_error_in_tfi: 8.9224\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 1.3648 - error_in_tfi: 19.3275 - val_loss: 0.6166 - val_error_in_tfi: 8.9936\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 1.4332 - error_in_tfi: 20.4541 - val_loss: 0.6152 - val_error_in_tfi: 9.0464\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 1.3529 - error_in_tfi: 18.9660 - val_loss: 0.6148 - val_error_in_tfi: 9.1253\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 1.3110 - error_in_tfi: 19.6634 - val_loss: 0.6143 - val_error_in_tfi: 9.1972\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.3003 - error_in_tfi: 19.5892 - val_loss: 0.6139 - val_error_in_tfi: 9.2654\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 1.3025 - error_in_tfi: 19.0203 - val_loss: 0.6131 - val_error_in_tfi: 9.3240\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.2560 - error_in_tfi: 18.8259 - val_loss: 0.6125 - val_error_in_tfi: 9.3880\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.1333 - error_in_tfi: 17.5447 - val_loss: 0.6122 - val_error_in_tfi: 9.4641\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.1338 - error_in_tfi: 17.9008 - val_loss: 0.6115 - val_error_in_tfi: 9.5363\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.0933 - error_in_tfi: 17.2323 - val_loss: 0.6110 - val_error_in_tfi: 9.6115\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.2562 - error_in_tfi: 18.8627 - val_loss: 0.6099 - val_error_in_tfi: 9.6690\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.2179 - error_in_tfi: 18.4255 - val_loss: 0.6099 - val_error_in_tfi: 9.7453\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.1078 - error_in_tfi: 17.7577 - val_loss: 0.6096 - val_error_in_tfi: 9.8114\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.1504 - error_in_tfi: 17.7610 - val_loss: 0.6084 - val_error_in_tfi: 9.8589\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.3563 - error_in_tfi: 19.1893 - val_loss: 0.6082 - val_error_in_tfi: 9.9328\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.2711 - error_in_tfi: 18.3969 - val_loss: 0.6086 - val_error_in_tfi: 10.0085\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.1471 - error_in_tfi: 18.2761 - val_loss: 0.6075 - val_error_in_tfi: 10.0622\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 1.1580 - error_in_tfi: 17.3784 - val_loss: 0.6072 - val_error_in_tfi: 10.1284\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.2533 - error_in_tfi: 18.7383 - val_loss: 0.6066 - val_error_in_tfi: 10.1804\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.1798 - error_in_tfi: 17.9898 - val_loss: 0.6067 - val_error_in_tfi: 10.2507\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.0936 - error_in_tfi: 17.8409 - val_loss: 0.6060 - val_error_in_tfi: 10.3160\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.2345 - error_in_tfi: 18.6737 - val_loss: 0.6054 - val_error_in_tfi: 10.3883\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 1.1284 - error_in_tfi: 17.4316 - val_loss: 0.6045 - val_error_in_tfi: 10.4442\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.0312 - error_in_tfi: 17.0520 - val_loss: 0.6040 - val_error_in_tfi: 10.4960\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 1.2216 - error_in_tfi: 18.7532 - val_loss: 0.6037 - val_error_in_tfi: 10.5619\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.1767 - error_in_tfi: 18.4181 - val_loss: 0.6033 - val_error_in_tfi: 10.6199\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.0707 - error_in_tfi: 17.2025 - val_loss: 0.6040 - val_error_in_tfi: 10.7004\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9245 - error_in_tfi: 15.2919 - val_loss: 0.6036 - val_error_in_tfi: 10.7615\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.0752 - error_in_tfi: 17.6456 - val_loss: 0.6033 - val_error_in_tfi: 10.8254\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.1241 - error_in_tfi: 18.2978 - val_loss: 0.6036 - val_error_in_tfi: 10.8908\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.1839 - error_in_tfi: 18.2714 - val_loss: 0.6033 - val_error_in_tfi: 10.9582\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.0802 - error_in_tfi: 16.9324 - val_loss: 0.6034 - val_error_in_tfi: 11.0233\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.0709 - error_in_tfi: 17.6924 - val_loss: 0.6038 - val_error_in_tfi: 11.0873\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.9838 - error_in_tfi: 16.5149 - val_loss: 0.6039 - val_error_in_tfi: 11.1450\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.1215 - error_in_tfi: 17.4510 - val_loss: 0.6041 - val_error_in_tfi: 11.2098\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.0911 - error_in_tfi: 16.7411 - val_loss: 0.6046 - val_error_in_tfi: 11.2745\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.1226 - error_in_tfi: 17.7387 - val_loss: 0.6049 - val_error_in_tfi: 11.3388\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.0201 - error_in_tfi: 16.9592 - val_loss: 0.6048 - val_error_in_tfi: 11.3950\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 141ms/step - loss: 1.0094 - error_in_tfi: 16.5303 - val_loss: 0.6048 - val_error_in_tfi: 11.4468\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.0598 - error_in_tfi: 16.6647 - val_loss: 0.6044 - val_error_in_tfi: 11.4943\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 1.1683 - error_in_tfi: 18.1589 - val_loss: 0.6043 - val_error_in_tfi: 11.5548\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5391 - error_in_tfi: 12.1384\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24] TEST: [25 26 27 28 29 30]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 5, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 5, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 5, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 5, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 5, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 5, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 5, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 5, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 5, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 5, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 5, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 5, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 5, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 5, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 5, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 5, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_6 (Dropout)            (None, 5, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 5, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 5, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 5, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 5, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 5, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 5, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 5)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          768         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,641\n",
      "Trainable params: 29,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.1417 - error_in_tfi: 16.9791 - val_loss: 1.2238 - val_error_in_tfi: 14.0325\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.1210 - error_in_tfi: 17.3772 - val_loss: 1.2293 - val_error_in_tfi: 14.0834\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.9742 - error_in_tfi: 15.9109 - val_loss: 1.2327 - val_error_in_tfi: 14.1186\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.9407 - error_in_tfi: 15.7682 - val_loss: 1.2387 - val_error_in_tfi: 14.1578\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.1952 - error_in_tfi: 17.7059 - val_loss: 1.2441 - val_error_in_tfi: 14.2031\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 1.0604 - error_in_tfi: 17.1713 - val_loss: 1.2493 - val_error_in_tfi: 14.2420\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 1.0432 - error_in_tfi: 17.0178 - val_loss: 1.2561 - val_error_in_tfi: 14.2799\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.9229 - error_in_tfi: 15.9372 - val_loss: 1.2632 - val_error_in_tfi: 14.3220\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.0526 - error_in_tfi: 17.1998 - val_loss: 1.2708 - val_error_in_tfi: 14.3643\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.9373 - error_in_tfi: 15.1547 - val_loss: 1.2796 - val_error_in_tfi: 14.4078\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1478 - error_in_tfi: 17.2549 - val_loss: 1.2873 - val_error_in_tfi: 14.4559\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.8969 - error_in_tfi: 14.4266\n",
      "(30, 4, 1) (30, 1)\n",
      "TRAIN: [ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] TEST: [0 1 2 3 4 5]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 4, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 4, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 4, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 4, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_2 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 4, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 4, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 4, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 4)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          640         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,513\n",
      "Trainable params: 29,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 1.0935 - error_in_tfi: 18.7140 - val_loss: 1.6373 - val_error_in_tfi: 23.8758\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.2899 - error_in_tfi: 20.4308 - val_loss: 1.6303 - val_error_in_tfi: 23.8134\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.2928 - error_in_tfi: 20.6444 - val_loss: 1.6176 - val_error_in_tfi: 23.7112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.2098 - error_in_tfi: 19.3872 - val_loss: 1.6071 - val_error_in_tfi: 23.6269\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.2595 - error_in_tfi: 19.9661 - val_loss: 1.5947 - val_error_in_tfi: 23.5262\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1743 - error_in_tfi: 19.1612 - val_loss: 1.5845 - val_error_in_tfi: 23.4502\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.2243 - error_in_tfi: 19.0268 - val_loss: 1.5711 - val_error_in_tfi: 23.3490\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.2599 - error_in_tfi: 20.2731 - val_loss: 1.5586 - val_error_in_tfi: 23.2503\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.3418 - error_in_tfi: 19.9589 - val_loss: 1.5462 - val_error_in_tfi: 23.1551\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.3422 - error_in_tfi: 20.5691 - val_loss: 1.5335 - val_error_in_tfi: 23.0574\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.2424 - error_in_tfi: 20.0966 - val_loss: 1.5236 - val_error_in_tfi: 22.9800\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.0723 - error_in_tfi: 18.7421 - val_loss: 1.5137 - val_error_in_tfi: 22.9018\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0714 - error_in_tfi: 19.0826 - val_loss: 1.5039 - val_error_in_tfi: 22.8235\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1295 - error_in_tfi: 18.6700 - val_loss: 1.4939 - val_error_in_tfi: 22.7406\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1816 - error_in_tfi: 19.2445 - val_loss: 1.4863 - val_error_in_tfi: 22.6768\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1923 - error_in_tfi: 18.8449 - val_loss: 1.4788 - val_error_in_tfi: 22.6119\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1957 - error_in_tfi: 19.7633 - val_loss: 1.4679 - val_error_in_tfi: 22.5212\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.2168 - error_in_tfi: 20.3920 - val_loss: 1.4597 - val_error_in_tfi: 22.4509\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1370 - error_in_tfi: 18.8922 - val_loss: 1.4493 - val_error_in_tfi: 22.3637\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9253 - error_in_tfi: 16.9986 - val_loss: 1.4403 - val_error_in_tfi: 22.2901\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0312 - error_in_tfi: 18.1426 - val_loss: 1.4316 - val_error_in_tfi: 22.2129\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1845 - error_in_tfi: 18.6191 - val_loss: 1.4225 - val_error_in_tfi: 22.1325\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9741 - error_in_tfi: 17.7930 - val_loss: 1.4153 - val_error_in_tfi: 22.0662\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1795 - error_in_tfi: 19.4232 - val_loss: 1.4075 - val_error_in_tfi: 21.9916\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1105 - error_in_tfi: 18.5904 - val_loss: 1.4008 - val_error_in_tfi: 21.9261\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1627 - error_in_tfi: 18.3790 - val_loss: 1.3925 - val_error_in_tfi: 21.8538\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0331 - error_in_tfi: 18.5919 - val_loss: 1.3816 - val_error_in_tfi: 21.7589\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0561 - error_in_tfi: 17.8446 - val_loss: 1.3705 - val_error_in_tfi: 21.6624\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0053 - error_in_tfi: 18.1538 - val_loss: 1.3598 - val_error_in_tfi: 21.5691\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0419 - error_in_tfi: 18.0333 - val_loss: 1.3505 - val_error_in_tfi: 21.4852\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0774 - error_in_tfi: 18.1259 - val_loss: 1.3404 - val_error_in_tfi: 21.3955\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.0342 - error_in_tfi: 17.9618 - val_loss: 1.3311 - val_error_in_tfi: 21.3155\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0415 - error_in_tfi: 17.7276 - val_loss: 1.3216 - val_error_in_tfi: 21.2348\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0033 - error_in_tfi: 17.4837 - val_loss: 1.3119 - val_error_in_tfi: 21.1563\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0460 - error_in_tfi: 18.5738 - val_loss: 1.3007 - val_error_in_tfi: 21.0607\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9315 - error_in_tfi: 16.5459 - val_loss: 1.2908 - val_error_in_tfi: 20.9728\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9087 - error_in_tfi: 17.2967 - val_loss: 1.2841 - val_error_in_tfi: 20.9101\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0603 - error_in_tfi: 18.5632 - val_loss: 1.2751 - val_error_in_tfi: 20.8242\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9540 - error_in_tfi: 17.2718 - val_loss: 1.2670 - val_error_in_tfi: 20.7474\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0134 - error_in_tfi: 18.0482 - val_loss: 1.2613 - val_error_in_tfi: 20.6950\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9311 - error_in_tfi: 17.3528 - val_loss: 1.2560 - val_error_in_tfi: 20.6392\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0013 - error_in_tfi: 17.9814 - val_loss: 1.2506 - val_error_in_tfi: 20.5833\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9225 - error_in_tfi: 16.6526 - val_loss: 1.2460 - val_error_in_tfi: 20.5338\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8598 - error_in_tfi: 16.7100 - val_loss: 1.2398 - val_error_in_tfi: 20.4740\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9234 - error_in_tfi: 16.6802 - val_loss: 1.2345 - val_error_in_tfi: 20.4217\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9451 - error_in_tfi: 16.9789 - val_loss: 1.2286 - val_error_in_tfi: 20.3635\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9467 - error_in_tfi: 17.1310 - val_loss: 1.2227 - val_error_in_tfi: 20.3024\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9134 - error_in_tfi: 17.0074 - val_loss: 1.2161 - val_error_in_tfi: 20.2333\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9283 - error_in_tfi: 17.1646 - val_loss: 1.2114 - val_error_in_tfi: 20.1852\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9177 - error_in_tfi: 17.1532 - val_loss: 1.2033 - val_error_in_tfi: 20.1097\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8542 - error_in_tfi: 16.0695 - val_loss: 1.1968 - val_error_in_tfi: 20.0477\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8745 - error_in_tfi: 16.3298 - val_loss: 1.1892 - val_error_in_tfi: 19.9788\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8076 - error_in_tfi: 16.3030 - val_loss: 1.1845 - val_error_in_tfi: 19.9335\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7940 - error_in_tfi: 15.3829 - val_loss: 1.1790 - val_error_in_tfi: 19.8793\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8324 - error_in_tfi: 16.9258 - val_loss: 1.1731 - val_error_in_tfi: 19.8221\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8212 - error_in_tfi: 15.6452 - val_loss: 1.1670 - val_error_in_tfi: 19.7640\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8670 - error_in_tfi: 16.8462 - val_loss: 1.1609 - val_error_in_tfi: 19.7052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8175 - error_in_tfi: 16.0376 - val_loss: 1.1542 - val_error_in_tfi: 19.6424\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8096 - error_in_tfi: 16.2738 - val_loss: 1.1476 - val_error_in_tfi: 19.5813\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8353 - error_in_tfi: 15.5820 - val_loss: 1.1407 - val_error_in_tfi: 19.5159\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8353 - error_in_tfi: 15.9925 - val_loss: 1.1350 - val_error_in_tfi: 19.4600\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8296 - error_in_tfi: 15.8084 - val_loss: 1.1283 - val_error_in_tfi: 19.3981\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7772 - error_in_tfi: 15.4762 - val_loss: 1.1231 - val_error_in_tfi: 19.3464\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8197 - error_in_tfi: 15.9450 - val_loss: 1.1179 - val_error_in_tfi: 19.2911\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9127 - error_in_tfi: 16.4859 - val_loss: 1.1115 - val_error_in_tfi: 19.2291\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7858 - error_in_tfi: 15.3386 - val_loss: 1.1071 - val_error_in_tfi: 19.1846\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7487 - error_in_tfi: 15.7317 - val_loss: 1.1032 - val_error_in_tfi: 19.1407\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8365 - error_in_tfi: 15.8005 - val_loss: 1.0989 - val_error_in_tfi: 19.0951\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8528 - error_in_tfi: 15.8376 - val_loss: 1.0917 - val_error_in_tfi: 19.0315\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7824 - error_in_tfi: 14.9497 - val_loss: 1.0851 - val_error_in_tfi: 18.9693\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8103 - error_in_tfi: 15.3687 - val_loss: 1.0779 - val_error_in_tfi: 18.9020\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7235 - error_in_tfi: 14.7684 - val_loss: 1.0697 - val_error_in_tfi: 18.8280\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7759 - error_in_tfi: 15.0160 - val_loss: 1.0627 - val_error_in_tfi: 18.7682\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6993 - error_in_tfi: 15.3726 - val_loss: 1.0548 - val_error_in_tfi: 18.6987\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8201 - error_in_tfi: 16.3313 - val_loss: 1.0457 - val_error_in_tfi: 18.6149\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8189 - error_in_tfi: 15.5402 - val_loss: 1.0387 - val_error_in_tfi: 18.5522\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8343 - error_in_tfi: 16.6048 - val_loss: 1.0304 - val_error_in_tfi: 18.4791\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8505 - error_in_tfi: 16.2751 - val_loss: 1.0220 - val_error_in_tfi: 18.4072\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7331 - error_in_tfi: 14.1033 - val_loss: 1.0167 - val_error_in_tfi: 18.3579\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7482 - error_in_tfi: 15.7129 - val_loss: 1.0105 - val_error_in_tfi: 18.3029\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7457 - error_in_tfi: 14.7552 - val_loss: 1.0039 - val_error_in_tfi: 18.2398\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7100 - error_in_tfi: 15.0650 - val_loss: 0.9967 - val_error_in_tfi: 18.1728\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7979 - error_in_tfi: 16.1507 - val_loss: 0.9900 - val_error_in_tfi: 18.1059\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7624 - error_in_tfi: 14.4879 - val_loss: 0.9849 - val_error_in_tfi: 18.0566\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8391 - error_in_tfi: 16.5399 - val_loss: 0.9797 - val_error_in_tfi: 18.0028\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8189 - error_in_tfi: 15.7575 - val_loss: 0.9734 - val_error_in_tfi: 17.9447\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6344 - error_in_tfi: 14.5667 - val_loss: 0.9703 - val_error_in_tfi: 17.9134\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6893 - error_in_tfi: 14.0143 - val_loss: 0.9667 - val_error_in_tfi: 17.8739\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7026 - error_in_tfi: 14.7667 - val_loss: 0.9612 - val_error_in_tfi: 17.8140\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7739 - error_in_tfi: 15.3778 - val_loss: 0.9549 - val_error_in_tfi: 17.7492\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7599 - error_in_tfi: 15.1848 - val_loss: 0.9488 - val_error_in_tfi: 17.6870\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7667 - error_in_tfi: 16.1184 - val_loss: 0.9443 - val_error_in_tfi: 17.6399\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7428 - error_in_tfi: 14.9882 - val_loss: 0.9398 - val_error_in_tfi: 17.5928\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7435 - error_in_tfi: 15.6611 - val_loss: 0.9348 - val_error_in_tfi: 17.5442\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6770 - error_in_tfi: 14.4048 - val_loss: 0.9301 - val_error_in_tfi: 17.4972\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7175 - error_in_tfi: 15.0711 - val_loss: 0.9250 - val_error_in_tfi: 17.4418\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6774 - error_in_tfi: 14.8654 - val_loss: 0.9211 - val_error_in_tfi: 17.4037\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7385 - error_in_tfi: 15.2285 - val_loss: 0.9149 - val_error_in_tfi: 17.3371\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7357 - error_in_tfi: 14.4832 - val_loss: 0.9098 - val_error_in_tfi: 17.2825\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7751 - error_in_tfi: 14.9946 - val_loss: 0.9037 - val_error_in_tfi: 17.2167\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8165 - error_in_tfi: 16.0973 - val_loss: 0.8991 - val_error_in_tfi: 17.1659\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6845 - error_in_tfi: 13.9752 - val_loss: 0.8948 - val_error_in_tfi: 17.1251\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7043 - error_in_tfi: 14.0699 - val_loss: 0.8905 - val_error_in_tfi: 17.0777\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6568 - error_in_tfi: 14.5858 - val_loss: 0.8868 - val_error_in_tfi: 17.0362\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6402 - error_in_tfi: 14.1247 - val_loss: 0.8804 - val_error_in_tfi: 16.9701\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6661 - error_in_tfi: 13.8509 - val_loss: 0.8736 - val_error_in_tfi: 16.9010\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7547 - error_in_tfi: 15.3343 - val_loss: 0.8695 - val_error_in_tfi: 16.8569\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6504 - error_in_tfi: 14.9406 - val_loss: 0.8654 - val_error_in_tfi: 16.8172\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6527 - error_in_tfi: 13.2223 - val_loss: 0.8599 - val_error_in_tfi: 16.7604\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6470 - error_in_tfi: 13.3825 - val_loss: 0.8519 - val_error_in_tfi: 16.6837\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6427 - error_in_tfi: 13.3969 - val_loss: 0.8461 - val_error_in_tfi: 16.6221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6381 - error_in_tfi: 12.8310 - val_loss: 0.8400 - val_error_in_tfi: 16.5631\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6963 - error_in_tfi: 14.4521 - val_loss: 0.8344 - val_error_in_tfi: 16.5039\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5866 - error_in_tfi: 13.4547 - val_loss: 0.8296 - val_error_in_tfi: 16.4569\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8176 - error_in_tfi: 16.0614 - val_loss: 0.8225 - val_error_in_tfi: 16.3849\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6932 - error_in_tfi: 14.4839 - val_loss: 0.8189 - val_error_in_tfi: 16.3466\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6799 - error_in_tfi: 13.7213 - val_loss: 0.8152 - val_error_in_tfi: 16.3125\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6117 - error_in_tfi: 13.7100 - val_loss: 0.8108 - val_error_in_tfi: 16.2658\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7231 - error_in_tfi: 15.1355 - val_loss: 0.8058 - val_error_in_tfi: 16.2170\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6475 - error_in_tfi: 13.7795 - val_loss: 0.7983 - val_error_in_tfi: 16.1436\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6856 - error_in_tfi: 14.4199 - val_loss: 0.7939 - val_error_in_tfi: 16.0995\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6509 - error_in_tfi: 14.4992 - val_loss: 0.7887 - val_error_in_tfi: 16.0473\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5594 - error_in_tfi: 13.2735 - val_loss: 0.7819 - val_error_in_tfi: 15.9778\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6629 - error_in_tfi: 13.3265 - val_loss: 0.7770 - val_error_in_tfi: 15.9240\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6831 - error_in_tfi: 14.1879 - val_loss: 0.7724 - val_error_in_tfi: 15.8776\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6891 - error_in_tfi: 14.5079 - val_loss: 0.7680 - val_error_in_tfi: 15.8316\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6237 - error_in_tfi: 13.6758 - val_loss: 0.7637 - val_error_in_tfi: 15.7812\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5629 - error_in_tfi: 12.9796 - val_loss: 0.7578 - val_error_in_tfi: 15.7204\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6184 - error_in_tfi: 13.6020 - val_loss: 0.7515 - val_error_in_tfi: 15.6518\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5931 - error_in_tfi: 13.6926 - val_loss: 0.7472 - val_error_in_tfi: 15.6086\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5808 - error_in_tfi: 12.9067 - val_loss: 0.7416 - val_error_in_tfi: 15.5488\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6627 - error_in_tfi: 14.2923 - val_loss: 0.7351 - val_error_in_tfi: 15.4743\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6195 - error_in_tfi: 13.4736 - val_loss: 0.7311 - val_error_in_tfi: 15.4279\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6227 - error_in_tfi: 13.6909 - val_loss: 0.7274 - val_error_in_tfi: 15.3878\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6432 - error_in_tfi: 13.4108 - val_loss: 0.7221 - val_error_in_tfi: 15.3271\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.6033 - error_in_tfi: 12.3220 - val_loss: 0.7173 - val_error_in_tfi: 15.2755\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.6380 - error_in_tfi: 13.4704 - val_loss: 0.7118 - val_error_in_tfi: 15.2170\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.7212 - error_in_tfi: 14.8382 - val_loss: 0.7067 - val_error_in_tfi: 15.1617\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6864 - error_in_tfi: 13.9331 - val_loss: 0.7008 - val_error_in_tfi: 15.1030\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6117 - error_in_tfi: 13.1607 - val_loss: 0.6958 - val_error_in_tfi: 15.0476\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6097 - error_in_tfi: 13.0710 - val_loss: 0.6896 - val_error_in_tfi: 14.9801\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5900 - error_in_tfi: 14.1068 - val_loss: 0.6871 - val_error_in_tfi: 14.9504\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6254 - error_in_tfi: 13.1434 - val_loss: 0.6845 - val_error_in_tfi: 14.9203\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5567 - error_in_tfi: 12.5701 - val_loss: 0.6818 - val_error_in_tfi: 14.8913\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 146s 37s/step - loss: 0.5251 - error_in_tfi: 12.1344 - val_loss: 0.6775 - val_error_in_tfi: 14.8434\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5844 - error_in_tfi: 12.8620 - val_loss: 0.6731 - val_error_in_tfi: 14.7925\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 0.5475 - error_in_tfi: 13.0440 - val_loss: 0.6690 - val_error_in_tfi: 14.7474\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.5499 - error_in_tfi: 13.0229 - val_loss: 0.6667 - val_error_in_tfi: 14.7179\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.5959 - error_in_tfi: 12.9417 - val_loss: 0.6620 - val_error_in_tfi: 14.6628\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5317 - error_in_tfi: 12.1613 - val_loss: 0.6585 - val_error_in_tfi: 14.6218\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.7121 - error_in_tfi: 14.7591 - val_loss: 0.6546 - val_error_in_tfi: 14.5711\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5914 - error_in_tfi: 12.7969 - val_loss: 0.6507 - val_error_in_tfi: 14.5199\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6357 - error_in_tfi: 14.3460 - val_loss: 0.6486 - val_error_in_tfi: 14.4935\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6138 - error_in_tfi: 13.7014 - val_loss: 0.6437 - val_error_in_tfi: 14.4343\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.6615 - error_in_tfi: 14.0476 - val_loss: 0.6405 - val_error_in_tfi: 14.3913\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.5574 - error_in_tfi: 12.3343 - val_loss: 0.6351 - val_error_in_tfi: 14.3309\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6586 - error_in_tfi: 14.2363 - val_loss: 0.6298 - val_error_in_tfi: 14.2715\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6053 - error_in_tfi: 13.5325 - val_loss: 0.6262 - val_error_in_tfi: 14.2315\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.5221 - error_in_tfi: 11.8505 - val_loss: 0.6219 - val_error_in_tfi: 14.1823\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6084 - error_in_tfi: 12.9531 - val_loss: 0.6176 - val_error_in_tfi: 14.1332\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6386 - error_in_tfi: 14.0365 - val_loss: 0.6140 - val_error_in_tfi: 14.0887\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6439 - error_in_tfi: 13.9854 - val_loss: 0.6094 - val_error_in_tfi: 14.0310\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5330 - error_in_tfi: 12.5957 - val_loss: 0.6039 - val_error_in_tfi: 13.9670\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6096 - error_in_tfi: 12.9012 - val_loss: 0.6000 - val_error_in_tfi: 13.9222\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6605 - error_in_tfi: 14.2494 - val_loss: 0.5961 - val_error_in_tfi: 13.8741\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4903 - error_in_tfi: 12.2093 - val_loss: 0.5935 - val_error_in_tfi: 13.8434\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.6175 - error_in_tfi: 13.1421 - val_loss: 0.5878 - val_error_in_tfi: 13.7799\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.5670 - error_in_tfi: 13.0857 - val_loss: 0.5845 - val_error_in_tfi: 13.7409\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6850 - error_in_tfi: 13.9714 - val_loss: 0.5780 - val_error_in_tfi: 13.6617\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.5232 - error_in_tfi: 12.9763 - val_loss: 0.5768 - val_error_in_tfi: 13.6456\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.5045 - error_in_tfi: 12.0758 - val_loss: 0.5736 - val_error_in_tfi: 13.6070\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5365 - error_in_tfi: 12.0969 - val_loss: 0.5680 - val_error_in_tfi: 13.5412\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5703 - error_in_tfi: 13.0877 - val_loss: 0.5648 - val_error_in_tfi: 13.5023\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.5812 - error_in_tfi: 12.9517 - val_loss: 0.5615 - val_error_in_tfi: 13.4602\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.5519 - error_in_tfi: 12.1491 - val_loss: 0.5576 - val_error_in_tfi: 13.4113\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.5104 - error_in_tfi: 11.3500 - val_loss: 0.5539 - val_error_in_tfi: 13.3631\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5154 - error_in_tfi: 12.4440 - val_loss: 0.5505 - val_error_in_tfi: 13.3223\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5435 - error_in_tfi: 12.3273 - val_loss: 0.5456 - val_error_in_tfi: 13.2598\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5712 - error_in_tfi: 12.7763 - val_loss: 0.5433 - val_error_in_tfi: 13.2324\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5135 - error_in_tfi: 12.8602 - val_loss: 0.5381 - val_error_in_tfi: 13.1719\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5560 - error_in_tfi: 12.2120 - val_loss: 0.5336 - val_error_in_tfi: 13.1171\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5982 - error_in_tfi: 13.2306 - val_loss: 0.5313 - val_error_in_tfi: 13.0842\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5005 - error_in_tfi: 12.5499 - val_loss: 0.5280 - val_error_in_tfi: 13.0423\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5492 - error_in_tfi: 12.8195 - val_loss: 0.5238 - val_error_in_tfi: 12.9922\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5280 - error_in_tfi: 11.9695 - val_loss: 0.5186 - val_error_in_tfi: 12.9270\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6031 - error_in_tfi: 12.4850 - val_loss: 0.5135 - val_error_in_tfi: 12.8620\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5553 - error_in_tfi: 12.7338 - val_loss: 0.5105 - val_error_in_tfi: 12.8188\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5275 - error_in_tfi: 12.3549 - val_loss: 0.5088 - val_error_in_tfi: 12.7924\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5185 - error_in_tfi: 11.2986 - val_loss: 0.5052 - val_error_in_tfi: 12.7426\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5403 - error_in_tfi: 12.2076 - val_loss: 0.5014 - val_error_in_tfi: 12.6903\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6874 - error_in_tfi: 13.3108 - val_loss: 0.4994 - val_error_in_tfi: 12.6600\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5148 - error_in_tfi: 11.2771 - val_loss: 0.4982 - val_error_in_tfi: 12.6438\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6178 - error_in_tfi: 13.1728 - val_loss: 0.4937 - val_error_in_tfi: 12.5841\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5143 - error_in_tfi: 11.5741 - val_loss: 0.4909 - val_error_in_tfi: 12.5479\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6050 - error_in_tfi: 12.7372 - val_loss: 0.4883 - val_error_in_tfi: 12.5125\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5439 - error_in_tfi: 11.8755 - val_loss: 0.4858 - val_error_in_tfi: 12.4782\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5831 - error_in_tfi: 12.9042 - val_loss: 0.4828 - val_error_in_tfi: 12.4391\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5404 - error_in_tfi: 12.3551 - val_loss: 0.4795 - val_error_in_tfi: 12.3910\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5164 - error_in_tfi: 12.0780 - val_loss: 0.4756 - val_error_in_tfi: 12.3409\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5208 - error_in_tfi: 12.2390 - val_loss: 0.4721 - val_error_in_tfi: 12.2975\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.2394 - error_in_tfi: 20.6121\n",
      "TRAIN: [ 0  1  2  3  4  5 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] TEST: [ 6  7  8  9 10 11]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 4, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 4, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 4, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 4, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_1[0][0]',  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 4, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 4, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 4, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 4)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          640         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,513\n",
      "Trainable params: 29,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 1.2180 - error_in_tfi: 20.7237 - val_loss: 1.2075 - val_error_in_tfi: 19.9697\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.0759 - error_in_tfi: 18.8438 - val_loss: 1.1974 - val_error_in_tfi: 19.8830\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.0689 - error_in_tfi: 18.2509 - val_loss: 1.1892 - val_error_in_tfi: 19.8106\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1727 - error_in_tfi: 18.7855 - val_loss: 1.1788 - val_error_in_tfi: 19.7223\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.1056 - error_in_tfi: 19.8620 - val_loss: 1.1673 - val_error_in_tfi: 19.6283\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1472 - error_in_tfi: 19.7178 - val_loss: 1.1559 - val_error_in_tfi: 19.5335\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1773 - error_in_tfi: 19.6769 - val_loss: 1.1447 - val_error_in_tfi: 19.4394\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.0321 - error_in_tfi: 18.5409 - val_loss: 1.1356 - val_error_in_tfi: 19.3628\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1277 - error_in_tfi: 19.9081 - val_loss: 1.1255 - val_error_in_tfi: 19.2767\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0614 - error_in_tfi: 18.8084 - val_loss: 1.1152 - val_error_in_tfi: 19.1865\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1251 - error_in_tfi: 18.2654 - val_loss: 1.1084 - val_error_in_tfi: 19.1226\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.1276 - error_in_tfi: 18.8022 - val_loss: 1.1021 - val_error_in_tfi: 19.0651\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.0483 - error_in_tfi: 17.6142 - val_loss: 1.0942 - val_error_in_tfi: 18.9925\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.2144 - error_in_tfi: 20.0157 - val_loss: 1.0862 - val_error_in_tfi: 18.9191\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1120 - error_in_tfi: 19.4674 - val_loss: 1.0790 - val_error_in_tfi: 18.8569\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1280 - error_in_tfi: 19.3968 - val_loss: 1.0681 - val_error_in_tfi: 18.7590\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.1521 - error_in_tfi: 19.3225 - val_loss: 1.0611 - val_error_in_tfi: 18.6971\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.0350 - error_in_tfi: 18.6408 - val_loss: 1.0515 - val_error_in_tfi: 18.6141\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0402 - error_in_tfi: 18.8887 - val_loss: 1.0440 - val_error_in_tfi: 18.5457\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0794 - error_in_tfi: 18.9137 - val_loss: 1.0343 - val_error_in_tfi: 18.4573\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9887 - error_in_tfi: 17.2343 - val_loss: 1.0274 - val_error_in_tfi: 18.3955\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1059 - error_in_tfi: 18.9316 - val_loss: 1.0191 - val_error_in_tfi: 18.3227\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.0283 - error_in_tfi: 18.4450 - val_loss: 1.0105 - val_error_in_tfi: 18.2441\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.1165 - error_in_tfi: 19.2778 - val_loss: 1.0027 - val_error_in_tfi: 18.1751\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9881 - error_in_tfi: 18.1958 - val_loss: 0.9943 - val_error_in_tfi: 18.1003\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0241 - error_in_tfi: 18.7470 - val_loss: 0.9854 - val_error_in_tfi: 18.0240\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9592 - error_in_tfi: 17.3841 - val_loss: 0.9765 - val_error_in_tfi: 17.9468\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.0009 - error_in_tfi: 17.4652 - val_loss: 0.9693 - val_error_in_tfi: 17.8838\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9112 - error_in_tfi: 17.7172 - val_loss: 0.9608 - val_error_in_tfi: 17.8070\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8886 - error_in_tfi: 16.5318 - val_loss: 0.9550 - val_error_in_tfi: 17.7553\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9656 - error_in_tfi: 17.7193 - val_loss: 0.9467 - val_error_in_tfi: 17.6771\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0599 - error_in_tfi: 18.8848 - val_loss: 0.9411 - val_error_in_tfi: 17.6285\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.0274 - error_in_tfi: 18.7161 - val_loss: 0.9354 - val_error_in_tfi: 17.5765\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.0017 - error_in_tfi: 17.9926 - val_loss: 0.9271 - val_error_in_tfi: 17.5019\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9759 - error_in_tfi: 17.6649 - val_loss: 0.9195 - val_error_in_tfi: 17.4316\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9866 - error_in_tfi: 17.4817 - val_loss: 0.9136 - val_error_in_tfi: 17.3792\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.9608 - error_in_tfi: 17.1812 - val_loss: 0.9071 - val_error_in_tfi: 17.3177\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.9618 - error_in_tfi: 17.6473 - val_loss: 0.9014 - val_error_in_tfi: 17.2642\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8901 - error_in_tfi: 17.4580 - val_loss: 0.8954 - val_error_in_tfi: 17.2034\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0200 - error_in_tfi: 18.0972 - val_loss: 0.8908 - val_error_in_tfi: 17.1591\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9920 - error_in_tfi: 18.6332 - val_loss: 0.8856 - val_error_in_tfi: 17.1099\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9583 - error_in_tfi: 17.7353 - val_loss: 0.8803 - val_error_in_tfi: 17.0572\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.9961 - error_in_tfi: 17.6757 - val_loss: 0.8736 - val_error_in_tfi: 16.9920\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0581 - error_in_tfi: 18.7519 - val_loss: 0.8657 - val_error_in_tfi: 16.9187\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9686 - error_in_tfi: 18.0528 - val_loss: 0.8562 - val_error_in_tfi: 16.8277\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0082 - error_in_tfi: 18.5237 - val_loss: 0.8498 - val_error_in_tfi: 16.7710\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9405 - error_in_tfi: 17.1820 - val_loss: 0.8407 - val_error_in_tfi: 16.6877\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9836 - error_in_tfi: 17.9712 - val_loss: 0.8332 - val_error_in_tfi: 16.6145\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9054 - error_in_tfi: 16.6230 - val_loss: 0.8266 - val_error_in_tfi: 16.5485\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9433 - error_in_tfi: 17.8934 - val_loss: 0.8193 - val_error_in_tfi: 16.4754\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9344 - error_in_tfi: 17.1623 - val_loss: 0.8135 - val_error_in_tfi: 16.4153\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8955 - error_in_tfi: 16.7114 - val_loss: 0.8080 - val_error_in_tfi: 16.3562\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9188 - error_in_tfi: 17.4684 - val_loss: 0.8036 - val_error_in_tfi: 16.3101\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9144 - error_in_tfi: 17.5925 - val_loss: 0.7974 - val_error_in_tfi: 16.2466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9447 - error_in_tfi: 17.7816 - val_loss: 0.7930 - val_error_in_tfi: 16.2007\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9315 - error_in_tfi: 17.6503 - val_loss: 0.7889 - val_error_in_tfi: 16.1590\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8815 - error_in_tfi: 16.3854 - val_loss: 0.7836 - val_error_in_tfi: 16.1042\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9392 - error_in_tfi: 17.5523 - val_loss: 0.7790 - val_error_in_tfi: 16.0588\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9678 - error_in_tfi: 17.8305 - val_loss: 0.7733 - val_error_in_tfi: 16.0006\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9182 - error_in_tfi: 16.8191 - val_loss: 0.7675 - val_error_in_tfi: 15.9392\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8820 - error_in_tfi: 16.7463 - val_loss: 0.7618 - val_error_in_tfi: 15.8785\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9635 - error_in_tfi: 17.8980 - val_loss: 0.7574 - val_error_in_tfi: 15.8341\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9110 - error_in_tfi: 16.9583 - val_loss: 0.7534 - val_error_in_tfi: 15.7939\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9135 - error_in_tfi: 16.9025 - val_loss: 0.7494 - val_error_in_tfi: 15.7530\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8571 - error_in_tfi: 16.9084 - val_loss: 0.7415 - val_error_in_tfi: 15.6711\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8349 - error_in_tfi: 16.1297 - val_loss: 0.7358 - val_error_in_tfi: 15.6097\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8485 - error_in_tfi: 16.5487 - val_loss: 0.7298 - val_error_in_tfi: 15.5503\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8959 - error_in_tfi: 17.0335 - val_loss: 0.7252 - val_error_in_tfi: 15.5014\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9403 - error_in_tfi: 17.1782 - val_loss: 0.7202 - val_error_in_tfi: 15.4525\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8750 - error_in_tfi: 16.9686 - val_loss: 0.7133 - val_error_in_tfi: 15.3798\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8904 - error_in_tfi: 16.6095 - val_loss: 0.7073 - val_error_in_tfi: 15.3156\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.8915 - error_in_tfi: 16.4367 - val_loss: 0.7011 - val_error_in_tfi: 15.2478\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7988 - error_in_tfi: 15.7220 - val_loss: 0.6976 - val_error_in_tfi: 15.2091\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.8301 - error_in_tfi: 16.6457 - val_loss: 0.6920 - val_error_in_tfi: 15.1479\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8945 - error_in_tfi: 16.7388 - val_loss: 0.6869 - val_error_in_tfi: 15.0898\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.8764 - error_in_tfi: 16.4471 - val_loss: 0.6822 - val_error_in_tfi: 15.0424\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.8503 - error_in_tfi: 16.6031 - val_loss: 0.6773 - val_error_in_tfi: 14.9884\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.8638 - error_in_tfi: 15.9211 - val_loss: 0.6717 - val_error_in_tfi: 14.9283\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8279 - error_in_tfi: 16.4300 - val_loss: 0.6661 - val_error_in_tfi: 14.8664\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8299 - error_in_tfi: 15.7101 - val_loss: 0.6600 - val_error_in_tfi: 14.7987\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.7970 - error_in_tfi: 15.9195 - val_loss: 0.6545 - val_error_in_tfi: 14.7391\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.8394 - error_in_tfi: 16.7422 - val_loss: 0.6496 - val_error_in_tfi: 14.6871\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8402 - error_in_tfi: 16.1897 - val_loss: 0.6440 - val_error_in_tfi: 14.6307\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8424 - error_in_tfi: 16.5739 - val_loss: 0.6399 - val_error_in_tfi: 14.5886\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.8107 - error_in_tfi: 15.8716 - val_loss: 0.6343 - val_error_in_tfi: 14.5295\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.8818 - error_in_tfi: 17.3948 - val_loss: 0.6285 - val_error_in_tfi: 14.4646\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.8354 - error_in_tfi: 16.2746 - val_loss: 0.6251 - val_error_in_tfi: 14.4271\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.8482 - error_in_tfi: 16.4818 - val_loss: 0.6184 - val_error_in_tfi: 14.3520\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8485 - error_in_tfi: 16.2094 - val_loss: 0.6143 - val_error_in_tfi: 14.3064\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8679 - error_in_tfi: 16.5687 - val_loss: 0.6085 - val_error_in_tfi: 14.2393\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.7975 - error_in_tfi: 15.6769 - val_loss: 0.6046 - val_error_in_tfi: 14.1985\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.8538 - error_in_tfi: 16.4272 - val_loss: 0.6001 - val_error_in_tfi: 14.1481\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.8436 - error_in_tfi: 16.8099 - val_loss: 0.5946 - val_error_in_tfi: 14.0857\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.8056 - error_in_tfi: 16.0984 - val_loss: 0.5880 - val_error_in_tfi: 14.0128\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.8038 - error_in_tfi: 15.9197 - val_loss: 0.5825 - val_error_in_tfi: 13.9523\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.7777 - error_in_tfi: 15.0343 - val_loss: 0.5779 - val_error_in_tfi: 13.8999\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.8097 - error_in_tfi: 15.3821 - val_loss: 0.5730 - val_error_in_tfi: 13.8457\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.8878 - error_in_tfi: 16.5911 - val_loss: 0.5704 - val_error_in_tfi: 13.8189\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.8208 - error_in_tfi: 16.0371 - val_loss: 0.5644 - val_error_in_tfi: 13.7533\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.7622 - error_in_tfi: 15.5107 - val_loss: 0.5590 - val_error_in_tfi: 13.6937\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.7891 - error_in_tfi: 14.9970 - val_loss: 0.5536 - val_error_in_tfi: 13.6317\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.7781 - error_in_tfi: 15.2472 - val_loss: 0.5486 - val_error_in_tfi: 13.5751\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.8038 - error_in_tfi: 15.8580 - val_loss: 0.5448 - val_error_in_tfi: 13.5307\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.8422 - error_in_tfi: 15.7460 - val_loss: 0.5413 - val_error_in_tfi: 13.4888\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.7468 - error_in_tfi: 14.7588 - val_loss: 0.5371 - val_error_in_tfi: 13.4385\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.7456 - error_in_tfi: 15.1087 - val_loss: 0.5334 - val_error_in_tfi: 13.3952\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7349 - error_in_tfi: 14.3665 - val_loss: 0.5282 - val_error_in_tfi: 13.3340\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.7649 - error_in_tfi: 15.5940 - val_loss: 0.5245 - val_error_in_tfi: 13.2915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.8173 - error_in_tfi: 15.5525 - val_loss: 0.5215 - val_error_in_tfi: 13.2580\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7916 - error_in_tfi: 15.7792 - val_loss: 0.5167 - val_error_in_tfi: 13.1970\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7877 - error_in_tfi: 15.4876 - val_loss: 0.5130 - val_error_in_tfi: 13.1522\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.8481 - error_in_tfi: 15.0183 - val_loss: 0.5085 - val_error_in_tfi: 13.0957\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.8022 - error_in_tfi: 14.7706 - val_loss: 0.5043 - val_error_in_tfi: 13.0439\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.7773 - error_in_tfi: 14.7505 - val_loss: 0.5001 - val_error_in_tfi: 12.9927\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7418 - error_in_tfi: 14.8758 - val_loss: 0.4975 - val_error_in_tfi: 12.9619\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.7790 - error_in_tfi: 15.0176 - val_loss: 0.4928 - val_error_in_tfi: 12.9034\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.7146 - error_in_tfi: 14.6128 - val_loss: 0.4888 - val_error_in_tfi: 12.8561\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7644 - error_in_tfi: 15.3003 - val_loss: 0.4858 - val_error_in_tfi: 12.8202\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8023 - error_in_tfi: 15.9115 - val_loss: 0.4818 - val_error_in_tfi: 12.7671\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7847 - error_in_tfi: 15.4201 - val_loss: 0.4787 - val_error_in_tfi: 12.7262\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7623 - error_in_tfi: 14.9978 - val_loss: 0.4730 - val_error_in_tfi: 12.6521\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7480 - error_in_tfi: 15.0360 - val_loss: 0.4702 - val_error_in_tfi: 12.6160\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6900 - error_in_tfi: 14.1964 - val_loss: 0.4670 - val_error_in_tfi: 12.5752\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7864 - error_in_tfi: 14.8749 - val_loss: 0.4624 - val_error_in_tfi: 12.5181\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.7985 - error_in_tfi: 15.4317 - val_loss: 0.4577 - val_error_in_tfi: 12.4594\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6595 - error_in_tfi: 13.6582 - val_loss: 0.4528 - val_error_in_tfi: 12.3975\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7547 - error_in_tfi: 15.2468 - val_loss: 0.4489 - val_error_in_tfi: 12.3460\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7694 - error_in_tfi: 15.3487 - val_loss: 0.4450 - val_error_in_tfi: 12.2959\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7406 - error_in_tfi: 14.5798 - val_loss: 0.4402 - val_error_in_tfi: 12.2300\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7633 - error_in_tfi: 15.6837 - val_loss: 0.4368 - val_error_in_tfi: 12.1870\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7254 - error_in_tfi: 14.7065 - val_loss: 0.4321 - val_error_in_tfi: 12.1269\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7755 - error_in_tfi: 15.3176 - val_loss: 0.4289 - val_error_in_tfi: 12.0863\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7689 - error_in_tfi: 15.3006 - val_loss: 0.4253 - val_error_in_tfi: 12.0421\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.7412 - error_in_tfi: 14.3957 - val_loss: 0.4212 - val_error_in_tfi: 11.9878\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7047 - error_in_tfi: 14.1051 - val_loss: 0.4161 - val_error_in_tfi: 11.9188\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.7468 - error_in_tfi: 14.9781 - val_loss: 0.4123 - val_error_in_tfi: 11.8675\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7595 - error_in_tfi: 15.4522 - val_loss: 0.4077 - val_error_in_tfi: 11.8044\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7512 - error_in_tfi: 15.1159 - val_loss: 0.4028 - val_error_in_tfi: 11.7354\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7187 - error_in_tfi: 14.2006 - val_loss: 0.3991 - val_error_in_tfi: 11.6837\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6971 - error_in_tfi: 14.4969 - val_loss: 0.3946 - val_error_in_tfi: 11.6203\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6965 - error_in_tfi: 14.3281 - val_loss: 0.3927 - val_error_in_tfi: 11.5916\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7696 - error_in_tfi: 14.4670 - val_loss: 0.3885 - val_error_in_tfi: 11.5302\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7640 - error_in_tfi: 14.9925 - val_loss: 0.3852 - val_error_in_tfi: 11.4833\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7782 - error_in_tfi: 15.7533 - val_loss: 0.3825 - val_error_in_tfi: 11.4450\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7689 - error_in_tfi: 15.0573 - val_loss: 0.3793 - val_error_in_tfi: 11.4006\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7286 - error_in_tfi: 13.9192 - val_loss: 0.3776 - val_error_in_tfi: 11.3772\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7809 - error_in_tfi: 15.2012 - val_loss: 0.3747 - val_error_in_tfi: 11.3370\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7433 - error_in_tfi: 14.8195 - val_loss: 0.3715 - val_error_in_tfi: 11.2911\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8293 - error_in_tfi: 15.7442 - val_loss: 0.3684 - val_error_in_tfi: 11.2446\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7095 - error_in_tfi: 14.3707 - val_loss: 0.3656 - val_error_in_tfi: 11.2046\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7378 - error_in_tfi: 15.2582 - val_loss: 0.3618 - val_error_in_tfi: 11.1508\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6704 - error_in_tfi: 14.2387 - val_loss: 0.3572 - val_error_in_tfi: 11.0855\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7221 - error_in_tfi: 14.5341 - val_loss: 0.3544 - val_error_in_tfi: 11.0467\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7317 - error_in_tfi: 14.5854 - val_loss: 0.3496 - val_error_in_tfi: 10.9775\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7465 - error_in_tfi: 14.9225 - val_loss: 0.3453 - val_error_in_tfi: 10.9134\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6889 - error_in_tfi: 13.8926 - val_loss: 0.3415 - val_error_in_tfi: 10.8575\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6976 - error_in_tfi: 14.7128 - val_loss: 0.3381 - val_error_in_tfi: 10.8066\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6691 - error_in_tfi: 13.8119 - val_loss: 0.3337 - val_error_in_tfi: 10.7421\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7314 - error_in_tfi: 14.8684 - val_loss: 0.3305 - val_error_in_tfi: 10.6972\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7140 - error_in_tfi: 14.0611 - val_loss: 0.3266 - val_error_in_tfi: 10.6395\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7034 - error_in_tfi: 14.2945 - val_loss: 0.3225 - val_error_in_tfi: 10.5753\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7578 - error_in_tfi: 15.0376 - val_loss: 0.3188 - val_error_in_tfi: 10.5163\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.6881 - error_in_tfi: 13.6635 - val_loss: 0.3163 - val_error_in_tfi: 10.4771\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.7005 - error_in_tfi: 14.3115 - val_loss: 0.3141 - val_error_in_tfi: 10.4423\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6818 - error_in_tfi: 13.6473 - val_loss: 0.3117 - val_error_in_tfi: 10.4046\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6531 - error_in_tfi: 13.4036 - val_loss: 0.3094 - val_error_in_tfi: 10.3670\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.6527 - error_in_tfi: 13.0102 - val_loss: 0.3075 - val_error_in_tfi: 10.3375\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6993 - error_in_tfi: 14.3888 - val_loss: 0.3055 - val_error_in_tfi: 10.3050\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6880 - error_in_tfi: 14.4852 - val_loss: 0.3021 - val_error_in_tfi: 10.2489\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.7207 - error_in_tfi: 14.8848 - val_loss: 0.3003 - val_error_in_tfi: 10.2189\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6931 - error_in_tfi: 14.0187 - val_loss: 0.2973 - val_error_in_tfi: 10.1665\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.7128 - error_in_tfi: 14.1299 - val_loss: 0.2947 - val_error_in_tfi: 10.1227\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.7442 - error_in_tfi: 14.5493 - val_loss: 0.2913 - val_error_in_tfi: 10.0663\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6914 - error_in_tfi: 14.1014 - val_loss: 0.2885 - val_error_in_tfi: 10.0199\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.6938 - error_in_tfi: 14.7102 - val_loss: 0.2857 - val_error_in_tfi: 9.9718\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.7358 - error_in_tfi: 14.4044 - val_loss: 0.2843 - val_error_in_tfi: 9.9495\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6413 - error_in_tfi: 13.5810 - val_loss: 0.2814 - val_error_in_tfi: 9.9026\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6713 - error_in_tfi: 13.8305 - val_loss: 0.2785 - val_error_in_tfi: 9.8530\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.7378 - error_in_tfi: 14.9403 - val_loss: 0.2768 - val_error_in_tfi: 9.8231\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6975 - error_in_tfi: 14.3515 - val_loss: 0.2738 - val_error_in_tfi: 9.7723\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6923 - error_in_tfi: 13.7296 - val_loss: 0.2700 - val_error_in_tfi: 9.7047\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6231 - error_in_tfi: 13.3983 - val_loss: 0.2669 - val_error_in_tfi: 9.6518\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.7013 - error_in_tfi: 14.3155 - val_loss: 0.2641 - val_error_in_tfi: 9.6031\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.7176 - error_in_tfi: 14.1618 - val_loss: 0.2618 - val_error_in_tfi: 9.5604\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.6819 - error_in_tfi: 13.0241 - val_loss: 0.2594 - val_error_in_tfi: 9.5170\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.7169 - error_in_tfi: 14.3155 - val_loss: 0.2585 - val_error_in_tfi: 9.4987\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.7555 - error_in_tfi: 14.9225 - val_loss: 0.2578 - val_error_in_tfi: 9.4847\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6897 - error_in_tfi: 14.1618 - val_loss: 0.2559 - val_error_in_tfi: 9.4521\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.7201 - error_in_tfi: 14.4346 - val_loss: 0.2535 - val_error_in_tfi: 9.4098\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6852 - error_in_tfi: 13.8544 - val_loss: 0.2510 - val_error_in_tfi: 9.3681\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6357 - error_in_tfi: 13.0304 - val_loss: 0.2490 - val_error_in_tfi: 9.3354\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.6980 - error_in_tfi: 14.3840 - val_loss: 0.2467 - val_error_in_tfi: 9.2949\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6893 - error_in_tfi: 13.9008 - val_loss: 0.2446 - val_error_in_tfi: 9.2563\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.6717 - error_in_tfi: 13.9189 - val_loss: 0.2435 - val_error_in_tfi: 9.2360\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.6574 - error_in_tfi: 13.4747 - val_loss: 0.2424 - val_error_in_tfi: 9.2152\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.6887 - error_in_tfi: 14.3506 - val_loss: 0.2395 - val_error_in_tfi: 9.1591\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.6783 - error_in_tfi: 14.2972 - val_loss: 0.2377 - val_error_in_tfi: 9.1238\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.6322 - error_in_tfi: 13.3693 - val_loss: 0.2347 - val_error_in_tfi: 9.0655\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6507 - error_in_tfi: 13.8441 - val_loss: 0.2333 - val_error_in_tfi: 9.0384\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6456 - error_in_tfi: 13.5049 - val_loss: 0.2308 - val_error_in_tfi: 8.9868\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4663 - error_in_tfi: 11.5119\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 18 19 20 21 22 23 24 25 26 27 28 29] TEST: [12 13 14 15 16 17]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 4, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 4, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 4, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 4, 1)         5           ['dropout_1[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 4, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 4, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 4, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 4)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          640         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,513\n",
      "Trainable params: 29,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 1.3034 - error_in_tfi: 19.8178 - val_loss: 0.5111 - val_error_in_tfi: 13.7262\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.1040 - error_in_tfi: 18.5634 - val_loss: 0.5139 - val_error_in_tfi: 13.7695\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.1803 - error_in_tfi: 19.1018 - val_loss: 0.5184 - val_error_in_tfi: 13.8296\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.1127 - error_in_tfi: 18.0371 - val_loss: 0.5239 - val_error_in_tfi: 13.9034\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.2474 - error_in_tfi: 20.0200 - val_loss: 0.5295 - val_error_in_tfi: 13.9799\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.2197 - error_in_tfi: 19.3706 - val_loss: 0.5351 - val_error_in_tfi: 14.0547\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.2212 - error_in_tfi: 19.1922 - val_loss: 0.5404 - val_error_in_tfi: 14.1274\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 1.1809 - error_in_tfi: 19.7059 - val_loss: 0.5457 - val_error_in_tfi: 14.1985\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.1011 - error_in_tfi: 18.5083 - val_loss: 0.5495 - val_error_in_tfi: 14.2538\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 1.0563 - error_in_tfi: 18.7645 - val_loss: 0.5540 - val_error_in_tfi: 14.3153\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.2736 - error_in_tfi: 20.1169 - val_loss: 0.5571 - val_error_in_tfi: 14.3604\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5937 - error_in_tfi: 14.0875\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 24 25 26 27 28 29] TEST: [18 19 20 21 22 23]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 4, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 4, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 4, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 4, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 4, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.__operators__.add_4 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 4, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 4, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 4)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          640         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,513\n",
      "Trainable params: 29,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.8642 - error_in_tfi: 16.9417 - val_loss: 0.9001 - val_error_in_tfi: 17.6976\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8120 - error_in_tfi: 16.2135 - val_loss: 0.8848 - val_error_in_tfi: 17.5364\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7936 - error_in_tfi: 15.9056 - val_loss: 0.8674 - val_error_in_tfi: 17.3578\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7520 - error_in_tfi: 15.8707 - val_loss: 0.8508 - val_error_in_tfi: 17.1845\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8225 - error_in_tfi: 15.8245 - val_loss: 0.8344 - val_error_in_tfi: 17.0081\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7909 - error_in_tfi: 15.4967 - val_loss: 0.8192 - val_error_in_tfi: 16.8449\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 150ms/step - loss: 0.6815 - error_in_tfi: 15.1934 - val_loss: 0.8046 - val_error_in_tfi: 16.6825\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.8074 - error_in_tfi: 16.5211 - val_loss: 0.7899 - val_error_in_tfi: 16.5202\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.7352 - error_in_tfi: 15.8004 - val_loss: 0.7742 - val_error_in_tfi: 16.3421\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.7523 - error_in_tfi: 15.8213 - val_loss: 0.7595 - val_error_in_tfi: 16.1728\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7155 - error_in_tfi: 15.0081 - val_loss: 0.7437 - val_error_in_tfi: 15.9878\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7231 - error_in_tfi: 14.9839 - val_loss: 0.7303 - val_error_in_tfi: 15.8326\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7544 - error_in_tfi: 15.6873 - val_loss: 0.7165 - val_error_in_tfi: 15.6737\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8254 - error_in_tfi: 17.1292 - val_loss: 0.7041 - val_error_in_tfi: 15.5355\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7051 - error_in_tfi: 15.4035 - val_loss: 0.6898 - val_error_in_tfi: 15.3760\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7457 - error_in_tfi: 15.7674 - val_loss: 0.6766 - val_error_in_tfi: 15.2182\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.7351 - error_in_tfi: 15.5434 - val_loss: 0.6645 - val_error_in_tfi: 15.0726\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7226 - error_in_tfi: 15.3504 - val_loss: 0.6538 - val_error_in_tfi: 14.9358\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6359 - error_in_tfi: 13.5562 - val_loss: 0.6427 - val_error_in_tfi: 14.7966\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.6735 - error_in_tfi: 14.6712 - val_loss: 0.6327 - val_error_in_tfi: 14.6680\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6980 - error_in_tfi: 14.4435 - val_loss: 0.6236 - val_error_in_tfi: 14.5463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6388 - error_in_tfi: 14.9114 - val_loss: 0.6143 - val_error_in_tfi: 14.4256\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5798 - error_in_tfi: 13.6147 - val_loss: 0.6044 - val_error_in_tfi: 14.2933\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6220 - error_in_tfi: 14.1424 - val_loss: 0.5951 - val_error_in_tfi: 14.1693\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6433 - error_in_tfi: 14.3848 - val_loss: 0.5874 - val_error_in_tfi: 14.0578\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6623 - error_in_tfi: 14.5147 - val_loss: 0.5793 - val_error_in_tfi: 13.9433\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6658 - error_in_tfi: 14.2131 - val_loss: 0.5697 - val_error_in_tfi: 13.8092\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5815 - error_in_tfi: 13.8597 - val_loss: 0.5619 - val_error_in_tfi: 13.7002\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6354 - error_in_tfi: 14.4968 - val_loss: 0.5529 - val_error_in_tfi: 13.5722\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6288 - error_in_tfi: 14.1771 - val_loss: 0.5442 - val_error_in_tfi: 13.4453\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5933 - error_in_tfi: 13.1652 - val_loss: 0.5360 - val_error_in_tfi: 13.3232\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5749 - error_in_tfi: 13.6634 - val_loss: 0.5288 - val_error_in_tfi: 13.2174\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5585 - error_in_tfi: 13.2615 - val_loss: 0.5198 - val_error_in_tfi: 13.0933\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5731 - error_in_tfi: 14.0993 - val_loss: 0.5108 - val_error_in_tfi: 12.9580\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6538 - error_in_tfi: 14.2957 - val_loss: 0.5025 - val_error_in_tfi: 12.8310\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.6142 - error_in_tfi: 13.8738 - val_loss: 0.4950 - val_error_in_tfi: 12.7167\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6660 - error_in_tfi: 14.4005 - val_loss: 0.4888 - val_error_in_tfi: 12.6281\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.6397 - error_in_tfi: 13.6572 - val_loss: 0.4814 - val_error_in_tfi: 12.5174\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.6028 - error_in_tfi: 13.5081 - val_loss: 0.4730 - val_error_in_tfi: 12.3856\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.4985 - error_in_tfi: 12.5647 - val_loss: 0.4662 - val_error_in_tfi: 12.2842\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5464 - error_in_tfi: 12.9969 - val_loss: 0.4604 - val_error_in_tfi: 12.2007\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.5411 - error_in_tfi: 12.9272 - val_loss: 0.4535 - val_error_in_tfi: 12.0954\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5523 - error_in_tfi: 13.6403 - val_loss: 0.4477 - val_error_in_tfi: 12.0070\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4916 - error_in_tfi: 12.0727 - val_loss: 0.4416 - val_error_in_tfi: 11.9214\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5829 - error_in_tfi: 13.7410 - val_loss: 0.4339 - val_error_in_tfi: 11.8067\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5944 - error_in_tfi: 13.5539 - val_loss: 0.4271 - val_error_in_tfi: 11.7045\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5105 - error_in_tfi: 12.6497 - val_loss: 0.4206 - val_error_in_tfi: 11.5975\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.5765 - error_in_tfi: 13.5202 - val_loss: 0.4137 - val_error_in_tfi: 11.4923\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.5404 - error_in_tfi: 12.4773 - val_loss: 0.4069 - val_error_in_tfi: 11.3805\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5764 - error_in_tfi: 13.3466 - val_loss: 0.4017 - val_error_in_tfi: 11.3001\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.5380 - error_in_tfi: 12.7753 - val_loss: 0.3966 - val_error_in_tfi: 11.2222\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.5196 - error_in_tfi: 12.5212 - val_loss: 0.3903 - val_error_in_tfi: 11.1180\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.4760 - error_in_tfi: 12.0020 - val_loss: 0.3845 - val_error_in_tfi: 11.0220\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4897 - error_in_tfi: 11.2107 - val_loss: 0.3782 - val_error_in_tfi: 10.9095\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5328 - error_in_tfi: 11.9752 - val_loss: 0.3734 - val_error_in_tfi: 10.8231\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4942 - error_in_tfi: 12.4745 - val_loss: 0.3693 - val_error_in_tfi: 10.7533\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4989 - error_in_tfi: 12.1443 - val_loss: 0.3642 - val_error_in_tfi: 10.6595\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6282 - error_in_tfi: 13.7804 - val_loss: 0.3602 - val_error_in_tfi: 10.5846\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4646 - error_in_tfi: 11.9294 - val_loss: 0.3553 - val_error_in_tfi: 10.5001\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.5694 - error_in_tfi: 13.4020 - val_loss: 0.3511 - val_error_in_tfi: 10.4349\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.5178 - error_in_tfi: 12.0166 - val_loss: 0.3456 - val_error_in_tfi: 10.3472\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.5790 - error_in_tfi: 13.1434 - val_loss: 0.3409 - val_error_in_tfi: 10.2697\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.4472 - error_in_tfi: 11.6184 - val_loss: 0.3362 - val_error_in_tfi: 10.1964\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.5047 - error_in_tfi: 12.2514 - val_loss: 0.3325 - val_error_in_tfi: 10.1401\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5086 - error_in_tfi: 12.2866 - val_loss: 0.3281 - val_error_in_tfi: 10.0856\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4774 - error_in_tfi: 12.1134 - val_loss: 0.3226 - val_error_in_tfi: 9.9983\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4112 - error_in_tfi: 10.7870 - val_loss: 0.3187 - val_error_in_tfi: 9.9345\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.5106 - error_in_tfi: 12.3972 - val_loss: 0.3155 - val_error_in_tfi: 9.8907\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5744 - error_in_tfi: 12.8969 - val_loss: 0.3121 - val_error_in_tfi: 9.8425\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5301 - error_in_tfi: 13.5181 - val_loss: 0.3086 - val_error_in_tfi: 9.7863\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5612 - error_in_tfi: 12.7374 - val_loss: 0.3052 - val_error_in_tfi: 9.7287\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.4564 - error_in_tfi: 12.1481 - val_loss: 0.3024 - val_error_in_tfi: 9.6758\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.5805 - error_in_tfi: 13.4147 - val_loss: 0.2993 - val_error_in_tfi: 9.6215\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4374 - error_in_tfi: 11.4497 - val_loss: 0.2964 - val_error_in_tfi: 9.5700\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.5137 - error_in_tfi: 12.4169 - val_loss: 0.2938 - val_error_in_tfi: 9.5284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.5861 - error_in_tfi: 13.3610 - val_loss: 0.2912 - val_error_in_tfi: 9.4808\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.4859 - error_in_tfi: 11.7894 - val_loss: 0.2885 - val_error_in_tfi: 9.4360\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4737 - error_in_tfi: 12.1039 - val_loss: 0.2856 - val_error_in_tfi: 9.3950\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5361 - error_in_tfi: 12.9285 - val_loss: 0.2836 - val_error_in_tfi: 9.3709\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4674 - error_in_tfi: 11.6721 - val_loss: 0.2799 - val_error_in_tfi: 9.3130\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4816 - error_in_tfi: 11.3975 - val_loss: 0.2766 - val_error_in_tfi: 9.2559\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.4717 - error_in_tfi: 11.2720 - val_loss: 0.2738 - val_error_in_tfi: 9.2100\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4982 - error_in_tfi: 12.0739 - val_loss: 0.2702 - val_error_in_tfi: 9.1437\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5308 - error_in_tfi: 12.3197 - val_loss: 0.2672 - val_error_in_tfi: 9.0936\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4792 - error_in_tfi: 12.2425 - val_loss: 0.2651 - val_error_in_tfi: 9.0688\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4686 - error_in_tfi: 12.0635 - val_loss: 0.2638 - val_error_in_tfi: 9.0510\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4971 - error_in_tfi: 11.7520 - val_loss: 0.2618 - val_error_in_tfi: 9.0155\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.3977 - error_in_tfi: 10.9873 - val_loss: 0.2593 - val_error_in_tfi: 8.9665\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.4882 - error_in_tfi: 11.4416 - val_loss: 0.2565 - val_error_in_tfi: 8.9126\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5243 - error_in_tfi: 12.8358 - val_loss: 0.2535 - val_error_in_tfi: 8.8556\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.4923 - error_in_tfi: 12.2170 - val_loss: 0.2522 - val_error_in_tfi: 8.8453\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.4797 - error_in_tfi: 11.7440 - val_loss: 0.2494 - val_error_in_tfi: 8.7949\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.5123 - error_in_tfi: 12.3611 - val_loss: 0.2468 - val_error_in_tfi: 8.7507\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5084 - error_in_tfi: 12.9022 - val_loss: 0.2432 - val_error_in_tfi: 8.6834\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3943 - error_in_tfi: 10.4582 - val_loss: 0.2391 - val_error_in_tfi: 8.5997\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.4866 - error_in_tfi: 12.2354 - val_loss: 0.2363 - val_error_in_tfi: 8.5421\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.4973 - error_in_tfi: 11.9164 - val_loss: 0.2345 - val_error_in_tfi: 8.5224\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4624 - error_in_tfi: 11.4092 - val_loss: 0.2312 - val_error_in_tfi: 8.4558\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4445 - error_in_tfi: 11.5354 - val_loss: 0.2283 - val_error_in_tfi: 8.3991\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4360 - error_in_tfi: 11.5128 - val_loss: 0.2268 - val_error_in_tfi: 8.3724\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.4980 - error_in_tfi: 12.5751 - val_loss: 0.2246 - val_error_in_tfi: 8.3344\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.4663 - error_in_tfi: 10.9569 - val_loss: 0.2213 - val_error_in_tfi: 8.2690\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.4438 - error_in_tfi: 11.8457 - val_loss: 0.2193 - val_error_in_tfi: 8.2364\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.3499 - error_in_tfi: 10.3877 - val_loss: 0.2171 - val_error_in_tfi: 8.1865\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.3877 - error_in_tfi: 10.3594 - val_loss: 0.2143 - val_error_in_tfi: 8.1281\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4374 - error_in_tfi: 11.1156 - val_loss: 0.2121 - val_error_in_tfi: 8.0896\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.5324 - error_in_tfi: 12.7926 - val_loss: 0.2103 - val_error_in_tfi: 8.0687\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4733 - error_in_tfi: 12.0450 - val_loss: 0.2081 - val_error_in_tfi: 8.0411\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4528 - error_in_tfi: 11.5621 - val_loss: 0.2057 - val_error_in_tfi: 8.0026\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4124 - error_in_tfi: 10.3958 - val_loss: 0.2042 - val_error_in_tfi: 7.9867\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3939 - error_in_tfi: 10.1671 - val_loss: 0.2020 - val_error_in_tfi: 7.9517\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.4923 - error_in_tfi: 12.4721 - val_loss: 0.2008 - val_error_in_tfi: 7.9398\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.4305 - error_in_tfi: 11.2719 - val_loss: 0.1991 - val_error_in_tfi: 7.9163\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4728 - error_in_tfi: 11.6123 - val_loss: 0.1971 - val_error_in_tfi: 7.8828\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5214 - error_in_tfi: 12.9557 - val_loss: 0.1949 - val_error_in_tfi: 7.8331\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.4135 - error_in_tfi: 10.5116 - val_loss: 0.1935 - val_error_in_tfi: 7.8178\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4124 - error_in_tfi: 11.5581 - val_loss: 0.1918 - val_error_in_tfi: 7.8010\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3310 - error_in_tfi: 10.2881 - val_loss: 0.1898 - val_error_in_tfi: 7.7618\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5002 - error_in_tfi: 11.7071 - val_loss: 0.1885 - val_error_in_tfi: 7.7508\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3778 - error_in_tfi: 11.2824 - val_loss: 0.1870 - val_error_in_tfi: 7.7295\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.4434 - error_in_tfi: 11.5672 - val_loss: 0.1859 - val_error_in_tfi: 7.7174\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4279 - error_in_tfi: 11.7387 - val_loss: 0.1842 - val_error_in_tfi: 7.6903\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3859 - error_in_tfi: 10.9549 - val_loss: 0.1816 - val_error_in_tfi: 7.6427\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4710 - error_in_tfi: 11.3173 - val_loss: 0.1793 - val_error_in_tfi: 7.6028\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3654 - error_in_tfi: 10.8309 - val_loss: 0.1773 - val_error_in_tfi: 7.5663\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.3954 - error_in_tfi: 10.7763 - val_loss: 0.1760 - val_error_in_tfi: 7.5476\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.4439 - error_in_tfi: 11.2364 - val_loss: 0.1744 - val_error_in_tfi: 7.5159\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.4378 - error_in_tfi: 11.7645 - val_loss: 0.1727 - val_error_in_tfi: 7.4813\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.4563 - error_in_tfi: 11.2672 - val_loss: 0.1719 - val_error_in_tfi: 7.4782\n",
      "Epoch 130/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 139ms/step - loss: 0.5416 - error_in_tfi: 12.8699 - val_loss: 0.1705 - val_error_in_tfi: 7.4535\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3869 - error_in_tfi: 10.8918 - val_loss: 0.1702 - val_error_in_tfi: 7.4675\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3693 - error_in_tfi: 10.6831 - val_loss: 0.1693 - val_error_in_tfi: 7.4516\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4483 - error_in_tfi: 11.5842 - val_loss: 0.1689 - val_error_in_tfi: 7.4507\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4694 - error_in_tfi: 11.6155 - val_loss: 0.1674 - val_error_in_tfi: 7.4217\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3807 - error_in_tfi: 11.1508 - val_loss: 0.1669 - val_error_in_tfi: 7.4164\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4590 - error_in_tfi: 11.7042 - val_loss: 0.1662 - val_error_in_tfi: 7.4139\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.5166 - error_in_tfi: 12.2642 - val_loss: 0.1654 - val_error_in_tfi: 7.4037\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3930 - error_in_tfi: 11.2901 - val_loss: 0.1650 - val_error_in_tfi: 7.4014\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4424 - error_in_tfi: 11.8527 - val_loss: 0.1641 - val_error_in_tfi: 7.3829\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4228 - error_in_tfi: 10.7593 - val_loss: 0.1638 - val_error_in_tfi: 7.3910\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.5006 - error_in_tfi: 11.5156 - val_loss: 0.1639 - val_error_in_tfi: 7.4089\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.2968 - error_in_tfi: 9.6880 - val_loss: 0.1634 - val_error_in_tfi: 7.4018\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4502 - error_in_tfi: 11.2125 - val_loss: 0.1622 - val_error_in_tfi: 7.3847\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3909 - error_in_tfi: 10.8424 - val_loss: 0.1607 - val_error_in_tfi: 7.3640\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3368 - error_in_tfi: 10.1324 - val_loss: 0.1598 - val_error_in_tfi: 7.3571\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3861 - error_in_tfi: 10.1987 - val_loss: 0.1587 - val_error_in_tfi: 7.3473\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3598 - error_in_tfi: 9.9023 - val_loss: 0.1585 - val_error_in_tfi: 7.3698\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4271 - error_in_tfi: 11.4841 - val_loss: 0.1576 - val_error_in_tfi: 7.3733\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3561 - error_in_tfi: 10.3681 - val_loss: 0.1555 - val_error_in_tfi: 7.3307\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4155 - error_in_tfi: 10.8004 - val_loss: 0.1538 - val_error_in_tfi: 7.3002\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3316 - error_in_tfi: 9.4930 - val_loss: 0.1529 - val_error_in_tfi: 7.3032\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3658 - error_in_tfi: 9.9273 - val_loss: 0.1526 - val_error_in_tfi: 7.3237\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3892 - error_in_tfi: 10.9143 - val_loss: 0.1527 - val_error_in_tfi: 7.3499\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4168 - error_in_tfi: 10.9693 - val_loss: 0.1526 - val_error_in_tfi: 7.3632\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.4188 - error_in_tfi: 10.7955 - val_loss: 0.1520 - val_error_in_tfi: 7.3600\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3783 - error_in_tfi: 10.1279 - val_loss: 0.1508 - val_error_in_tfi: 7.3384\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.3499 - error_in_tfi: 9.6627 - val_loss: 0.1495 - val_error_in_tfi: 7.3178\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4105 - error_in_tfi: 10.3481 - val_loss: 0.1484 - val_error_in_tfi: 7.3047\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.3944 - error_in_tfi: 10.4676 - val_loss: 0.1472 - val_error_in_tfi: 7.2942\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4028 - error_in_tfi: 10.3585 - val_loss: 0.1468 - val_error_in_tfi: 7.3102\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.4359 - error_in_tfi: 11.2695 - val_loss: 0.1464 - val_error_in_tfi: 7.3211\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3753 - error_in_tfi: 10.0449 - val_loss: 0.1446 - val_error_in_tfi: 7.2919\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4978 - error_in_tfi: 12.2599 - val_loss: 0.1431 - val_error_in_tfi: 7.2723\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3984 - error_in_tfi: 10.2843 - val_loss: 0.1411 - val_error_in_tfi: 7.2345\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3295 - error_in_tfi: 9.7997 - val_loss: 0.1403 - val_error_in_tfi: 7.2282\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3629 - error_in_tfi: 9.9868 - val_loss: 0.1398 - val_error_in_tfi: 7.2258\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4295 - error_in_tfi: 10.4962 - val_loss: 0.1395 - val_error_in_tfi: 7.2301\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.4020 - error_in_tfi: 10.8478 - val_loss: 0.1400 - val_error_in_tfi: 7.2571\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3011 - error_in_tfi: 9.2954 - val_loss: 0.1395 - val_error_in_tfi: 7.2603\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.4238 - error_in_tfi: 11.4632 - val_loss: 0.1382 - val_error_in_tfi: 7.2375\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.4069 - error_in_tfi: 10.2079 - val_loss: 0.1368 - val_error_in_tfi: 7.2127\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.4151 - error_in_tfi: 11.0425 - val_loss: 0.1363 - val_error_in_tfi: 7.2169\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.3557 - error_in_tfi: 10.1995 - val_loss: 0.1354 - val_error_in_tfi: 7.2129\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.4148 - error_in_tfi: 11.6965 - val_loss: 0.1341 - val_error_in_tfi: 7.1968\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.3512 - error_in_tfi: 9.4522 - val_loss: 0.1321 - val_error_in_tfi: 7.1497\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3646 - error_in_tfi: 9.9151 - val_loss: 0.1307 - val_error_in_tfi: 7.1139\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.4539 - error_in_tfi: 11.5866 - val_loss: 0.1303 - val_error_in_tfi: 7.1108\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3465 - error_in_tfi: 10.0358 - val_loss: 0.1295 - val_error_in_tfi: 7.1125\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.4022 - error_in_tfi: 10.8634 - val_loss: 0.1280 - val_error_in_tfi: 7.0774\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.2946 - error_in_tfi: 9.6417 - val_loss: 0.1269 - val_error_in_tfi: 7.0596\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.3819 - error_in_tfi: 11.1265 - val_loss: 0.1257 - val_error_in_tfi: 7.0370\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4076 - error_in_tfi: 10.2859 - val_loss: 0.1248 - val_error_in_tfi: 7.0142\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3282 - error_in_tfi: 9.3119 - val_loss: 0.1239 - val_error_in_tfi: 6.9934\n",
      "Epoch 184/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3472 - error_in_tfi: 10.0737 - val_loss: 0.1242 - val_error_in_tfi: 7.0056\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3600 - error_in_tfi: 10.7154 - val_loss: 0.1242 - val_error_in_tfi: 7.0162\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.3391 - error_in_tfi: 10.4149 - val_loss: 0.1239 - val_error_in_tfi: 7.0171\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.3855 - error_in_tfi: 10.8591 - val_loss: 0.1240 - val_error_in_tfi: 7.0349\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.3281 - error_in_tfi: 9.7249 - val_loss: 0.1237 - val_error_in_tfi: 7.0407\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.3699 - error_in_tfi: 10.4906 - val_loss: 0.1225 - val_error_in_tfi: 7.0201\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.3098 - error_in_tfi: 9.7780 - val_loss: 0.1211 - val_error_in_tfi: 6.9857\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.3817 - error_in_tfi: 11.2301 - val_loss: 0.1202 - val_error_in_tfi: 6.9802\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3043 - error_in_tfi: 8.1842 - val_loss: 0.1185 - val_error_in_tfi: 6.9411\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3573 - error_in_tfi: 10.2774 - val_loss: 0.1177 - val_error_in_tfi: 6.9268\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3660 - error_in_tfi: 10.4000 - val_loss: 0.1166 - val_error_in_tfi: 6.9020\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3885 - error_in_tfi: 10.4434 - val_loss: 0.1158 - val_error_in_tfi: 6.8923\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.2947 - error_in_tfi: 9.3027 - val_loss: 0.1149 - val_error_in_tfi: 6.8801\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3528 - error_in_tfi: 10.0983 - val_loss: 0.1146 - val_error_in_tfi: 6.8806\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.2876 - error_in_tfi: 8.7437 - val_loss: 0.1143 - val_error_in_tfi: 6.8850\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.2998 - error_in_tfi: 9.5252 - val_loss: 0.1140 - val_error_in_tfi: 6.8912\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.3740 - error_in_tfi: 10.2642 - val_loss: 0.1143 - val_error_in_tfi: 6.9202\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 2.3461 - error_in_tfi: 26.3240\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] TEST: [24 25 26 27 28 29]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 1)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 4, 1)        7169        ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 1)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4, 1)        2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 4, 1)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 4, 4)         8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4, 4)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 4, 1)         5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4, 1)        2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4, 1)        2           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4, 4)         0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 4, 1)         5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4, 1)        2           ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_2[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4, 1)        2           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4, 4)         0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 4, 1)         5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4, 1)        2           ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 4, 1)        7169        ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4, 1)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4, 1)        2           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 4, 4)         8           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4, 4)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 4, 1)         5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4, 1)        2           ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 4, 1)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 4)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          640         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,513\n",
      "Trainable params: 29,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 0.9315 - error_in_tfi: 17.5510 - val_loss: 1.1837 - val_error_in_tfi: 17.8956\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.9127 - error_in_tfi: 17.1828 - val_loss: 1.1919 - val_error_in_tfi: 17.9275\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 1.0081 - error_in_tfi: 18.4768 - val_loss: 1.2041 - val_error_in_tfi: 17.9875\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.8441 - error_in_tfi: 16.2359 - val_loss: 1.2160 - val_error_in_tfi: 18.0464\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.8774 - error_in_tfi: 17.2658 - val_loss: 1.2272 - val_error_in_tfi: 18.1022\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.9066 - error_in_tfi: 17.0292 - val_loss: 1.2373 - val_error_in_tfi: 18.1536\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.9679 - error_in_tfi: 17.8991 - val_loss: 1.2524 - val_error_in_tfi: 18.2491\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.7977 - error_in_tfi: 16.3275 - val_loss: 1.2640 - val_error_in_tfi: 18.3162\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.8245 - error_in_tfi: 16.5159 - val_loss: 1.2735 - val_error_in_tfi: 18.3657\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.8759 - error_in_tfi: 17.4388 - val_loss: 1.2820 - val_error_in_tfi: 18.3992\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7182 - error_in_tfi: 15.7854 - val_loss: 1.2931 - val_error_in_tfi: 18.4411\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.3270 - error_in_tfi: 18.5834\n"
     ]
    }
   ],
   "source": [
    "error_scores_US=[]\n",
    "error_scores_WN=[]\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "def cross_val(df,error_scores):\n",
    "    X,y,x_scaler,y_scaler=data_prep(df)\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "        model,history=prediction(x_train,y_train,y_scaler)\n",
    "        val_mse,val_mae=model.evaluate(x_test,y_test) #evaluating using unseen data\n",
    "        error_scores.append(val_mae)\n",
    "    return history,x_scaler,y_scaler,error_scores\n",
    "\n",
    "US_history,US_x_scaler,US_y_scaler,US_error_scores=cross_val(df_stable_US,error_scores_US)\n",
    "WN_history,WN_x_scaler,WN_y_scaler,WN_error_scores=cross_val(df_stable_WN,error_scores_WN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe3266ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US_error\n",
      "--------\n",
      "[16.647953033447266, 16.048969268798828, 23.443729400634766, 12.138420104980469, 14.426615715026855]\n",
      "\n",
      "\n",
      "16.541137504577637\n",
      "\n",
      "\n",
      "WN_error\n",
      "--------\n",
      "[20.61210823059082, 11.511922836303711, 14.087471008300781, 26.32403564453125, 18.58338165283203]\n",
      "\n",
      "\n",
      "18.223783874511717\n"
     ]
    }
   ],
   "source": [
    "print(\"US_error\")\n",
    "print(\"--------\")\n",
    "print(US_error_scores)\n",
    "US_error=np.mean(US_error_scores)\n",
    "print(\"\\n\")\n",
    "print(US_error)\n",
    "print(\"\\n\")\n",
    "print(\"WN_error\")\n",
    "print(\"--------\")\n",
    "print(WN_error_scores)\n",
    "WN_error=np.mean(WN_error_scores)\n",
    "print(\"\\n\")\n",
    "print(WN_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969db9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tinnitus)",
   "language": "python",
   "name": "tinnitus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
