{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "189c37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Attention #from attention import Attention\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import kaleido ##pip install -U kaleido ##to save a plotly fig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "\n",
    "\n",
    "##https://keras.io/examples/timeseries/timeseries_transformer_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f61176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.8.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbce8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "ddf1 = pd.read_csv('Stable_Data_CSV.csv')\n",
    "df2 = pd.read_csv('12w_features.csv')\n",
    "df_stable = df2[df2.set_index(['participant_id']).index.isin(df1.set_index(['participant_id']).index)]\n",
    "df_stable_US = df_stable.loc[(df_stable['arm'] == 1)]\n",
    "df_stable_WN = df_stable.loc[(df_stable['arm'] == 2)]\n",
    "print(len(df_stable_US))\n",
    "print(len(df_stable_WN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82d2617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stable_US=(df_stable_US[['c_3','sl_1','a_2','q_1','tfi_total','c_3.1','sl_1.1','a_2.1','q_1.1','tfi_total.1','c_3.2','sl_1.2','a_2.2','q_1.2','tfi_total.2','tfi_total.3']])\n",
    "df_stable_WN=(df_stable_WN[['sl_2','r_1','e_1','tfi_total','sl_2.1','r_1.1','e_1.1','tfi_total.1','sl_2.2','r_1.2','e_1.2','tfi_total.2','tfi_total.3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a054c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl_2</th>\n",
       "      <th>r_1</th>\n",
       "      <th>e_1</th>\n",
       "      <th>tfi_total</th>\n",
       "      <th>sl_2.1</th>\n",
       "      <th>r_1.1</th>\n",
       "      <th>e_1.1</th>\n",
       "      <th>tfi_total.1</th>\n",
       "      <th>sl_2.2</th>\n",
       "      <th>r_1.2</th>\n",
       "      <th>e_1.2</th>\n",
       "      <th>tfi_total.2</th>\n",
       "      <th>tfi_total.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>72.4</td>\n",
       "      <td>80.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>47.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>71.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>57.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.4</td>\n",
       "      <td>64.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.6</td>\n",
       "      <td>44.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>52.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>51.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>55.2</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>55.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>55.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>55.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>37.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>74.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73.6</td>\n",
       "      <td>83.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>55.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.2</td>\n",
       "      <td>42.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>35.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>56.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.8</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>61.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>35.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>69.2</td>\n",
       "      <td>67.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>77.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>79.2</td>\n",
       "      <td>79.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.2</td>\n",
       "      <td>51.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>58.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.4</td>\n",
       "      <td>63.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>54.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>57.6</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>43.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>42.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>79.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>60.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>71.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>33.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>65.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>60.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46.8</td>\n",
       "      <td>29.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>88.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>72.8</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>69.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>55.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>75.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70.8</td>\n",
       "      <td>61.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>48.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>43.6</td>\n",
       "      <td>50.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>69.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>69.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>65.6</td>\n",
       "      <td>52.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sl_2   r_1   e_1  tfi_total  sl_2.1  r_1.1  e_1.1  tfi_total.1  sl_2.2  \\\n",
       "51   7.0   8.0   8.0       72.4     5.0    8.0    6.0         64.0     8.0   \n",
       "52   6.0   7.0   5.0       60.0     3.0    5.0    4.0         47.6     6.0   \n",
       "53   8.0   9.0   5.0       68.0     6.0    6.0    3.0         57.2     6.0   \n",
       "55   6.0   6.0   4.0       41.2     3.0    7.0    2.0         37.2     2.0   \n",
       "56   1.0   4.0   6.0       52.8     3.0    3.0    4.0         51.2     5.0   \n",
       "57   1.0   5.0   5.0       55.2     1.0    5.0    6.0         55.2     0.0   \n",
       "59   6.0   8.0   9.0       50.8     5.0    8.0    6.0         55.2     2.0   \n",
       "60  10.0   7.0   2.0       74.4     7.0    7.0    3.0         63.2     6.0   \n",
       "61   3.0   7.0   6.0       55.6     3.0    5.0    5.0         47.2     0.0   \n",
       "63   1.0   2.0   2.0       53.6     1.0    7.0    2.0         51.6     2.0   \n",
       "64   9.0   8.0   7.0       56.8     8.0    7.0    3.0         52.0     6.0   \n",
       "67   9.0   8.0   8.0       74.0     9.0    8.0    8.0         74.0     7.0   \n",
       "69   5.0   8.0   2.0       42.0     4.0    4.0    2.0         32.0     3.0   \n",
       "70  10.0   9.0   8.0       78.0     9.0    6.0    4.0         70.0     8.0   \n",
       "71   6.0   8.0   8.0       78.4     5.0    8.0    8.0         77.6     6.0   \n",
       "72   6.0   8.0   1.0       51.6     8.0    9.0    1.0         63.6     9.0   \n",
       "73   3.0   7.0   4.0       46.0     2.0    6.0    4.0         38.4     2.0   \n",
       "75   4.0   6.0   6.0       58.4     7.0    8.0    7.0         60.8     7.0   \n",
       "76   6.0   5.0   6.0       54.8     3.0    4.0    7.0         50.8     7.0   \n",
       "77   2.0   5.0   6.0       43.6     1.0    4.0    7.0         42.8     1.0   \n",
       "79   6.0   8.0   8.0       82.8     6.0    9.0    7.0         79.2     0.0   \n",
       "80   0.0   4.0   6.0       40.0     0.0    4.0    5.0         45.6     0.0   \n",
       "81   2.0   8.0   9.0       60.4     2.0   10.0    9.0         71.6     0.0   \n",
       "83   5.0  10.0   3.0       63.2     4.0    8.0    2.0         53.6     3.0   \n",
       "84   6.0   8.0   6.0       65.2     6.0    9.0    6.0         60.4     6.0   \n",
       "87   8.0  10.0   9.0       88.4     8.0    2.0    7.0         78.0     6.0   \n",
       "91  10.0  10.0  10.0       69.2     9.0    6.0    2.0         61.2     9.0   \n",
       "92  10.0  10.0  10.0       80.8    10.0   10.0   10.0         75.2    10.0   \n",
       "94   2.0   7.0   7.0       48.8     6.0    7.0    7.0         50.0     4.0   \n",
       "95   8.0   9.0   9.0       69.2     8.0    7.0    8.0         69.6     7.0   \n",
       "\n",
       "    r_1.2  e_1.2  tfi_total.2  tfi_total.3  \n",
       "51    9.0    7.0         72.4         80.4  \n",
       "52    6.0    6.0         64.0         71.2  \n",
       "53    6.0    4.0         60.4         64.8  \n",
       "55    6.0    2.0         27.6         44.8  \n",
       "56    4.0    5.0         55.2         70.0  \n",
       "57    1.0    1.0         14.0         14.0  \n",
       "59    6.0    1.0         26.0         37.6  \n",
       "60   10.0    4.0         73.6         83.2  \n",
       "61    3.0    5.0         35.2         42.8  \n",
       "63    2.0    1.0         44.0         35.2  \n",
       "64    6.0    1.0         34.8         38.8  \n",
       "67    5.0    5.0         58.0         61.2  \n",
       "69    7.0    0.0         33.6         35.2  \n",
       "70    8.0    8.0         69.2         67.2  \n",
       "71    9.0    8.0         79.2         79.6  \n",
       "72    9.0    1.0         55.2         51.2  \n",
       "73    6.0    2.0         30.8         46.0  \n",
       "75    8.0    5.0         60.4         63.2  \n",
       "76    6.0    6.0         57.6         21.6  \n",
       "77    1.0    1.0         18.4         38.0  \n",
       "79    3.0    0.0         18.4         26.4  \n",
       "80    2.0    1.0         15.2         10.0  \n",
       "81    2.0    2.0         12.8          2.8  \n",
       "83    3.0    2.0         33.2         33.6  \n",
       "84    7.0    3.0         46.8         29.2  \n",
       "87    9.0    7.0         72.8         64.0  \n",
       "91    7.0    5.0         64.0         55.2  \n",
       "92   10.0   10.0         70.8         61.6  \n",
       "94    6.0    4.0         43.6         50.8  \n",
       "95    8.0    7.0         65.6         52.8  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stable_WN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "969db9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps=3\n",
    "n_features_US=5\n",
    "n_features_WN=4\n",
    "\n",
    "def data_prep(df,n_features):\n",
    "    sc_X = StandardScaler()\n",
    "    sc_y = StandardScaler()\n",
    "    \n",
    "    X=df.drop(['tfi_total.3'],axis=1)\n",
    "    y=df[['tfi_total.3']]\n",
    "    \n",
    "    #scale x\n",
    "    x_scaler=sc_X.fit(X)\n",
    "    X=x_scaler.transform(X)\n",
    "    #scale y\n",
    "    y_scaler=sc_y.fit(y)\n",
    "    y=y_scaler.transform(y)\n",
    "    \n",
    "    \"\"\"\n",
    "    Reshape rule:\n",
    "    tensor of shape (batch size, sequence length, features), \n",
    "    where sequence length is the number of time steps and features is each input timeseries.\n",
    "    \"\"\"\n",
    "    #X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    X = X.reshape((X.shape[0], n_timesteps, n_features))\n",
    "    print(X.shape,y.shape)\n",
    "    return X,y,x_scaler,y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b062212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Include residual connections, layer normalization, and dropout.\n",
    "The resulting layer can be stacked multiple times.\n",
    "The projection layers are implemented through `keras.layers.Conv1D`.\n",
    "\"\"\"\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x) \n",
    "    x = layers.Dense(units=inputs.shape[-1])(x) \n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    K.clear_session()\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    \"\"\"    \n",
    "    can stack multiple of the transformer_encoder blocks and \n",
    "    can also proceed to add the final Multi-Layer Perceptron regression head.\n",
    "    \"\"\"\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    \"\"\"\n",
    "    a pooling layer is used to to reduce the output tensor of the TransformerEncoder \n",
    "    part of our model down to a vector of features for each data point in the current batch.\n",
    "    \"\"\"\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x) \n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71c420cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and evaluate\n",
    "\n",
    "\n",
    "def prediction(x_train,y_train,y_scaler):\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    model = build_model(\n",
    "        input_shape,\n",
    "        head_size=5, # key_dim - Size of each attention head for query and key\n",
    "        num_heads=4, # Number of attention heads\n",
    "        ff_dim=5, # Hidden layer size in feed forward network inside transformer\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "\n",
    "    def error_in_tfi(y_true,y_pred): \n",
    "        y=y_true.numpy()\n",
    "        yhat=y_pred.numpy()\n",
    "        y=np.reshape(y, (1,-1))\n",
    "        yhat=np.reshape(yhat, (1,-1))\n",
    "        y=y_scaler.inverse_transform(y)\n",
    "        yhat=y_scaler.inverse_transform(yhat)\n",
    "        y=tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        yhat=tf.convert_to_tensor(yhat, dtype=tf.float32)\n",
    "        return K.mean(abs(y - yhat), axis=-1)  #K.mean(square(y_true - y_pred), axis=-1)\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        #metrics=[keras.metrics.MeanAbsoluteError()],\n",
    "        run_eagerly=True,\n",
    "        metrics=[error_in_tfi],\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "    history=model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=500,\n",
    "        batch_size=4,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    return model,history\n",
    "    #model.evaluate(x_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f879549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 3, 5) (31, 1)\n",
      "TRAIN: [ 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30] TEST: [0 1 2 3 4 5 6]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 5)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 5)        465         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 5)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 5)        10          ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 5)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         30          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 5)         30          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 5)        10          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 5)        10          ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 5)         30          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 5)        10          ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 5)        10          ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 5)         30          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 5)        10          ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 5)        10          ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3, 5)         30          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 5)        10          ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,821\n",
      "Trainable params: 2,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 1.1395 - error_in_tfi: 19.1418 - val_loss: 0.1653 - val_error_in_tfi: 6.9273\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 1.0862 - error_in_tfi: 19.3486 - val_loss: 0.1440 - val_error_in_tfi: 6.6961\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 1.0715 - error_in_tfi: 19.2035 - val_loss: 0.1286 - val_error_in_tfi: 6.4824\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9825 - error_in_tfi: 18.8415 - val_loss: 0.1151 - val_error_in_tfi: 6.2228\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.9816 - error_in_tfi: 18.4371 - val_loss: 0.1026 - val_error_in_tfi: 5.9705\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.9743 - error_in_tfi: 18.1041 - val_loss: 0.0943 - val_error_in_tfi: 5.7380\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8488 - error_in_tfi: 17.2900 - val_loss: 0.0891 - val_error_in_tfi: 5.4699\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8791 - error_in_tfi: 18.1210 - val_loss: 0.0802 - val_error_in_tfi: 5.1514\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8888 - error_in_tfi: 18.2162 - val_loss: 0.0796 - val_error_in_tfi: 5.0162\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7460 - error_in_tfi: 16.5253 - val_loss: 0.0685 - val_error_in_tfi: 4.6173\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.6928 - error_in_tfi: 15.8536 - val_loss: 0.0657 - val_error_in_tfi: 4.5058\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7395 - error_in_tfi: 15.8151 - val_loss: 0.0595 - val_error_in_tfi: 4.3472\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5990 - error_in_tfi: 14.6467 - val_loss: 0.0524 - val_error_in_tfi: 4.0626\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.6480 - error_in_tfi: 14.3905 - val_loss: 0.0516 - val_error_in_tfi: 3.9951\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.5991 - error_in_tfi: 13.5411 - val_loss: 0.0464 - val_error_in_tfi: 3.6876\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 130ms/step - loss: 0.5557 - error_in_tfi: 13.7920 - val_loss: 0.0427 - val_error_in_tfi: 3.4500\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6148 - error_in_tfi: 14.9288 - val_loss: 0.0390 - val_error_in_tfi: 3.3144\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5417 - error_in_tfi: 13.2852 - val_loss: 0.0367 - val_error_in_tfi: 3.1392\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.4805 - error_in_tfi: 11.6000 - val_loss: 0.0331 - val_error_in_tfi: 2.8624\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.5619 - error_in_tfi: 14.0245 - val_loss: 0.0320 - val_error_in_tfi: 2.6908\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.5850 - error_in_tfi: 13.7623 - val_loss: 0.0344 - val_error_in_tfi: 2.7342\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 1s 128ms/step - loss: 0.4478 - error_in_tfi: 11.9546 - val_loss: 0.0341 - val_error_in_tfi: 2.6474\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.5297 - error_in_tfi: 12.2229 - val_loss: 0.0346 - val_error_in_tfi: 2.6302\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.4479 - error_in_tfi: 11.9222 - val_loss: 0.0339 - val_error_in_tfi: 2.5479\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.4481 - error_in_tfi: 11.8048 - val_loss: 0.0337 - val_error_in_tfi: 2.4703\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.4858 - error_in_tfi: 11.5561 - val_loss: 0.0348 - val_error_in_tfi: 2.3821\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.5026 - error_in_tfi: 12.0651 - val_loss: 0.0329 - val_error_in_tfi: 2.2001\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.4353 - error_in_tfi: 11.0714 - val_loss: 0.0365 - val_error_in_tfi: 2.3037\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.4522 - error_in_tfi: 11.4577 - val_loss: 0.0354 - val_error_in_tfi: 2.1546\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4042 - error_in_tfi: 10.9922 - val_loss: 0.0357 - val_error_in_tfi: 2.0875\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3839 - error_in_tfi: 10.8338\n",
      "TRAIN: [ 0  1  2  3  4  5  6 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n",
      " 30] TEST: [ 7  8  9 10 11 12]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 5)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 5)        465         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout (Dropout)              (None, 3, 5)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 5)        10          ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 5)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         30          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 5)         30          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 5)        10          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 5)        10          ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 5)         30          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 5)        10          ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 5)        10          ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 5)         30          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 5)        10          ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 5)        10          ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3, 5)         30          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layer_normalization_7 (LayerNo  (None, 3, 5)        10          ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,821\n",
      "Trainable params: 2,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 1.1121 - error_in_tfi: 18.5822 - val_loss: 0.1227 - val_error_in_tfi: 5.9180\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.1001 - error_in_tfi: 18.0801 - val_loss: 0.1004 - val_error_in_tfi: 5.6605\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.9336 - error_in_tfi: 16.2713 - val_loss: 0.0820 - val_error_in_tfi: 5.2973\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8669 - error_in_tfi: 15.7170 - val_loss: 0.0689 - val_error_in_tfi: 4.9334\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.8082 - error_in_tfi: 15.9826 - val_loss: 0.0586 - val_error_in_tfi: 4.5895\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.7333 - error_in_tfi: 14.4126 - val_loss: 0.0506 - val_error_in_tfi: 4.2011\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6991 - error_in_tfi: 14.8481 - val_loss: 0.0460 - val_error_in_tfi: 4.2160\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6704 - error_in_tfi: 14.2155 - val_loss: 0.0428 - val_error_in_tfi: 4.3085\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.5707 - error_in_tfi: 13.4409 - val_loss: 0.0404 - val_error_in_tfi: 4.3933\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.5148 - error_in_tfi: 12.3867 - val_loss: 0.0399 - val_error_in_tfi: 4.6810\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.5192 - error_in_tfi: 12.8177 - val_loss: 0.0398 - val_error_in_tfi: 4.6085\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4160 - error_in_tfi: 11.1602 - val_loss: 0.0388 - val_error_in_tfi: 4.4833\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4739 - error_in_tfi: 12.1362 - val_loss: 0.0381 - val_error_in_tfi: 4.3744\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.3657 - error_in_tfi: 10.5872 - val_loss: 0.0384 - val_error_in_tfi: 4.2513\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.3652 - error_in_tfi: 10.7623 - val_loss: 0.0404 - val_error_in_tfi: 4.3221\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.3262 - error_in_tfi: 9.5545 - val_loss: 0.0405 - val_error_in_tfi: 4.4523\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.2745 - error_in_tfi: 8.6779 - val_loss: 0.0413 - val_error_in_tfi: 4.7016\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2777 - error_in_tfi: 8.9404 - val_loss: 0.0429 - val_error_in_tfi: 4.7892\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.2876 - error_in_tfi: 9.2281 - val_loss: 0.0443 - val_error_in_tfi: 4.9458\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 0.3002 - error_in_tfi: 9.0826 - val_loss: 0.0452 - val_error_in_tfi: 4.7227\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.2423 - error_in_tfi: 8.7675 - val_loss: 0.0428 - val_error_in_tfi: 4.1990\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2312 - error_in_tfi: 8.0596 - val_loss: 0.0331 - val_error_in_tfi: 3.2116\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.2752 - error_in_tfi: 8.6961 - val_loss: 0.0313 - val_error_in_tfi: 2.8808\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.2244 - error_in_tfi: 8.0677 - val_loss: 0.0337 - val_error_in_tfi: 2.5133\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.2381 - error_in_tfi: 8.3586 - val_loss: 0.0281 - val_error_in_tfi: 2.4686\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.2119 - error_in_tfi: 8.2222 - val_loss: 0.0260 - val_error_in_tfi: 2.4077\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.1896 - error_in_tfi: 7.8683 - val_loss: 0.0225 - val_error_in_tfi: 1.6037\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.1708 - error_in_tfi: 7.4474 - val_loss: 0.0253 - val_error_in_tfi: 2.7175\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.2044 - error_in_tfi: 8.0647 - val_loss: 0.0207 - val_error_in_tfi: 2.1562\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.1946 - error_in_tfi: 7.7977 - val_loss: 0.0211 - val_error_in_tfi: 1.8918\n",
      "Epoch 31/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.1836 - error_in_tfi: 7.8347 - val_loss: 0.0218 - val_error_in_tfi: 1.7997\n",
      "Epoch 32/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.1446 - error_in_tfi: 7.1307 - val_loss: 0.0211 - val_error_in_tfi: 1.7766\n",
      "Epoch 33/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.1694 - error_in_tfi: 7.3091 - val_loss: 0.0213 - val_error_in_tfi: 1.8680\n",
      "Epoch 34/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1607 - error_in_tfi: 7.0580 - val_loss: 0.0213 - val_error_in_tfi: 2.0035\n",
      "Epoch 35/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.1490 - error_in_tfi: 6.8831 - val_loss: 0.0267 - val_error_in_tfi: 2.5899\n",
      "Epoch 36/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1452 - error_in_tfi: 6.8411 - val_loss: 0.0316 - val_error_in_tfi: 2.9818\n",
      "Epoch 37/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1701 - error_in_tfi: 6.9508 - val_loss: 0.0381 - val_error_in_tfi: 3.7030\n",
      "Epoch 38/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.1348 - error_in_tfi: 6.3986 - val_loss: 0.0480 - val_error_in_tfi: 4.6028\n",
      "Epoch 39/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.1077 - error_in_tfi: 5.5825 - val_loss: 0.0536 - val_error_in_tfi: 4.9793\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9239 - error_in_tfi: 12.2964\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 19 20 21 22 23 24 25 26 27 28 29\n",
      " 30] TEST: [13 14 15 16 17 18]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_1 (InputLayer)           [(None, 3, 5)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 5)        465         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 5)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 5)        10          ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 5)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         30          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 5)         30          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 5)        10          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 5)        10          ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 5)         30          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 5)        10          ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 5)        10          ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 5)         30          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 5)        10          ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 5)        10          ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_6[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3, 5)         30          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 5)        10          ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,821\n",
      "Trainable params: 2,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.9716 - error_in_tfi: 17.1324 - val_loss: 0.2856 - val_error_in_tfi: 9.1665\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0068 - error_in_tfi: 17.6570 - val_loss: 0.2716 - val_error_in_tfi: 9.1310\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9020 - error_in_tfi: 16.6575 - val_loss: 0.2564 - val_error_in_tfi: 9.0378\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.8495 - error_in_tfi: 16.6474 - val_loss: 0.2452 - val_error_in_tfi: 9.0412\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7142 - error_in_tfi: 15.0753 - val_loss: 0.2389 - val_error_in_tfi: 9.0421\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6706 - error_in_tfi: 14.6276 - val_loss: 0.2330 - val_error_in_tfi: 9.0614\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6311 - error_in_tfi: 14.0183 - val_loss: 0.2291 - val_error_in_tfi: 9.0897\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5948 - error_in_tfi: 14.1071 - val_loss: 0.2261 - val_error_in_tfi: 9.0014\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4981 - error_in_tfi: 12.7423 - val_loss: 0.2190 - val_error_in_tfi: 8.9822\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4845 - error_in_tfi: 12.4743 - val_loss: 0.2168 - val_error_in_tfi: 9.0837\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4853 - error_in_tfi: 12.2684 - val_loss: 0.2093 - val_error_in_tfi: 8.9825\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.5082 - error_in_tfi: 13.2223 - val_loss: 0.1938 - val_error_in_tfi: 8.5696\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4057 - error_in_tfi: 11.2461 - val_loss: 0.1908 - val_error_in_tfi: 8.5870\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4421 - error_in_tfi: 12.2225 - val_loss: 0.1861 - val_error_in_tfi: 8.4454\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3892 - error_in_tfi: 11.0693 - val_loss: 0.1712 - val_error_in_tfi: 7.7101\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3559 - error_in_tfi: 10.0574 - val_loss: 0.1554 - val_error_in_tfi: 7.1353\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.3451 - error_in_tfi: 10.4461 - val_loss: 0.1555 - val_error_in_tfi: 7.3147\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3244 - error_in_tfi: 10.1518 - val_loss: 0.1535 - val_error_in_tfi: 7.3420\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3848 - error_in_tfi: 10.8958 - val_loss: 0.1494 - val_error_in_tfi: 7.1055\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.3726 - error_in_tfi: 10.4392 - val_loss: 0.1440 - val_error_in_tfi: 6.7638\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.2730 - error_in_tfi: 9.2754 - val_loss: 0.1374 - val_error_in_tfi: 6.3491\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3055 - error_in_tfi: 9.4572 - val_loss: 0.1348 - val_error_in_tfi: 6.2235\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.2826 - error_in_tfi: 9.4816 - val_loss: 0.1326 - val_error_in_tfi: 6.1079\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.2772 - error_in_tfi: 9.2252 - val_loss: 0.1295 - val_error_in_tfi: 5.9743\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.3119 - error_in_tfi: 9.3927 - val_loss: 0.1352 - val_error_in_tfi: 5.9861\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.2650 - error_in_tfi: 8.7148 - val_loss: 0.1386 - val_error_in_tfi: 6.0133\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.2772 - error_in_tfi: 9.1743 - val_loss: 0.1419 - val_error_in_tfi: 6.0066\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.2337 - error_in_tfi: 8.3425 - val_loss: 0.1432 - val_error_in_tfi: 5.9317\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.2875 - error_in_tfi: 9.0610 - val_loss: 0.1434 - val_error_in_tfi: 5.7871\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.2469 - error_in_tfi: 8.4218 - val_loss: 0.1486 - val_error_in_tfi: 5.6680\n",
      "Epoch 31/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.2684 - error_in_tfi: 8.9258 - val_loss: 0.1424 - val_error_in_tfi: 5.5776\n",
      "Epoch 32/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.2634 - error_in_tfi: 9.0378 - val_loss: 0.1375 - val_error_in_tfi: 5.5163\n",
      "Epoch 33/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.2365 - error_in_tfi: 8.1829 - val_loss: 0.1388 - val_error_in_tfi: 5.4442\n",
      "Epoch 34/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2201 - error_in_tfi: 7.4068 - val_loss: 0.1385 - val_error_in_tfi: 5.4024\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6905 - error_in_tfi: 15.7931\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 25 26 27 28 29\n",
      " 30] TEST: [19 20 21 22 23 24]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 5)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 5)        465         ['input_1[0][0]',                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 5)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 5)        10          ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 5)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         30          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 5)         30          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 5)        10          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 5)        10          ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 5)         30          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 5)        10          ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 5)        10          ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 5)         30          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 5)        10          ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 5)        10          ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_7 (Dense)                (None, 3, 5)         30          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 5)        10          ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,821\n",
      "Trainable params: 2,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.9878 - error_in_tfi: 19.9737 - val_loss: 0.2330 - val_error_in_tfi: 5.6016\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8558 - error_in_tfi: 18.4598 - val_loss: 0.2054 - val_error_in_tfi: 5.2407\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9066 - error_in_tfi: 18.4878 - val_loss: 0.1824 - val_error_in_tfi: 5.0825\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8370 - error_in_tfi: 18.4132 - val_loss: 0.1657 - val_error_in_tfi: 5.0296\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8296 - error_in_tfi: 17.8659 - val_loss: 0.1573 - val_error_in_tfi: 5.2487\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7458 - error_in_tfi: 17.1853 - val_loss: 0.1495 - val_error_in_tfi: 5.3477\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6910 - error_in_tfi: 15.9400 - val_loss: 0.1450 - val_error_in_tfi: 5.6300\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6057 - error_in_tfi: 15.0079 - val_loss: 0.1391 - val_error_in_tfi: 5.9083\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.5551 - error_in_tfi: 14.7488 - val_loss: 0.1379 - val_error_in_tfi: 6.1813\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4844 - error_in_tfi: 13.4058 - val_loss: 0.1302 - val_error_in_tfi: 6.1175\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.5407 - error_in_tfi: 14.1147 - val_loss: 0.1248 - val_error_in_tfi: 6.2519\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4519 - error_in_tfi: 11.8809 - val_loss: 0.1199 - val_error_in_tfi: 6.2826\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.4889 - error_in_tfi: 13.4147 - val_loss: 0.1147 - val_error_in_tfi: 6.0959\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.5210 - error_in_tfi: 13.0209 - val_loss: 0.1054 - val_error_in_tfi: 5.8077\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.3598 - error_in_tfi: 10.8475 - val_loss: 0.1084 - val_error_in_tfi: 5.9007\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3468 - error_in_tfi: 10.3758 - val_loss: 0.1036 - val_error_in_tfi: 5.7422\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.4101 - error_in_tfi: 11.2469 - val_loss: 0.1049 - val_error_in_tfi: 5.7929\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.3837 - error_in_tfi: 10.9575 - val_loss: 0.1050 - val_error_in_tfi: 5.8496\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4187 - error_in_tfi: 11.0879 - val_loss: 0.0999 - val_error_in_tfi: 5.5622\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.3830 - error_in_tfi: 10.0888 - val_loss: 0.1035 - val_error_in_tfi: 5.4939\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4052 - error_in_tfi: 11.2155 - val_loss: 0.0915 - val_error_in_tfi: 4.8630\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3238 - error_in_tfi: 9.8492 - val_loss: 0.0883 - val_error_in_tfi: 4.7098\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.3502 - error_in_tfi: 10.2352 - val_loss: 0.0943 - val_error_in_tfi: 5.0531\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.3840 - error_in_tfi: 10.4375 - val_loss: 0.1080 - val_error_in_tfi: 5.5758\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.3248 - error_in_tfi: 10.4505 - val_loss: 0.1143 - val_error_in_tfi: 5.8934\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.3424 - error_in_tfi: 9.7952 - val_loss: 0.1227 - val_error_in_tfi: 6.1934\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.3252 - error_in_tfi: 10.1516 - val_loss: 0.1330 - val_error_in_tfi: 6.4888\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.3087 - error_in_tfi: 9.7799 - val_loss: 0.1388 - val_error_in_tfi: 6.6665\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2795 - error_in_tfi: 9.1505 - val_loss: 0.1366 - val_error_in_tfi: 6.5754\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.2854 - error_in_tfi: 8.8109 - val_loss: 0.1375 - val_error_in_tfi: 6.6196\n",
      "Epoch 31/500\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.2639 - error_in_tfi: 8.7847 - val_loss: 0.1471 - val_error_in_tfi: 6.9267\n",
      "Epoch 32/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.3064 - error_in_tfi: 10.0721 - val_loss: 0.1502 - val_error_in_tfi: 6.9636\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4981 - error_in_tfi: 12.9777\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24] TEST: [25 26 27 28 29 30]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 5)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 5)        465         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 5)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 5)        10          ['dropout[0][0]']                \n",
      " alization)                                                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 5)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         30          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 5)         30          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 5)        10          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 5)        10          ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 5)         30          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 5)        10          ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 5)        10          ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 5)         30          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 5)        10          ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 5)        465         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 5)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 5)        10          ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         30          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3, 5)         30          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 5)        10          ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 5)        0           ['layer_normalization_7[0][0]',  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,821\n",
      "Trainable params: 2,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 1.1119 - error_in_tfi: 19.3775 - val_loss: 1.1543 - val_error_in_tfi: 19.0964\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.0965 - error_in_tfi: 19.6029 - val_loss: 1.1141 - val_error_in_tfi: 18.9455\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.9565 - error_in_tfi: 18.5829 - val_loss: 1.0717 - val_error_in_tfi: 18.7088\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.9818 - error_in_tfi: 18.7263 - val_loss: 1.0283 - val_error_in_tfi: 18.5412\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8741 - error_in_tfi: 17.7143 - val_loss: 0.9834 - val_error_in_tfi: 18.4764\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.7811 - error_in_tfi: 16.3615 - val_loss: 0.9390 - val_error_in_tfi: 18.3750\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.7008 - error_in_tfi: 15.9827 - val_loss: 0.8984 - val_error_in_tfi: 18.1865\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.6876 - error_in_tfi: 16.0701 - val_loss: 0.8610 - val_error_in_tfi: 18.0747\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.6895 - error_in_tfi: 15.2721 - val_loss: 0.8274 - val_error_in_tfi: 17.9078\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6610 - error_in_tfi: 14.8587 - val_loss: 0.7967 - val_error_in_tfi: 17.6384\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.6075 - error_in_tfi: 14.5791 - val_loss: 0.7688 - val_error_in_tfi: 17.2118\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.6351 - error_in_tfi: 14.7395 - val_loss: 0.7435 - val_error_in_tfi: 16.7568\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.5702 - error_in_tfi: 13.3748 - val_loss: 0.7186 - val_error_in_tfi: 16.3950\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.5351 - error_in_tfi: 13.5778 - val_loss: 0.6932 - val_error_in_tfi: 15.9071\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.4787 - error_in_tfi: 12.8151 - val_loss: 0.6749 - val_error_in_tfi: 15.4940\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4366 - error_in_tfi: 11.9252 - val_loss: 0.6542 - val_error_in_tfi: 15.0780\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.4278 - error_in_tfi: 11.7130 - val_loss: 0.6405 - val_error_in_tfi: 14.6644\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3887 - error_in_tfi: 10.9883 - val_loss: 0.6242 - val_error_in_tfi: 14.1853\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.4572 - error_in_tfi: 11.7103 - val_loss: 0.6006 - val_error_in_tfi: 13.9031\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4344 - error_in_tfi: 11.0551 - val_loss: 0.5839 - val_error_in_tfi: 13.6083\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.4452 - error_in_tfi: 11.7214 - val_loss: 0.5669 - val_error_in_tfi: 13.2793\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.4336 - error_in_tfi: 11.3348 - val_loss: 0.5527 - val_error_in_tfi: 12.8513\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4069 - error_in_tfi: 10.8605 - val_loss: 0.5443 - val_error_in_tfi: 12.4867\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.3719 - error_in_tfi: 10.4513 - val_loss: 0.5332 - val_error_in_tfi: 12.1018\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.3924 - error_in_tfi: 10.4621 - val_loss: 0.5229 - val_error_in_tfi: 11.8741\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.3672 - error_in_tfi: 10.5629 - val_loss: 0.5122 - val_error_in_tfi: 11.7088\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3373 - error_in_tfi: 9.6779 - val_loss: 0.4999 - val_error_in_tfi: 11.5861\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3871 - error_in_tfi: 10.5591 - val_loss: 0.4908 - val_error_in_tfi: 11.5420\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3629 - error_in_tfi: 10.5637 - val_loss: 0.4774 - val_error_in_tfi: 11.5142\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3013 - error_in_tfi: 9.1711 - val_loss: 0.4742 - val_error_in_tfi: 11.5025\n",
      "Epoch 31/500\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.2979 - error_in_tfi: 9.5857 - val_loss: 0.4741 - val_error_in_tfi: 11.4562\n",
      "Epoch 32/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.2909 - error_in_tfi: 8.7963 - val_loss: 0.4705 - val_error_in_tfi: 11.3938\n",
      "Epoch 33/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.2885 - error_in_tfi: 9.4375 - val_loss: 0.4641 - val_error_in_tfi: 11.2857\n",
      "Epoch 34/500\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.3439 - error_in_tfi: 9.3303 - val_loss: 0.4521 - val_error_in_tfi: 11.1519\n",
      "Epoch 35/500\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.2717 - error_in_tfi: 8.9854 - val_loss: 0.4450 - val_error_in_tfi: 11.0099\n",
      "Epoch 36/500\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.3450 - error_in_tfi: 9.6162 - val_loss: 0.3975 - val_error_in_tfi: 10.3319\n",
      "Epoch 37/500\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.3355 - error_in_tfi: 10.0870 - val_loss: 0.3756 - val_error_in_tfi: 10.0437\n",
      "Epoch 38/500\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.2938 - error_in_tfi: 9.1960 - val_loss: 0.3670 - val_error_in_tfi: 10.0129\n",
      "Epoch 39/500\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.2445 - error_in_tfi: 8.5814 - val_loss: 0.3763 - val_error_in_tfi: 10.2197\n",
      "Epoch 40/500\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.2708 - error_in_tfi: 8.5498 - val_loss: 0.3578 - val_error_in_tfi: 9.9713\n",
      "Epoch 41/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3015 - error_in_tfi: 8.9480 - val_loss: 0.3461 - val_error_in_tfi: 9.8277\n",
      "Epoch 42/500\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 0.2222 - error_in_tfi: 8.0344 - val_loss: 0.3395 - val_error_in_tfi: 9.6666\n",
      "Epoch 43/500\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.2744 - error_in_tfi: 8.9999 - val_loss: 0.3351 - val_error_in_tfi: 9.5314\n",
      "Epoch 44/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.2266 - error_in_tfi: 8.0439 - val_loss: 0.3214 - val_error_in_tfi: 9.3628\n",
      "Epoch 45/500\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.2781 - error_in_tfi: 9.0624 - val_loss: 0.3150 - val_error_in_tfi: 9.2953\n",
      "Epoch 46/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.2677 - error_in_tfi: 8.9798 - val_loss: 0.3146 - val_error_in_tfi: 9.2039\n",
      "Epoch 47/500\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.2477 - error_in_tfi: 7.9622 - val_loss: 0.3069 - val_error_in_tfi: 8.9996\n",
      "Epoch 48/500\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.2345 - error_in_tfi: 8.4283 - val_loss: 0.2996 - val_error_in_tfi: 8.9171\n",
      "Epoch 49/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.2315 - error_in_tfi: 8.1190 - val_loss: 0.2896 - val_error_in_tfi: 8.8293\n",
      "Epoch 50/500\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.1939 - error_in_tfi: 7.5375 - val_loss: 0.2829 - val_error_in_tfi: 8.6992\n",
      "Epoch 51/500\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.2197 - error_in_tfi: 7.6034 - val_loss: 0.2753 - val_error_in_tfi: 8.7818\n",
      "Epoch 52/500\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.2072 - error_in_tfi: 7.9135 - val_loss: 0.2965 - val_error_in_tfi: 9.3032\n",
      "Epoch 53/500\n",
      "5/5 [==============================] - 1s 130ms/step - loss: 0.2348 - error_in_tfi: 7.8813 - val_loss: 0.3265 - val_error_in_tfi: 9.6487\n",
      "Epoch 54/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.2144 - error_in_tfi: 7.6041 - val_loss: 0.3313 - val_error_in_tfi: 9.6537\n",
      "Epoch 55/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.1907 - error_in_tfi: 7.4775 - val_loss: 0.3344 - val_error_in_tfi: 9.5513\n",
      "Epoch 56/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1884 - error_in_tfi: 7.1347 - val_loss: 0.3438 - val_error_in_tfi: 9.5859\n",
      "Epoch 57/500\n",
      "5/5 [==============================] - 1s 129ms/step - loss: 0.1574 - error_in_tfi: 6.0369 - val_loss: 0.3457 - val_error_in_tfi: 9.5280\n",
      "Epoch 58/500\n",
      "5/5 [==============================] - 1s 130ms/step - loss: 0.2520 - error_in_tfi: 7.7553 - val_loss: 0.3472 - val_error_in_tfi: 9.5244\n",
      "Epoch 59/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.2036 - error_in_tfi: 8.1056 - val_loss: 0.3435 - val_error_in_tfi: 9.4530\n",
      "Epoch 60/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.2516 - error_in_tfi: 8.1569 - val_loss: 0.3433 - val_error_in_tfi: 9.3663\n",
      "Epoch 61/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.2037 - error_in_tfi: 7.1586 - val_loss: 0.3418 - val_error_in_tfi: 9.3461\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2536 - error_in_tfi: 9.7469\n",
      "(30, 3, 4) (30, 1)\n",
      "TRAIN: [ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] TEST: [0 1 2 3 4 5]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 4)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 4)        384         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 4)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 4)        8           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 4)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         25          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 4)         24          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 4)        8           ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 4)        8           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 4)         24          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 4)        8           ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_2[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 4)        8           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 4)         24          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 4)        8           ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 4)        8           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3, 4)         24          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 4)        8           ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,437\n",
      "Trainable params: 2,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 193ms/step - loss: 1.2074 - error_in_tfi: 19.3207 - val_loss: 0.1613 - val_error_in_tfi: 5.6503\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.2056 - error_in_tfi: 18.0302 - val_loss: 0.1349 - val_error_in_tfi: 5.0189\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.0736 - error_in_tfi: 16.5272 - val_loss: 0.1243 - val_error_in_tfi: 4.7814\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9967 - error_in_tfi: 17.0746 - val_loss: 0.1167 - val_error_in_tfi: 4.6096\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8267 - error_in_tfi: 14.7428 - val_loss: 0.0971 - val_error_in_tfi: 3.9742\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8291 - error_in_tfi: 14.5029 - val_loss: 0.0869 - val_error_in_tfi: 3.5519\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8339 - error_in_tfi: 15.4868 - val_loss: 0.0800 - val_error_in_tfi: 3.1264\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7155 - error_in_tfi: 13.4676 - val_loss: 0.0729 - val_error_in_tfi: 2.9136\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6023 - error_in_tfi: 12.3916 - val_loss: 0.0666 - val_error_in_tfi: 2.9908\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6807 - error_in_tfi: 14.6319 - val_loss: 0.0636 - val_error_in_tfi: 3.0600\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5773 - error_in_tfi: 12.7698 - val_loss: 0.0616 - val_error_in_tfi: 3.5438\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.5862 - error_in_tfi: 12.1969 - val_loss: 0.0656 - val_error_in_tfi: 4.2633\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.5816 - error_in_tfi: 12.0330 - val_loss: 0.0730 - val_error_in_tfi: 4.8297\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.5734 - error_in_tfi: 12.7950 - val_loss: 0.0813 - val_error_in_tfi: 5.3302\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.5600 - error_in_tfi: 11.8938 - val_loss: 0.0880 - val_error_in_tfi: 5.7143\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.4367 - error_in_tfi: 10.1507 - val_loss: 0.0981 - val_error_in_tfi: 6.0983\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.5100 - error_in_tfi: 11.2755 - val_loss: 0.1102 - val_error_in_tfi: 6.4934\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.4325 - error_in_tfi: 11.2615 - val_loss: 0.1249 - val_error_in_tfi: 6.9334\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.4395 - error_in_tfi: 10.4499 - val_loss: 0.1428 - val_error_in_tfi: 7.4060\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.4300 - error_in_tfi: 10.6251 - val_loss: 0.1671 - val_error_in_tfi: 8.0961\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.3855 - error_in_tfi: 9.9065 - val_loss: 0.1836 - val_error_in_tfi: 8.5992\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.1172 - error_in_tfi: 20.6272\n",
      "TRAIN: [ 0  1  2  3  4  5 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] TEST: [ 6  7  8  9 10 11]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 4)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 4)        384         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 4)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 4)        8           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 4)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         25          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 4)         24          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 4)        8           ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 4)        8           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 4)         24          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 4)        8           ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 4)        8           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 4)         24          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 4)        8           ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_6 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 4)        8           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3, 4)         24          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 4)        8           ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,437\n",
      "Trainable params: 2,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 1.2205 - error_in_tfi: 19.9546 - val_loss: 0.1652 - val_error_in_tfi: 5.8664\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 1.2137 - error_in_tfi: 19.8860 - val_loss: 0.1167 - val_error_in_tfi: 4.6361\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.0433 - error_in_tfi: 17.5038 - val_loss: 0.0767 - val_error_in_tfi: 3.2771\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.9870 - error_in_tfi: 17.2275 - val_loss: 0.0483 - val_error_in_tfi: 2.5295\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8499 - error_in_tfi: 16.1016 - val_loss: 0.0358 - val_error_in_tfi: 2.8499\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7708 - error_in_tfi: 15.0150 - val_loss: 0.0332 - val_error_in_tfi: 3.3007\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.8293 - error_in_tfi: 15.6085 - val_loss: 0.0369 - val_error_in_tfi: 4.0918\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.6972 - error_in_tfi: 14.7311 - val_loss: 0.0420 - val_error_in_tfi: 4.6692\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.7285 - error_in_tfi: 14.6571 - val_loss: 0.0561 - val_error_in_tfi: 5.3623\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.6686 - error_in_tfi: 14.7842 - val_loss: 0.0743 - val_error_in_tfi: 6.0023\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.5884 - error_in_tfi: 13.0684 - val_loss: 0.0929 - val_error_in_tfi: 6.5188\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.5197 - error_in_tfi: 12.1617 - val_loss: 0.1166 - val_error_in_tfi: 7.2819\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.5237 - error_in_tfi: 12.6425 - val_loss: 0.1327 - val_error_in_tfi: 7.6955\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5788 - error_in_tfi: 12.7167 - val_loss: 0.1511 - val_error_in_tfi: 8.1848\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.5182 - error_in_tfi: 12.3644 - val_loss: 0.1764 - val_error_in_tfi: 8.7893\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.4617 - error_in_tfi: 10.4893 - val_loss: 0.1946 - val_error_in_tfi: 9.2019\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4005 - error_in_tfi: 9.5321\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 18 19 20 21 22 23 24 25 26 27 28 29] TEST: [12 13 14 15 16 17]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 4)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 4)        384         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 4)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 4)        8           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 4)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         25          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 4)         24          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 4)        8           ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 4)        8           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 4)         24          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 4)        8           ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 4)        8           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 4)         24          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 4)        8           ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 4)        8           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3, 4)         24          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 4)        8           ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Total params: 2,437\n",
      "Trainable params: 2,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 1.1415 - error_in_tfi: 19.3475 - val_loss: 0.0445 - val_error_in_tfi: 2.9546\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 1.0365 - error_in_tfi: 18.5211 - val_loss: 0.0383 - val_error_in_tfi: 3.3378\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9905 - error_in_tfi: 17.7199 - val_loss: 0.0386 - val_error_in_tfi: 3.8095\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.8844 - error_in_tfi: 16.7837 - val_loss: 0.0403 - val_error_in_tfi: 4.0482\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.8986 - error_in_tfi: 16.3104 - val_loss: 0.0470 - val_error_in_tfi: 4.6958\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.8380 - error_in_tfi: 15.4648 - val_loss: 0.0575 - val_error_in_tfi: 5.2762\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9197 - error_in_tfi: 16.3239 - val_loss: 0.0723 - val_error_in_tfi: 5.8769\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.7053 - error_in_tfi: 14.3181 - val_loss: 0.0858 - val_error_in_tfi: 6.2892\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.8745 - error_in_tfi: 16.0945 - val_loss: 0.1042 - val_error_in_tfi: 6.7999\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.6517 - error_in_tfi: 13.9442 - val_loss: 0.1334 - val_error_in_tfi: 7.6063\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.7070 - error_in_tfi: 13.9876 - val_loss: 0.1589 - val_error_in_tfi: 8.3512\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.7701 - error_in_tfi: 15.3600 - val_loss: 0.2052 - val_error_in_tfi: 9.5390\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4032 - error_in_tfi: 11.0785\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 24 25 26 27 28 29] TEST: [18 19 20 21 22 23]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 4)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 4)        384         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 4)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 4)        8           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 4)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         25          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 4)         24          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 4)        8           ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 4)        8           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 4)         24          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 4)        8           ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 4)        8           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_4[0][0]',  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 4)         24          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 4)        8           ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 4)        8           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3, 4)         24          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 4)        8           ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,437\n",
      "Trainable params: 2,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.9656 - error_in_tfi: 17.3530 - val_loss: 0.3853 - val_error_in_tfi: 8.7347\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9300 - error_in_tfi: 16.8603 - val_loss: 0.2984 - val_error_in_tfi: 7.1790\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.8039 - error_in_tfi: 15.1215 - val_loss: 0.2236 - val_error_in_tfi: 5.5635\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.9001 - error_in_tfi: 16.0727 - val_loss: 0.1588 - val_error_in_tfi: 3.7394\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6823 - error_in_tfi: 13.6606 - val_loss: 0.1169 - val_error_in_tfi: 3.7185\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7474 - error_in_tfi: 14.2235 - val_loss: 0.0839 - val_error_in_tfi: 4.1919\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.7304 - error_in_tfi: 14.0340 - val_loss: 0.0712 - val_error_in_tfi: 5.0132\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.6300 - error_in_tfi: 13.2386 - val_loss: 0.0792 - val_error_in_tfi: 5.8981\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.5059 - error_in_tfi: 11.7143 - val_loss: 0.1069 - val_error_in_tfi: 6.8698\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.5216 - error_in_tfi: 12.2839 - val_loss: 0.1507 - val_error_in_tfi: 8.1520\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.4320 - error_in_tfi: 10.5614 - val_loss: 0.2106 - val_error_in_tfi: 9.4350\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.3918 - error_in_tfi: 9.8230 - val_loss: 0.2672 - val_error_in_tfi: 10.5701\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.3317 - error_in_tfi: 8.8703 - val_loss: 0.3268 - val_error_in_tfi: 11.8321\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.3883 - error_in_tfi: 10.4839 - val_loss: 0.3677 - val_error_in_tfi: 12.6467\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.3611 - error_in_tfi: 10.2588 - val_loss: 0.3864 - val_error_in_tfi: 13.0544\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.2766 - error_in_tfi: 8.5374 - val_loss: 0.4076 - val_error_in_tfi: 13.4781\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.3228 - error_in_tfi: 9.6972 - val_loss: 0.4106 - val_error_in_tfi: 13.6021\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.7492 - error_in_tfi: 23.9994\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] TEST: [24 25 26 27 28 29]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 4)]       0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 4)        384         ['input_1[0][0]',                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 4)         0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 4)        8           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 4)        0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 5)         25          ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 5)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 4)         24          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 4)        8           ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_1[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 3, 4)        8           ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_2[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 3, 5)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3, 4)         24          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 3, 4)        8           ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_3[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 3, 4)        8           ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 5)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3, 4)         24          ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 3, 4)        8           ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_5[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 3, 4)        384         ['tf.__operators__.add_5[0][0]', \n",
      " eadAttention)                                                    'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 3, 4)         0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 3, 4)        8           ['dropout_6[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_6[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3, 5)         25          ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 3, 5)         0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_7 (Dense)                (None, 3, 4)         24          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 3, 4)        8           ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 3, 4)        0           ['layer_normalization_7[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 3)           0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 128)          512         ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,437\n",
      "Trainable params: 2,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.1550 - error_in_tfi: 18.2096 - val_loss: 1.6482 - val_error_in_tfi: 18.4209\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.0054 - error_in_tfi: 17.5899 - val_loss: 1.5437 - val_error_in_tfi: 17.4023\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.9071 - error_in_tfi: 16.5404 - val_loss: 1.4869 - val_error_in_tfi: 17.0102\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.8162 - error_in_tfi: 15.2383 - val_loss: 1.4066 - val_error_in_tfi: 16.5028\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6825 - error_in_tfi: 14.0099 - val_loss: 1.3507 - val_error_in_tfi: 16.0961\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.6984 - error_in_tfi: 13.5081 - val_loss: 1.3147 - val_error_in_tfi: 15.8429\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6146 - error_in_tfi: 13.1538 - val_loss: 1.2755 - val_error_in_tfi: 15.5602\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.5577 - error_in_tfi: 12.3351 - val_loss: 1.2806 - val_error_in_tfi: 15.6233\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.5110 - error_in_tfi: 11.9901 - val_loss: 1.2862 - val_error_in_tfi: 15.7085\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.4882 - error_in_tfi: 11.5075 - val_loss: 1.3072 - val_error_in_tfi: 15.8748\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4830 - error_in_tfi: 11.4998 - val_loss: 1.3090 - val_error_in_tfi: 15.8574\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4446 - error_in_tfi: 11.1394 - val_loss: 1.2948 - val_error_in_tfi: 15.7731\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3846 - error_in_tfi: 9.6689 - val_loss: 1.2405 - val_error_in_tfi: 15.4424\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4930 - error_in_tfi: 11.4102 - val_loss: 1.1880 - val_error_in_tfi: 15.1278\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3519 - error_in_tfi: 9.5527 - val_loss: 1.1228 - val_error_in_tfi: 14.6839\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.4080 - error_in_tfi: 10.2802 - val_loss: 1.0476 - val_error_in_tfi: 14.1582\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.3860 - error_in_tfi: 10.4305 - val_loss: 0.9805 - val_error_in_tfi: 13.6582\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3495 - error_in_tfi: 9.7395 - val_loss: 0.9264 - val_error_in_tfi: 13.2307\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3908 - error_in_tfi: 9.6370 - val_loss: 0.8821 - val_error_in_tfi: 12.8946\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.4054 - error_in_tfi: 10.8051 - val_loss: 0.8399 - val_error_in_tfi: 12.5964\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3710 - error_in_tfi: 9.2777 - val_loss: 0.8026 - val_error_in_tfi: 12.3152\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 1s 128ms/step - loss: 0.3678 - error_in_tfi: 9.1251 - val_loss: 0.7776 - val_error_in_tfi: 12.0156\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.3180 - error_in_tfi: 8.8985 - val_loss: 0.7486 - val_error_in_tfi: 11.6699\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3243 - error_in_tfi: 9.5383 - val_loss: 0.7238 - val_error_in_tfi: 11.4467\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3516 - error_in_tfi: 8.5923 - val_loss: 0.6908 - val_error_in_tfi: 11.1443\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2982 - error_in_tfi: 7.8094 - val_loss: 0.6515 - val_error_in_tfi: 10.7433\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3302 - error_in_tfi: 8.3169 - val_loss: 0.6283 - val_error_in_tfi: 10.6044\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3183 - error_in_tfi: 8.7445 - val_loss: 0.6054 - val_error_in_tfi: 10.4552\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.3735 - error_in_tfi: 9.4800 - val_loss: 0.5980 - val_error_in_tfi: 10.3503\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3241 - error_in_tfi: 8.8521 - val_loss: 0.5809 - val_error_in_tfi: 10.1651\n",
      "Epoch 31/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.2818 - error_in_tfi: 8.3238 - val_loss: 0.5562 - val_error_in_tfi: 9.9597\n",
      "Epoch 32/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3560 - error_in_tfi: 9.4374 - val_loss: 0.5333 - val_error_in_tfi: 9.6538\n",
      "Epoch 33/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3148 - error_in_tfi: 9.0436 - val_loss: 0.5203 - val_error_in_tfi: 9.4156\n",
      "Epoch 34/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.3101 - error_in_tfi: 8.6327 - val_loss: 0.5182 - val_error_in_tfi: 9.3943\n",
      "Epoch 35/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.2893 - error_in_tfi: 7.7050 - val_loss: 0.5180 - val_error_in_tfi: 9.3461\n",
      "Epoch 36/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2916 - error_in_tfi: 8.1645 - val_loss: 0.5163 - val_error_in_tfi: 9.3152\n",
      "Epoch 37/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2985 - error_in_tfi: 8.6913 - val_loss: 0.5001 - val_error_in_tfi: 9.1082\n",
      "Epoch 38/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2424 - error_in_tfi: 7.5323 - val_loss: 0.4902 - val_error_in_tfi: 9.0505\n",
      "Epoch 39/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2549 - error_in_tfi: 8.5397 - val_loss: 0.4763 - val_error_in_tfi: 8.9394\n",
      "Epoch 40/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2881 - error_in_tfi: 8.0751 - val_loss: 0.4595 - val_error_in_tfi: 8.6919\n",
      "Epoch 41/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2899 - error_in_tfi: 8.1754 - val_loss: 0.4566 - val_error_in_tfi: 8.6946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.2544 - error_in_tfi: 7.3167 - val_loss: 0.4460 - val_error_in_tfi: 8.4658\n",
      "Epoch 43/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.3788 - error_in_tfi: 9.4413 - val_loss: 0.4397 - val_error_in_tfi: 8.1749\n",
      "Epoch 44/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2783 - error_in_tfi: 7.6761 - val_loss: 0.4369 - val_error_in_tfi: 8.1344\n",
      "Epoch 45/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.3152 - error_in_tfi: 8.4026 - val_loss: 0.4354 - val_error_in_tfi: 8.0432\n",
      "Epoch 46/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.3068 - error_in_tfi: 8.2963 - val_loss: 0.4440 - val_error_in_tfi: 8.1484\n",
      "Epoch 47/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.2335 - error_in_tfi: 6.7997 - val_loss: 0.4473 - val_error_in_tfi: 8.1592\n",
      "Epoch 48/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2625 - error_in_tfi: 7.3063 - val_loss: 0.4526 - val_error_in_tfi: 8.3772\n",
      "Epoch 49/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.2529 - error_in_tfi: 7.7182 - val_loss: 0.4549 - val_error_in_tfi: 8.4993\n",
      "Epoch 50/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.2711 - error_in_tfi: 7.8019 - val_loss: 0.4492 - val_error_in_tfi: 8.3389\n",
      "Epoch 51/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.2392 - error_in_tfi: 7.9632 - val_loss: 0.4479 - val_error_in_tfi: 8.2962\n",
      "Epoch 52/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.3117 - error_in_tfi: 8.1402 - val_loss: 0.4427 - val_error_in_tfi: 8.1136\n",
      "Epoch 53/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.2904 - error_in_tfi: 7.9582 - val_loss: 0.4294 - val_error_in_tfi: 7.8102\n",
      "Epoch 54/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2375 - error_in_tfi: 7.8268 - val_loss: 0.4251 - val_error_in_tfi: 7.7121\n",
      "Epoch 55/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.2537 - error_in_tfi: 7.1871 - val_loss: 0.4241 - val_error_in_tfi: 7.6188\n",
      "Epoch 56/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.3305 - error_in_tfi: 8.4582 - val_loss: 0.4285 - val_error_in_tfi: 7.7009\n",
      "Epoch 57/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2795 - error_in_tfi: 8.1684 - val_loss: 0.4282 - val_error_in_tfi: 7.5964\n",
      "Epoch 58/500\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.2548 - error_in_tfi: 8.3405 - val_loss: 0.4313 - val_error_in_tfi: 7.6918\n",
      "Epoch 59/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.3090 - error_in_tfi: 8.2399 - val_loss: 0.4339 - val_error_in_tfi: 7.8109\n",
      "Epoch 60/500\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.2828 - error_in_tfi: 7.7446 - val_loss: 0.4307 - val_error_in_tfi: 7.6993\n",
      "Epoch 61/500\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.2882 - error_in_tfi: 7.5650 - val_loss: 0.4262 - val_error_in_tfi: 7.5960\n",
      "Epoch 62/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.2979 - error_in_tfi: 7.2620 - val_loss: 0.4205 - val_error_in_tfi: 7.5112\n",
      "Epoch 63/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.2440 - error_in_tfi: 7.1709 - val_loss: 0.4215 - val_error_in_tfi: 7.5716\n",
      "Epoch 64/500\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.2544 - error_in_tfi: 7.8755 - val_loss: 0.4300 - val_error_in_tfi: 7.6933\n",
      "Epoch 65/500\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.2218 - error_in_tfi: 7.2629 - val_loss: 0.4404 - val_error_in_tfi: 7.8445\n",
      "Epoch 66/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.2293 - error_in_tfi: 6.3259 - val_loss: 0.4513 - val_error_in_tfi: 8.1791\n",
      "Epoch 67/500\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.2156 - error_in_tfi: 7.4521 - val_loss: 0.4581 - val_error_in_tfi: 8.4795\n",
      "Epoch 68/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.3073 - error_in_tfi: 7.8506 - val_loss: 0.4629 - val_error_in_tfi: 8.6531\n",
      "Epoch 69/500\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 0.2284 - error_in_tfi: 6.7818 - val_loss: 0.4625 - val_error_in_tfi: 8.2924\n",
      "Epoch 70/500\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.3455 - error_in_tfi: 8.4980 - val_loss: 0.4569 - val_error_in_tfi: 8.0382\n",
      "Epoch 71/500\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2490 - error_in_tfi: 7.2628 - val_loss: 0.4516 - val_error_in_tfi: 7.7793\n",
      "Epoch 72/500\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.2808 - error_in_tfi: 7.1187 - val_loss: 0.4509 - val_error_in_tfi: 7.6920\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5502 - error_in_tfi: 13.7149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "error_scores_US=[]\n",
    "error_scores_WN=[]\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "\n",
    "def cross_val(df,n_features,error_scores):\n",
    "    X,y,x_scaler,y_scaler=data_prep(df,n_features)\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model,history=prediction(x_train,y_train,y_scaler)\n",
    "        val_mse,val_mae=model.evaluate(x_test,y_test) #returns -- test loss, test metrics\n",
    "        error_scores.append(val_mae)\n",
    "    return history,x_scaler,y_scaler,error_scores\n",
    "\n",
    "US_history,US_x_scaler,US_y_scaler,US_error_scores=cross_val(df_stable_US,n_features_US,error_scores_US)\n",
    "WN_history,WN_x_scaler,WN_y_scaler,WN_error_scores=cross_val(df_stable_WN,n_features_WN,error_scores_WN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03cfb065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US_error\n",
      "--------\n",
      "[10.833792686462402, 12.296378135681152, 15.793066024780273, 12.977690696716309, 9.746867179870605]\n",
      "\n",
      "\n",
      "12.329558944702148\n",
      "\n",
      "\n",
      "WN_error\n",
      "--------\n",
      "[20.627243041992188, 9.532112121582031, 11.078472137451172, 23.999364852905273, 13.714899063110352]\n",
      "\n",
      "\n",
      "15.790418243408203\n"
     ]
    }
   ],
   "source": [
    "print(\"US_error\")\n",
    "print(\"--------\")\n",
    "print(US_error_scores)\n",
    "US_error=np.mean(US_error_scores)\n",
    "print(\"\\n\")\n",
    "print(US_error)\n",
    "print(\"\\n\")\n",
    "print(\"WN_error\")\n",
    "print(\"--------\")\n",
    "print(WN_error_scores)\n",
    "WN_error=np.mean(WN_error_scores)\n",
    "print(\"\\n\")\n",
    "print(WN_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb40e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tinnitus)",
   "language": "python",
   "name": "tinnitus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
