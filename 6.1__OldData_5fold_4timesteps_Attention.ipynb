{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189c37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Attention #from attention import Attention\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import kaleido ##pip install -U kaleido ##to save a plotly fig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbce8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('longitudinal behavioral data.csv')\n",
    "df=df.drop(['subject','2weeks_class','1month_class','2months_class','3months_class'],axis=1)\n",
    "print(len(df))\n",
    "#df.dtypes[df.dtypes==object]\n",
    "df['Annoying.4'] = df['Annoying.4'].replace('.', 2)\n",
    "df['Annoying.4'] = df['Annoying.4'].astype('float32')\n",
    "#df.columns.get_loc(\"3months_TFI\") -->80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de55c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate dependent and independent features \n",
    "X=df.iloc[:,:80].values\n",
    "y=df.iloc[:,-20:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4667a039",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 4, 20) (18, 20)\n",
      "(4, 20)\n",
      "TRAIN: [ 4  5  6  7  8  9 10 11 12 13 14 15 16 17] TEST: [0 1 2 3]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:56:39.186678: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 14.8973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:56:40.599146: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 725ms/step - loss: 14.8973 - val_loss: 27.1682\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 13.9969 - val_loss: 25.2222\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 14.1368 - val_loss: 23.6188\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 12.6503 - val_loss: 22.2554\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 12.5478 - val_loss: 21.3212\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 11.9266 - val_loss: 20.9888\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 11.0308 - val_loss: 20.8440\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 10.5842 - val_loss: 20.7635\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 10.7427 - val_loss: 20.6978\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 11.1123 - val_loss: 20.6575\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 10.8827 - val_loss: 20.6493\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 10.6325 - val_loss: 20.6441\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 11.2481 - val_loss: 20.6406\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 10.8744 - val_loss: 20.6384\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 11.0012 - val_loss: 20.6380\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.8133 - val_loss: 20.6380\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 11.1398 - val_loss: 20.6395\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 11.4677 - val_loss: 20.6416\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.8839 - val_loss: 20.6432\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.5478 - val_loss: 20.6449\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.7404 - val_loss: 20.6468\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.7959 - val_loss: 20.6489\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.9532 - val_loss: 20.6517\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.8219 - val_loss: 20.6590\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.4738 - val_loss: 20.6686\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 11.1360 - val_loss: 20.6893\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 10.9049 - val_loss: 20.7097\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 10.6011 - val_loss: 20.7280\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 10.8006 - val_loss: 20.7455\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 10.6651 - val_loss: 20.7520\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 10.5414 - val_loss: 20.7543\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 10.6191 - val_loss: 20.7569\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.4582 - val_loss: 20.7587\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 10.6043 - val_loss: 20.7576\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 10.3686 - val_loss: 20.7547\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.7403 - val_loss: 20.7556\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.7693 - val_loss: 20.7526\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.6253 - val_loss: 20.7557\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 10.6387 - val_loss: 20.7587\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 10.5275 - val_loss: 20.7623\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 10.8972 - val_loss: 20.7630\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.5833 - val_loss: 20.7595\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 10.6221 - val_loss: 20.7555\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 10.7031 - val_loss: 20.7526\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.4481 - val_loss: 20.7489\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 10.5342 - val_loss: 20.7436\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 10.5201 - val_loss: 20.7407\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 10.4601 - val_loss: 20.7353\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 10.4356 - val_loss: 20.7322\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.4782 - val_loss: 20.7337\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 10.5433 - val_loss: 20.7317\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 10.5975 - val_loss: 20.7275\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 10.5664 - val_loss: 20.7270\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.5147 - val_loss: 20.7278\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.4244 - val_loss: 20.7283\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 10.5316 - val_loss: 20.7272\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 10.5001 - val_loss: 20.7296\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 10.5690 - val_loss: 20.7311\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 10.5760 - val_loss: 20.7329\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.4283 - val_loss: 20.7320\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 10.3711 - val_loss: 20.7326\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.5516 - val_loss: 20.7329\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.4216 - val_loss: 20.7344\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 10.4691 - val_loss: 20.7355\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.5426 - val_loss: 20.7344\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.5904 - val_loss: 20.7356\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 10.5697 - val_loss: 20.7436\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.4952 - val_loss: 20.7485\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.5018 - val_loss: 20.7570\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.5021 - val_loss: 20.7678\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 10.5020 - val_loss: 20.7773\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 10.4191 - val_loss: 20.7824\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.4855 - val_loss: 20.7826\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.3366 - val_loss: 20.7783\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.6767 - val_loss: 20.7773\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.4708 - val_loss: 20.7730\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 10.4612 - val_loss: 20.7702\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.6493 - val_loss: 20.7724\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.3904 - val_loss: 20.7706\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.5915 - val_loss: 20.7737\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.5197 - val_loss: 20.7726\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 81ms/step - loss: 10.5328 - val_loss: 20.7671\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.4433 - val_loss: 20.7625\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.5312 - val_loss: 20.7629\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.4480 - val_loss: 20.7653\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.5892 - val_loss: 20.7722\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 10.5585 - val_loss: 20.7745\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.4842 - val_loss: 20.7744\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 10.4800 - val_loss: 20.7737\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 10.3886 - val_loss: 20.7748\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 10.4478 - val_loss: 20.7733\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 10.5700 - val_loss: 20.7711\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 10.4087 - val_loss: 20.7663\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 10.4586 - val_loss: 20.7672\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 10.4287 - val_loss: 20.7657\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.5130 - val_loss: 20.7595\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 10.5437 - val_loss: 20.7559\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 10.3829 - val_loss: 20.7491\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.4379 - val_loss: 20.7376\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 10.4539 - val_loss: 20.7284\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 8.6928\n",
      "TRAIN: [ 0  1  2  3  8  9 10 11 12 13 14 15 16 17] TEST: [4 5 6 7]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:56:56.384504: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 18.3667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:56:57.879581: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 746ms/step - loss: 18.3667 - val_loss: 32.4495\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 17.0994 - val_loss: 30.4496\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 15.6875 - val_loss: 28.4725\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 14.7812 - val_loss: 26.5393\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 14.1349 - val_loss: 24.7835\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 11.9782 - val_loss: 23.3268\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 11.4691 - val_loss: 22.0648\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 11.0379 - val_loss: 21.2653\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 10.7277 - val_loss: 20.9939\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 10.0736 - val_loss: 20.8648\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 10.0028 - val_loss: 20.7888\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 10.0258 - val_loss: 20.7227\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 9.9057 - val_loss: 20.6745\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 9.9416 - val_loss: 20.6686\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.2786 - val_loss: 20.6644\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 10.2909 - val_loss: 20.6616\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.7425 - val_loss: 20.6595\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 10.4093 - val_loss: 20.6585\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.8983 - val_loss: 20.6574\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 9.7882 - val_loss: 20.6568\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.7874 - val_loss: 20.6566\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.0265 - val_loss: 20.6568\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 10.1190 - val_loss: 20.6570\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 10.2660 - val_loss: 20.6573\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 10.0716 - val_loss: 20.6579\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.1809 - val_loss: 20.6588\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.7373 - val_loss: 20.6597\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.3052 - val_loss: 20.6610\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9759 - val_loss: 20.6621\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8081 - val_loss: 20.6630\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.0518 - val_loss: 20.6646\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.0249 - val_loss: 20.6701\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.0652 - val_loss: 20.6797\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.0595 - val_loss: 20.6884\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8557 - val_loss: 20.6964\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 10.1041 - val_loss: 20.7062\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.0179 - val_loss: 20.7131\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8839 - val_loss: 20.7171\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 10.0991 - val_loss: 20.7224\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.8710 - val_loss: 20.7238\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.9234 - val_loss: 20.7229\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.9467 - val_loss: 20.7225\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9710 - val_loss: 20.7247\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.8984 - val_loss: 20.7267\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.1267 - val_loss: 20.7318\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.9635 - val_loss: 20.7376\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9957 - val_loss: 20.7443\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.9451 - val_loss: 20.7466\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.8643 - val_loss: 20.7440\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.9659 - val_loss: 20.7421\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.9698 - val_loss: 20.7404\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.9697 - val_loss: 20.7372\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.8983 - val_loss: 20.7294\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9623 - val_loss: 20.7247\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.8453 - val_loss: 20.7234\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.9975 - val_loss: 20.7240\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8710 - val_loss: 20.7222\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8864 - val_loss: 20.7223\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9084 - val_loss: 20.7224\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8385 - val_loss: 20.7223\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7894 - val_loss: 20.7222\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8334 - val_loss: 20.7192\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.9710 - val_loss: 20.7169\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8591 - val_loss: 20.7126\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8353 - val_loss: 20.7076\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7823 - val_loss: 20.7004\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8481 - val_loss: 20.6944\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8273 - val_loss: 20.6921\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9154 - val_loss: 20.6921\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.9260 - val_loss: 20.6948\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8214 - val_loss: 20.6933\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9167 - val_loss: 20.6935\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8639 - val_loss: 20.6919\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7925 - val_loss: 20.6887\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7755 - val_loss: 20.6847\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7850 - val_loss: 20.6806\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.8318 - val_loss: 20.6754\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8644 - val_loss: 20.6697\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7917 - val_loss: 20.6618\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.8532 - val_loss: 20.6604\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8608 - val_loss: 20.6593\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.8729 - val_loss: 20.6587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9086 - val_loss: 20.6589\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8787 - val_loss: 20.6603\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8448 - val_loss: 20.6667\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.8609 - val_loss: 20.6698\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.8805 - val_loss: 20.6724\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7878 - val_loss: 20.6732\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.7640 - val_loss: 20.6731\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.8131 - val_loss: 20.6738\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.7586 - val_loss: 20.6756\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.7997 - val_loss: 20.6752\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8561 - val_loss: 20.6784\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8279 - val_loss: 20.6804\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8511 - val_loss: 20.6823\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.8665 - val_loss: 20.6855\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7836 - val_loss: 20.6868\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7471 - val_loss: 20.6854\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.7275 - val_loss: 20.6809\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7956 - val_loss: 20.6758\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 10.4551\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7 12 13 14 15 16 17] TEST: [ 8  9 10 11]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:57:13.937175: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 10.7353"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:57:15.500876: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 811ms/step - loss: 10.7353 - val_loss: 22.0599\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 10.7773 - val_loss: 21.3341\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 10.4317 - val_loss: 21.0476\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 10.2498 - val_loss: 20.8975\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 10.5570 - val_loss: 20.8161\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 9.8556 - val_loss: 20.7494\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 10.2101 - val_loss: 20.7147\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 9.6292 - val_loss: 20.7009\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 9.7637 - val_loss: 20.6959\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 10.4581 - val_loss: 20.6918\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 10.1903 - val_loss: 20.6888\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 9.8011 - val_loss: 20.6867\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 10.3921 - val_loss: 20.6858\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 9.9836 - val_loss: 20.6850\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 10.2484 - val_loss: 20.6852\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.8707 - val_loss: 20.6898\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7840 - val_loss: 20.7086\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.1597 - val_loss: 20.7361\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.0045 - val_loss: 20.7621\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7266 - val_loss: 20.7778\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.9247 - val_loss: 20.7880\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.8370 - val_loss: 20.7925\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.9303 - val_loss: 20.7852\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.7992 - val_loss: 20.7762\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.9071 - val_loss: 20.7732\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.7323 - val_loss: 20.7684\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.8394 - val_loss: 20.7644\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.6712 - val_loss: 20.7565\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.7675 - val_loss: 20.7468\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.7177 - val_loss: 20.7339\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.7423 - val_loss: 20.7173\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.6842 - val_loss: 20.7008\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.8160 - val_loss: 20.6900\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.6689 - val_loss: 20.6822\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.9100 - val_loss: 20.6784\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.7207 - val_loss: 20.6815\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.6565 - val_loss: 20.6854\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.9041 - val_loss: 20.6894\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.7707 - val_loss: 20.6895\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.7534 - val_loss: 20.6840\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.7519 - val_loss: 20.6800\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.5979 - val_loss: 20.6733\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.8439 - val_loss: 20.6732\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.7042 - val_loss: 20.6778\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.6875 - val_loss: 20.6795\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.8072 - val_loss: 20.6830\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.5817 - val_loss: 20.6808\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.7439 - val_loss: 20.6811\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.6335 - val_loss: 20.6872\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.6038 - val_loss: 20.6915\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.6587 - val_loss: 20.6879\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 9.7638 - val_loss: 20.6817\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.6382 - val_loss: 20.6721\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.7236 - val_loss: 20.6616\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.6961 - val_loss: 20.6511\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 9.6852 - val_loss: 20.6457\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.6193 - val_loss: 20.6419\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 9.6569 - val_loss: 20.6409\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 9.7200 - val_loss: 20.6396\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 9.6760 - val_loss: 20.6394\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 9.6479 - val_loss: 20.6382\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.6832 - val_loss: 20.6343\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.6972 - val_loss: 20.6301\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.6783 - val_loss: 20.6260\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.7407 - val_loss: 20.6208\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.6761 - val_loss: 20.6142\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.6104 - val_loss: 20.6117\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.5549 - val_loss: 20.6104\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 9.7144 - val_loss: 20.6088\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.5455 - val_loss: 20.6084\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 9.5817 - val_loss: 20.6114\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.6739 - val_loss: 20.6109\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.6343 - val_loss: 20.6097\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.6102 - val_loss: 20.6064\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.6684 - val_loss: 20.6033\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.5796 - val_loss: 20.6016\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 9.5859 - val_loss: 20.5990\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.5895 - val_loss: 20.5952\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.6119 - val_loss: 20.5948\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.5749 - val_loss: 20.5946\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 9.5895 - val_loss: 20.5946\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 9.5723 - val_loss: 20.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.5777 - val_loss: 20.5907\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.5326 - val_loss: 20.5889\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 9.4992 - val_loss: 20.5867\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.5177 - val_loss: 20.5854\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.5764 - val_loss: 20.5834\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.5302 - val_loss: 20.5837\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.5499 - val_loss: 20.5821\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.5342 - val_loss: 20.5785\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 9.5341 - val_loss: 20.5742\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.5197 - val_loss: 20.5706\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.5514 - val_loss: 20.5663\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 9.5393 - val_loss: 20.5669\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.5490 - val_loss: 20.5670\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.5315 - val_loss: 20.5697\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 9.5122 - val_loss: 20.5721\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 9.5276 - val_loss: 20.5752\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 9.5369 - val_loss: 20.5745\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 9.5317 - val_loss: 20.5724\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 11.5865\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 15 16 17] TEST: [12 13 14]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:57:31.312973: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 12.9111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:57:32.475515: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 578ms/step - loss: 12.9111 - val_loss: 24.1635\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 12.1465 - val_loss: 22.9962\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 11.5796 - val_loss: 21.9488\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 11.5319 - val_loss: 21.2812\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 10.6288 - val_loss: 20.9962\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 10.7125 - val_loss: 20.8528\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 9.9911 - val_loss: 20.7778\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 10.1775 - val_loss: 20.7126\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 10.2065 - val_loss: 20.6568\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 10.3776 - val_loss: 20.6390\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 10.0540 - val_loss: 20.6261\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 10.0104 - val_loss: 20.6221\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 10.4013 - val_loss: 20.6190\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 10.3935 - val_loss: 20.6172\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.1911 - val_loss: 20.6158\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 10.1728 - val_loss: 20.6148\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.9995 - val_loss: 20.6140\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9508 - val_loss: 20.6135\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.8952 - val_loss: 20.6131\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.8554 - val_loss: 20.6128\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 10.1233 - val_loss: 20.6127\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 9.9922 - val_loss: 20.6130\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 10.2227 - val_loss: 20.6135\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 10.1584 - val_loss: 20.6178\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 10.1568 - val_loss: 20.6217\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.0514 - val_loss: 20.6254\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.1152 - val_loss: 20.6301\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.4212 - val_loss: 20.6354\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.8868 - val_loss: 20.6407\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.1571 - val_loss: 20.6451\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9849 - val_loss: 20.6528\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9920 - val_loss: 20.6583\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 9.8544 - val_loss: 20.6593\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.1652 - val_loss: 20.6624\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.8982 - val_loss: 20.6612\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.8548 - val_loss: 20.6570\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.8307 - val_loss: 20.6513\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9705 - val_loss: 20.6463\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.0250 - val_loss: 20.6437\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.0794 - val_loss: 20.6418\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9454 - val_loss: 20.6398\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 10.0285 - val_loss: 20.6381\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0988 - val_loss: 20.6370\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.0717 - val_loss: 20.6384\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0369 - val_loss: 20.6397\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9391 - val_loss: 20.6410\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9292 - val_loss: 20.6430\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0087 - val_loss: 20.6454\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.8980 - val_loss: 20.6486\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0140 - val_loss: 20.6521\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9589 - val_loss: 20.6559\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.8912 - val_loss: 20.6570\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.9926 - val_loss: 20.6575\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.8878 - val_loss: 20.6556\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.8740 - val_loss: 20.6540\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9776 - val_loss: 20.6542\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9153 - val_loss: 20.6552\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9663 - val_loss: 20.6568\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.8907 - val_loss: 20.6559\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.8962 - val_loss: 20.6533\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9760 - val_loss: 20.6506\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9176 - val_loss: 20.6469\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9442 - val_loss: 20.6448\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.9408 - val_loss: 20.6429\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.8934 - val_loss: 20.6412\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9432 - val_loss: 20.6406\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9351 - val_loss: 20.6397\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.9382 - val_loss: 20.6398\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.8862 - val_loss: 20.6390\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 9.8954 - val_loss: 20.6374\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.8976 - val_loss: 20.6363\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9406 - val_loss: 20.6345\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.8456 - val_loss: 20.6343\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 9.9255 - val_loss: 20.6340\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 9.8797 - val_loss: 20.6333\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 9.8675 - val_loss: 20.6329\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.9553 - val_loss: 20.6335\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 9.8483 - val_loss: 20.6329\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 9.9343 - val_loss: 20.6317\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.8205 - val_loss: 20.6319\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.8908 - val_loss: 20.6308\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9182 - val_loss: 20.6305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.8939 - val_loss: 20.6314\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 9.8618 - val_loss: 20.6312\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 9.8280 - val_loss: 20.6309\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 9.8860 - val_loss: 20.6309\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.9535 - val_loss: 20.6308\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.9333 - val_loss: 20.6309\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 9.9052 - val_loss: 20.6309\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.8495 - val_loss: 20.6310\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.9091 - val_loss: 20.6310\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.8212 - val_loss: 20.6302\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.8788 - val_loss: 20.6303\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.8520 - val_loss: 20.6306\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.8176 - val_loss: 20.6312\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.8924 - val_loss: 20.6306\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 9.8967 - val_loss: 20.6310\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.8105 - val_loss: 20.6299\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 9.8967 - val_loss: 20.6284\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.9044 - val_loss: 20.6278\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.1238\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [15 16 17]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:57:44.816629: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 16.8220"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 06:57:46.074626: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 597ms/step - loss: 16.8220 - val_loss: 15.9993\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 15.7625 - val_loss: 15.1491\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 14.8328 - val_loss: 14.3196\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 13.5109 - val_loss: 13.5001\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 12.2567 - val_loss: 12.7030\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 11.9804 - val_loss: 12.0100\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 11.0759 - val_loss: 11.4692\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 10.7530 - val_loss: 11.0032\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 10.3988 - val_loss: 10.6455\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 10.3198 - val_loss: 10.4508\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 10.0134 - val_loss: 10.3165\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 10.2787 - val_loss: 10.2531\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.1652 - val_loss: 10.2146\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.1382 - val_loss: 10.1841\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 10.2056 - val_loss: 10.1607\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.1569 - val_loss: 10.1428\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.0407 - val_loss: 10.1296\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.4418 - val_loss: 10.1237\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 10.1292 - val_loss: 10.1230\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.3383 - val_loss: 10.1226\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0395 - val_loss: 10.1236\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.1126 - val_loss: 10.1283\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.1516 - val_loss: 10.1334\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9624 - val_loss: 10.1408\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9915 - val_loss: 10.1471\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9536 - val_loss: 10.1533\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0081 - val_loss: 10.1597\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9745 - val_loss: 10.1646\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 10.0814 - val_loss: 10.1703\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.1258 - val_loss: 10.1753\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.0078 - val_loss: 10.1789\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.1430 - val_loss: 10.1817\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.1178 - val_loss: 10.1853\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0422 - val_loss: 10.1899\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0244 - val_loss: 10.1938\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0700 - val_loss: 10.1967\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0036 - val_loss: 10.1994\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0806 - val_loss: 10.2024\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0263 - val_loss: 10.2044\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.9769 - val_loss: 10.2064\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.1229 - val_loss: 10.2096\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.1457 - val_loss: 10.2115\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9410 - val_loss: 10.2126\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9283 - val_loss: 10.2128\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9831 - val_loss: 10.2128\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0908 - val_loss: 10.2117\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0466 - val_loss: 10.2115\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.1082 - val_loss: 10.2125\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0192 - val_loss: 10.2124\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9197 - val_loss: 10.2114\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.0207 - val_loss: 10.2100\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.1131 - val_loss: 10.2093\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.1410 - val_loss: 10.2073\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.9928 - val_loss: 10.2066\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.0081 - val_loss: 10.2049\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 10.0085 - val_loss: 10.2040\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 10.0211 - val_loss: 10.2047\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.1346 - val_loss: 10.2056\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.9372 - val_loss: 10.2060\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0316 - val_loss: 10.2053\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.0442 - val_loss: 10.2051\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.0300 - val_loss: 10.2054\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 10.0396 - val_loss: 10.2048\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.0849 - val_loss: 10.2060\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.0631 - val_loss: 10.2073\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.9880 - val_loss: 10.2074\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.0508 - val_loss: 10.2076\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 9.9478 - val_loss: 10.2059\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.0297 - val_loss: 10.2046\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.9622 - val_loss: 10.2036\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.0233 - val_loss: 10.2009\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.0742 - val_loss: 10.2013\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0541 - val_loss: 10.2019\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.9916 - val_loss: 10.2030\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.0667 - val_loss: 10.2033\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9791 - val_loss: 10.2017\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.9708 - val_loss: 10.1984\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 10.0108 - val_loss: 10.1956\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.9387 - val_loss: 10.1938\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9985 - val_loss: 10.1937\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.9831 - val_loss: 10.1934\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 72ms/step - loss: 10.0680 - val_loss: 10.1967\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.0441 - val_loss: 10.1992\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.0002 - val_loss: 10.2007\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.9788 - val_loss: 10.2022\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0334 - val_loss: 10.2052\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9711 - val_loss: 10.2066\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.0058 - val_loss: 10.2076\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.9476 - val_loss: 10.2068\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 10.0381 - val_loss: 10.2063\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 9.9519 - val_loss: 10.2059\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9601 - val_loss: 10.2047\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9263 - val_loss: 10.2049\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9936 - val_loss: 10.2048\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.0279 - val_loss: 10.2042\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9294 - val_loss: 10.2065\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9374 - val_loss: 10.2078\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 9.9474 - val_loss: 10.2060\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 9.9553 - val_loss: 10.2027\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.9623 - val_loss: 10.2005\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 20.7453\n"
     ]
    }
   ],
   "source": [
    "# with scaled data\n",
    "n_epochs=100\n",
    "n_batch_size=6 #batch training\n",
    "n_timesteps=4\n",
    "n_features=20\n",
    "lr=0.01\n",
    "kfold = KFold(n_splits=5)\n",
    "error_scores=[]\n",
    "\n",
    "\"\"\"\n",
    "    Reshape rule:\n",
    "    tensor of shape (batch size, sequence length, features), \n",
    "    where sequence length is the number of time steps and features is each input timeseries.\n",
    "\"\"\"\n",
    "\n",
    "# Reshape input to be [batch size, sequence length, features]\n",
    "X = X.reshape((X.shape[0],n_timesteps,n_features))\n",
    "print(X.shape,y.shape)\n",
    "input_shape=(n_timesteps,n_features)\n",
    "print(input_shape)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Include residual connections, layer normalization, and dropout.\n",
    "The resulting layer can be stacked multiple times.\n",
    "The projection layers are implemented through `keras.layers.Conv1D`.\n",
    "\"\"\"\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(units=inputs.shape[-1])(x) \n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    K.clear_session()\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    \"\"\"    \n",
    "    can stack multiple of the transformer_encoder blocks and \n",
    "    can also proceed to add the final Multi-Layer Perceptron regression head.\n",
    "    \"\"\"\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    \"\"\"\n",
    "    a pooling layer is used to to reduce the output tensor of the TransformerEncoder \n",
    "    part of our model down to a vector of features for each data point in the current batch.\n",
    "    \"\"\"\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x) \n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "def prediction(x_train,y_train):\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    model = build_model(\n",
    "        input_shape,\n",
    "        head_size=5, # Embedding size for each token #key_dim\n",
    "        num_heads=4, # Number of attention heads\n",
    "        ff_dim=5, # Hidden layer size in feed forward network inside transformer\n",
    "        num_transformer_blocks=4,\n",
    "        mlp_units=[128],\n",
    "        mlp_dropout=0.4,\n",
    "        dropout=0.25,\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"mae\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        #metrics=[keras.metrics.MeanAbsoluteError()],\n",
    "\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cross_val(df,error_scores):\n",
    "    #X,y,input_shape,x_scaler,y_scaler=data_prep(df)\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model=prediction(x_train,y_train)\n",
    "        history=model.fit(x_train, y_train,epochs=n_epochs,batch_size=n_batch_size,validation_split=0.2,verbose =1)#validation_data=(x_test,y_test) #validation_split=0.2\n",
    "        #val_mse,val_mae=model.evaluate(x_test,y_test)\n",
    "        #error_scores.append(val_mae)\n",
    "        val_mae=model.evaluate(x_test,y_test)\n",
    "        error_scores.append(val_mae)\n",
    "    return history,error_scores\n",
    "\n",
    "history,error_scores=cross_val(df,error_scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029e96bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "[8.692800521850586, 10.455135345458984, 11.586532592773438, 10.123847007751465, 20.745262145996094]\n",
      "\n",
      "\n",
      "12.320715522766113\n"
     ]
    }
   ],
   "source": [
    "print(\"error\")\n",
    "print(error_scores)\n",
    "error=np.mean(error_scores)\n",
    "print(\"\\n\")\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eee2cdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "        train_loss=history.history['loss']\n",
    "        val_loss=history.history['val_loss']\n",
    "        x=list(range(1,len(val_loss)+1))\n",
    "        plt.plot(x,train_loss,label='training loss')\n",
    "        plt.plot(x,val_loss,color='red',label='val_loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss vs. Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1581e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tinnitus)",
   "language": "python",
   "name": "tinnitus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
